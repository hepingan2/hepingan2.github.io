<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>PASS-Assistant Document</title>
      <link href="/2024/05/27/PASS-Assistant-Document/"/>
      <url>/2024/05/27/PASS-Assistant-Document/</url>
      
        <content type="html"><![CDATA[<p><img src="https://img2.imgtp.com/2024/04/28/mrPpQmag.png"></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>全新的Ai整合Web网站，基于近年来大火的ChatGPT3.5，ChatGPT4和智谱清言GLM-4v大模型开发了基础的智能对话系统。该网站拥有基础的登录系统，积分系统，用户管理系统，兑换码系统，订单系统，收藏功能和服务器配置系统等，还有预设角色和模型对话，SD-AI绘画模块，私人GPT预设模块，数字人平台模块等新奇功能。</p><p>•SD-AI绘画模块：采用远程调用云服务器sd绘画接口，能在线实现文生图，图生图功能，用户在线完成绘画功能后，可以选择将其发布到该网站给予其它浏览人观看，实现共享型绘画分享社区。<br>•私人GPT预设模块：用户能自己填写对话模型api，密钥，模型预设，回复速率等设置，实现自己私人定制的GPT，打造更符合自身的预设。<br>•数字人平台：基于智能问答模型(GPT3.5&#x2F;GLM-4V)，GPT-SOVITS模型和阿里云函数计算FC应用数字人接口集合而成的在线数字人问答平台，用户能够在预设的数字人模型中与其对话，发送消息后会经过这三个模型加工处理，返回一个对刚刚消息处理的数字人对话视频。该模块平台除了对话功能，还有在线定制数字人功能，用户仅需上传人物头像，一分钟录制语音和模型预设词就可在线合成数字人模型，并且可以将模型分享到该网站。</p><h2 id="部分接口文档及设计"><a href="#部分接口文档及设计" class="headerlink" title="部分接口文档及设计"></a>部分接口文档及设计</h2><p>接口文档由本人亲自编写，有错误欢迎指正哦~(qq1973016127)</p><p><strong>控制台(Admin)</strong></p><p>对ai模型各个配置可直接在可视化面板修改，因为后端所有请求中都是查询redis中的配置数据进行接口调用，所以配置信息也是存储在redis中的。</p><p>获取终端数据：GET &#x2F;admin&#x2F;server&#x2F;get&#x2F;terminal</p><p>更新终端数据：POST &#x2F;admin&#x2F;server&#x2F;put&#x2F;terminal</p><p>获取总人数：GET &#x2F;admin&#x2F;user&#x2F;get&#x2F;count</p><p>获取用户信息：POST &#x2F;user&#x2F;current&#x2F;info</p><p><strong>黑夜&#x2F;日间 主题切换</strong></p><p><strong>Web 对话记忆(历史对话记录)</strong></p><p><strong>超级实验室功能 (含绘图功能 NewBing claude 免费GPT)</strong></p><p><strong>对话暂停</strong></p><p><strong>商品</strong></p><p>获取订单分页数据：&#x2F;admin&#x2F;orders&#x2F;page?pageNum&#x3D;a&amp;prompt&#x3D; b &amp;status&#x3D;c</p><p>删除交易产品：&#x2F;admin&#x2F;product&#x2F;delete&#x2F;‘ + data</p><p>添加交易产品：&#x2F;admin&#x2F;product&#x2F;put&#x2F;data</p><p>获取交易产品：&#x2F;admin&#x2F;product&#x2F;get&#x2F;page?pageNum&#x3D;’ + a + ‘&amp;prompt&#x3D;’ + b,</p><p><strong>支付宝支付</strong></p><p>构建订单：url: ‘&#x2F;pay&#x2F;alipay&#x2F;pay&#x2F;‘ + data,<br>method: ‘POST’</p><p>状态查询：url: ‘&#x2F;pay&#x2F;alipay&#x2F;status&#x2F;‘ + data,<br>method: ‘POST’</p><p>用户订单page：url: ‘&#x2F;pay&#x2F;orders&#x2F;page?pageNum&#x3D;’ + data,<br>method: ‘GET’</p><p><strong>GPT 流对话</strong></p><p><strong>对话功能收藏功能 以及 收藏对话回溯场景</strong></p><p><strong>支持 GPT 自定义敏感词拦截 以及 微信铭感词拦截</strong></p><p><strong>SD MJ 文生图 图生图</strong></p><p><strong>支持自定义 GPT 预设词功能版块(无限制定义)</strong></p><p><strong>项目工件模块化</strong></p><p><strong>Ai 币兑换码</strong> </p><p>生产交换码：&#x2F;admin&#x2F;exchange&#x2F;build</p><p>删除交换码：&#x2F;admin&#x2F;exchange&#x2F;delete&#x2F;（data）</p><p>兑换交换码：&#x2F;admin&#x2F;exchange&#x2F;get&#x2F;page?pageNum&#x3D;’ + a + ‘&amp;prompt&#x3D;’ + b,</p><p><strong>登录注册</strong></p><p>找回密码：POST &#x2F;auth&#x2F;email&#x2F;password&#x2F;back</p><p>注册：url: ‘&#x2F;auth&#x2F;email&#x2F;enroll’,<br>method: ‘POST’</p><p>邮箱登录：url: ‘&#x2F;auth&#x2F;email&#x2F;login’,<br>method: ‘POST’</p><p>是否登录成功：url: ‘&#x2F;auth&#x2F;wechat&#x2F;code&#x2F;result?verifyCode&#x3D;’ + data,<br>method: ‘GET’</p><p><strong>创意广场(允许将优秀个人作品展示至鉴赏 所有用户可视)</strong></p><p><strong>支持 GPT 对话暂停输出以及继续输出</strong></p><p><strong>内置后台管理 (用户管理 绘图管理 服务器管理 兑换码管理)</strong></p><p><strong>接入百度翻译 API</strong></p><p><strong>收藏</strong></p><p>添加收藏：url: ‘&#x2F;user&#x2F;stat&#x2F;put&#x2F;data’,<br>method: ‘POST’,<br>data</p><p>删除收藏：url: ‘&#x2F;user&#x2F;star&#x2F;delete&#x2F;‘ + data,<br>method: ‘POST’</p><p><strong>绘图类 API 列队处理</strong></p><p><strong>绘画模型控制台修改</strong></p><p>查询：GET &#x2F;admin&#x2F;sd&#x2F;page&#x2F;model?pageNum&#x3D;?</p><p>新增：POST &#x2F;admin&#x2F;sd&#x2F;put&#x2F;data</p><p>删除：&#x2F;admin&#x2F;sd&#x2F;delete&#x2F;data</p><p><strong>数据懒异步处理</strong></p><p><strong>支持主流语言以及配置类 代码高亮</strong></p><p><strong>公告</strong></p><p>url: ‘&#x2F;public&#x2F;get&#x2F;announcement’,<br>method: ‘GET’</p>]]></content>
      
      
      <categories>
          
          <category> 站内资源 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PASS-Aissistant </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PASS-Assistant Demonstration</title>
      <link href="/2024/05/27/PASS-Assistant-Demonstration/"/>
      <url>/2024/05/27/PASS-Assistant-Demonstration/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>一些免费推荐的数字人平台</title>
      <link href="/2024/04/18/%E4%B8%80%E4%BA%9B%E5%85%8D%E8%B4%B9%E6%8E%A8%E8%8D%90%E7%9A%84%E6%95%B0%E5%AD%97%E4%BA%BA%E5%B9%B3%E5%8F%B0/"/>
      <url>/2024/04/18/%E4%B8%80%E4%BA%9B%E5%85%8D%E8%B4%B9%E6%8E%A8%E8%8D%90%E7%9A%84%E6%95%B0%E5%AD%97%E4%BA%BA%E5%B9%B3%E5%8F%B0/</url>
      
        <content type="html"><![CDATA[<p>-转载于知乎</p><p><strong>我尝试了众多的人工智能工具，我来推荐下免费的数字人的工具并介绍下数字人的应用场景。</strong><img src="https://pic1.zhimg.com/80/v2-bba028ca96be13b3a0011090e2fa3182_720w.webp?source=1def8aca" alt="img">我的AI绘画作品随着人工智能和3D技术的不断发展，数字人的概念逐渐走进人们的视野。数字人是指通过计算机程序和3D技术创建的虚拟人物，其外貌、语言、行为等都可以根据设计者的要求进行定制。数字人短视频服务能将个人的数字化形象与声音进行高精度复刻，并定制专属的<a href="https://www.zhihu.com/search?q=AI%E6%95%B0%E5%AD%97%E4%BA%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">AI数字人</a>，生成的内容通常包含1-3分钟的视频以及20分钟的声音素材。此外，通过快捷的模板输入方式，<a href="https://www.zhihu.com/search?q=%E5%8F%AA%E9%9C%80%E4%B8%80%E5%88%86%E9%92%9F&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">只需一分钟</a>即可批量生成数字人视频，大幅度降低了近90%的制作成本与时间消耗。该服务在口播博主、企业家、医生、律师、个人情感<a href="https://www.zhihu.com/search?q=IP&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">IP</a>、信息流广告、短视频带货、<a href="https://www.zhihu.com/search?q=%E7%9B%B4%E6%92%AD%E7%94%B5%E5%95%86&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">直播电商</a>以及跨境电商等多个领域具有广泛的应用价值。<img src="https://picx.zhimg.com/80/v2-c79aa964f647d14b8a152ef55049dab3_720w.webp?source=1def8aca" alt="img">我的AI绘画作品<strong>一、免费数字人<strong><strong>需要注意的是以下优秀的数字人推荐虽然免费，但是仅限新<a href="https://www.zhihu.com/search?q=%E6%B3%A8%E5%86%8C%E7%94%A8%E6%88%B7&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">注册用户</a>，有时长限制，如果想要长时间制作，建议可开通会员或者注册新邮箱进行多次尝试</strong></strong>1、<a href="https://www.zhihu.com/search?q=DID%E6%95%B0%E5%AD%97%E4%BA%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">DID数字人</a><strong>D-ID自2017年成立以来，一直专注于提供高质量的A拟真人视频产品服务和开发。他可以帮助用户快速、便捷地创建自己的数字人。 这是一个免费的国外网站，只需要采用邮箱注册就可以了。<img src="https://picx.zhimg.com/80/v2-119141aa7ef2aa904ee979938b0a2b8f_720w.webp?source=1def8aca" alt="img">用户只需上传人像照片并输入所需内容，即可利用平台提供的AI语音机器人将其自动转换为语音。通过这种方式，我们能够合成出高度逼真的、仿佛真人开口说话的视频。此外，作为新用户注册，可以获赠20个积分作为欢迎礼。但是需要注意的是每个DID账号只能生成5分钟的视频，如果你还想免费试用，后期需多注册账号完成的视频效果可以看我制作的视频内容：<a href="https://www.zhihu.com/zvideo/1694029915765305344">如何用Ai工具让图片说话588 播放 · 14 赞同视频<img src="https://pic1.zhimg.com/v2-f1e1f036f9832b0f079f628f0c9744d2_r.jpg?source=2231c908" alt="点击可播放视频"></a>具体要怎么操作呢？我这里简单介绍下：1. 注册好账号后，点击 Create Video<img src="https://picx.zhimg.com/80/v2-65c3eece2e8ecb27d4a7d212527c56ba_720w.webp?source=1def8aca" alt="img">2. 分别有视频创作主播（Chioose a Presenter）和 AI数字人（Generate AI presenter）两个选项，根据需要选择<img src="https://picx.zhimg.com/80/v2-3e9c491b43200e79f1deee23969f723e_720w.webp?source=1def8aca" alt="img"><img src="https://pic1.zhimg.com/80/v2-1ebac222ebf32517e2375359155b39c9_720w.webp?source=1def8aca" alt="img">3.可以选择系统已经设置好的人员，或者也可以点击ADD自己上传图片上传，下图展示的是我自己用AI绘画生成的形象<img src="https://pica.zhimg.com/80/v2-f65549d34c2e759820c694f19cbdf7ea_720w.webp?source=1def8aca" alt="img">4. 可以选择用AI机器人说话，在右侧的Script下自己输入文字，还可以增加语气停顿等要求，Language选择Chinese，就可以选择中文啦，然后在Voices选择男女不同的<a href="https://www.zhihu.com/search?q=%E6%92%AD%E5%BD%B1%E5%91%98&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">播影员</a>名称。<img src="https://picx.zhimg.com/80/v2-4664933e9535470ba00263cb482837a7_720w.webp?source=1def8aca" alt="img">5.也可以直接上传你自己的音频文件，点击Audio上传6.最后选择GENERATE VIDEO合成最终视频。可以下载.<img src="https://picx.zhimg.com/80/v2-7e3f66da8bc01612e5e61f14c72e2c7f_720w.webp?source=1def8aca" alt="img">推荐了这个AI工具进行数字人制作，可以有效助力自媒体运营，但文案不会写、图片视频不会做、运营不会搞怎么办?<a href="https://www.zhihu.com/search?q=%E7%9F%A5%E4%B9%8E%E7%9F%A5%E5%AD%A6%E5%A0%82&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">知乎知学堂</a>最近推出了一门名为【AI 提效神器】的直播课，帮你打开思路，在这个工作方式大变革的时代，提升变现效率，减少信息差，该课程共计2小时，主要介绍了AI对各行各业工作模式的潜在影响，会教你怎么学会制作AI视频。建议对自媒体和人工智能感兴趣的初学者可以去试着听下这门课程，里面的视频课件介绍能够让你更好的了解各种AI工具的应用AI工具提效训练营🔥送工具精选+Prompt设计指南仅需0.1元 立即领取免费领取报名后别忘了添加老师，这4个资料对于想要利用ai来搞钱的人实在太有用了！<img src="https://picx.zhimg.com/80/v2-5314f45aef1dfb9cf88b4afada963586_720w.webp?source=1def8aca" alt="img">只要听完免费的课程就可以从知乎领取上图的那些资料，帮助自己更好地迎合AI时代的到来，规划自己的职业生涯，以应对AI带来的巨变时代。2、<a href="https://www.zhihu.com/search?q=HEYGEN%E6%95%B0%E5%AD%97%E4%BA%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">HEYGEN数字人</a><a href="https://www.zhihu.com/search?q=HeyGen&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">HeyGen</a>是一款先进的<a href="https://www.zhihu.com/search?q=AIGC&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">AIGC</a>产品，专门为用户提供宣传视频创建的支持。借助<a href="https://www.zhihu.com/search?q=Al%E6%95%B0%E5%AD%97%E4%BA%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">Al数字人</a>的技术，HeyGen能够生成具有专业水准的宣传视频。<img src="https://picx.zhimg.com/80/v2-17fc78506956669abc369e30bb7952f3_720w.webp?source=1def8aca" alt="img">值得一提的是，无论是背景素材还是解说人像，HeyGen均提供系统自带的资源，从而确保用户在免费版或付费版的使用中均无需担忧版权问题。此外，HeyGen的操作界面简洁直观，使用户能够轻松上手并高效完成视频制作。HeyGen的官方网站（需要非大陆IP才能登陆）<a href="https://link.zhihu.com/?target=http://heygen.com/">http://heygen.com/heygen.com/</a>操作步骤：1.注册账号,也可以直接用GOOGLE账号登陆<img src="https://pic1.zhimg.com/80/v2-610aef1dcfc9a81577c4f7f58715d6b0_720w.webp?source=1def8aca" alt="img">2.进入操作页面，点击“create video’<img src="https://picx.zhimg.com/80/v2-c44bf7a20fdc7bb31c0f8ea8b238135d_720w.webp?source=1def8aca" alt="img">3.上传图片定制数字人“Create Video”<a href="https://www.zhihu.com/search?q=%E9%A1%B5%E9%9D%A2%E5%B8%83%E5%B1%80&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">页面布局</a>清晰、直观，左侧提供了模板和头像选择的便捷途径，而右侧则集中了文本、声音等核心编辑功能，使视频制作过程既系统化又个性化。<img src="https://picx.zhimg.com/80/v2-dc854b43d2750f8dcb271b9b55fb17d2_720w.webp?source=1def8aca" alt="img">在头像选择上，系统提供了推荐的头像供用户<a href="https://www.zhihu.com/search?q=%E5%BF%AB%E9%80%9F%E9%80%89%E6%8B%A9&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">快速选择</a>，还允许用户上传自定义的虚拟头像。自定义头像的上传和应用流程简洁明了，用户只需几步操作即可完成。右侧的编辑区域提供了丰富的文本和声音编辑选项，使用户能够精细调整视频内容和风格。4.其他步骤基本与DID相同，不在赘述。</strong>HeyGen的优缺点：</strong>HeyGen的数字人像没有版权问题，为用户提供了合法、安全的使用环境。用户可自定义解说内容，仅需简单输入文字，即可将想法轻松转化为语音，降低了创作的难度。<img src="https://pica.zhimg.com/80/v2-20fca3021980d1fe4d3646d52f8b64aa_720w.webp?source=1def8aca" alt="img">我的AI绘画作品另外HeyGen系统自带丰富的解说声音库，提供几十种声音供用户选择，能满足不同风格和情境的需求。同时，用户还可根据需要调节视频动效的展现方式，使生成的视频更具个性化和专业感。视频输出方面，支持横屏和竖屏两种格式，并能输出高质量的4K视频，满足不同平台和场景的播放需求，进一步拓宽了应用范围。<img src="https://pic1.zhimg.com/80/v2-2ec958cdf860b5127099569b9f317c05_720w.webp?source=1def8aca" alt="img">我的AI绘画作品然而，正如任何产品都存在改进空间一样，HeyGen在<a href="https://www.zhihu.com/search?q=%E8%A7%86%E9%A2%91%E7%BC%96%E8%BE%91&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">视频编辑</a>功能上略显不足。目前，用户无法直接对生成的视频进行裁切，需要在下载后使用本地<a href="https://www.zhihu.com/search?q=%E8%A7%86%E9%A2%91%E5%89%AA%E8%BE%91%E8%BD%AF%E4%BB%B6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">视频剪辑软件</a>进行编辑。这一步骤虽然增加了后期处理的复杂性，但也为用户提供了更专业的视频编辑可能性。二、其他数字人信息我们需要认识到，定制的数字人是需要投入不小成本的。国内也有很有优秀的做数字人的公司，下面我就从我个人用户角度做了一些了解，列出我的个人收集到的一些信息观点，仅供各位参考。1. <a href="https://www.zhihu.com/search?q=%E7%A1%85%E5%9F%BA%E6%99%BA%E8%83%BD&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">硅基智能</a>（<a href="https://www.zhihu.com/search?q=%E7%A1%85%E8%AF%AD&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">硅语</a>APP）<img src="https://pic1.zhimg.com/80/v2-ec2c576aa35f3cdbfca37a5e5c26c46f_720w.webp?source=1def8aca" alt="img">图片来源于互联网产品优势：专注C端直播，擅长营销产品劣势：全国代理贴牌多；门槛不低，收费较多网站链接：<a href="https://link.zhihu.com/?target=https://www.guiji.ai/">guiji-owswww.guiji.ai/</a>2.<a href="https://www.zhihu.com/search?q=%E8%B5%9B%E7%81%B5%E5%8A%9B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">赛灵力</a><img src="https://picx.zhimg.com/80/v2-76a6d8074e66c629f4e4330e13b96283_720w.webp?source=1def8aca" alt="img">图片来源于互联网产品优势：专注3D数字人和互动数字人，主要GB端；2017年开始做数字人，6年技术积累，所有底层技术自研，技术行业顶尖；有<a href="https://www.zhihu.com/search?q=GPU&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">GPU</a>机房&#x2F;3D<a href="https://www.zhihu.com/search?q=%E9%98%B5%E5%88%97%E7%9B%B8%E6%9C%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">阵列相机</a>集群&#x2F;百万级拍摄室产品劣势：价格贵，但效果好网站链接：<a href="https://link.zhihu.com/?target=https://www.cyrinn.cn/">yuyin-officialwww.cyrinn.cn/<img src="https://pic2.zhimg.com/v2-1d5c2472466ad4307dfd8c4f40ffa705_180x120.jpg" alt="img"></a>3. 腾讯智影<img src="https://picx.zhimg.com/80/v2-542bb8ccc6b6fc88797fd4b74abbdb54_720w.webp?source=1def8aca" alt="img">产品优势：2022年下半年开始做数字人，定位换脸网站链接：<a href="https://link.zhihu.com/?target=https://zenvideo.qq.com/">腾讯智影-在线智能视频创作平台zenvideo.qq.com&#x2F;</a>4.奇妙元（北京出门问问）<img src="https://picx.zhimg.com/80/v2-9470fa27ede02875ee0c66cd94c8ebb2_720w.webp?source=1def8aca" alt="img">产品优势：做声音的公司，2023年初出数字人网站链接：<a href="https://link.zhihu.com/?target=https://www.weta365.com/asset/digital">https://www.weta365.com/asset/digitalwww.weta365.com/asset/digital</a>5.<strong>万兴播爆</strong><img src="https://pic1.zhimg.com/80/v2-4a67b805626d23482ba2a7232a27d3aa_720w.webp?source=1def8aca" alt="img">产品优势：出海，匹配海外用户的审美，更容易吸引到海外客户。网站链接：<a href="https://link.zhihu.com/?target=https://virbo.wondershare.cn/">【官网】万兴播爆-数字人短视频营销神器virbo.wondershare.cn&#x2F;<img src="https://pic4.zhimg.com/v2-f1658c0791c689387c929a904fba07f7_ipico.jpg" alt="img"></a>三、数字人短视频优劣势我个人认为<a href="https://www.zhihu.com/search?q=%E6%95%B0%E5%AD%97%E4%BA%BA%E6%8A%80%E6%9C%AF&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">数字人技术</a>为短视频制作带来了显著的优势，但是也存在一些局限性，下面总结下我的个人观点：<img src="https://pica.zhimg.com/80/v2-4784ee71874be92843269f7066183654_720w.webp?source=1def8aca" alt="img">我的AI绘画作品<strong>1. 优点：****成本效益显著</strong>：传统的短视频制作涉及设备、人力和后期编辑等多方面的费用。数字人技术则通过一次拍摄，后续利用音频生成视频的方式，大大降低了重复拍摄的成本，只需简单的剪辑即可上线，从而显著节省了短视频的制作成本。<strong>时间效率高</strong>：对于企业而言，时间成本至关重要。数字人技术允许企业领导或老板一次性塑造自己的形象，之后仅需定期录制音频，即可快速生成口播视频。这种方式不仅避免了长时间拍摄和NG造成的时间浪费，而且可以实现<a href="https://www.zhihu.com/search?q=%E9%AB%98%E9%A2%91%E6%9B%B4%E6%96%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">高频更新</a>，甚至日更多次。<strong>降低运营成本</strong>：以往，单个质量尚可的短视频的制作成本平均在500-1000元之间。要实现<a href="https://www.zhihu.com/search?q=%E6%97%A5%E6%9B%B4&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3341864025%7D">日更</a>，一个月的运营成本至少为1.5万元。相比之下，数字人短视频的制作成本大幅降低，足以支持持续半年的每日更新。<img src="https://picx.zhimg.com/80/v2-9d672130fccf68ba22f9c610c9ca21c0_720w.webp?source=1def8aca" alt="img">我的AI绘画作品<strong>2.缺点：****应用范围的限制</strong>：当前数字人技术主要适用于口播和口述类视频，对于需要丰富剧情和情感表达的视频内容，该技术尚无法满足。<strong>情感表达的固化</strong>：一旦数字人形象确定，其口播语言中的情感表达便固定下来，无法灵活调整。如需更改情感表达，需要重新塑造数字人形象。<strong>话语权的集中</strong>：目前数字人短视频更多地体现为企业老板或领导人的个人观点表达，这种内容与企业紧密相连，可能导致话语权过于集中，缺乏多样性。<img src="https://pic1.zhimg.com/80/v2-6a943da69eb4ca60193103f431fe2544_720w.webp?source=1def8aca" alt="img">我的AI绘画作品结语通过以上的介绍，你是否对数字人工具有了一个初步的了解呢?希望我的回答对你有帮助我是<a href="https://www.zhihu.com/search?q=%E5%BE%B7%E9%87%8C%E5%85%8B%E6%96%87&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3052677444%7D">德里克文</a>，一个对AI绘画，人工智能有强烈兴趣，从业多年的<a href="https://www.zhihu.com/search?q=%E5%AE%A4%E5%86%85%E8%AE%BE%E8%AE%A1%E5%B8%88&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3052677444%7D">室内设计师</a>！如果对我的文章内容感兴趣，请帮忙关注点赞收藏，谢谢！</p><p><a href="https://www.zhihu.com/question/579615043/answer/3341864025">发布于 2023-12-28 17:50</a></p><p>赞同 7120 条评论</p><p>分享</p><p>收藏喜欢</p><p>收起</p><p><a href="https://www.zhihu.com/people/liang-ye-feng-hua-han"><img src="https://pica.zhimg.com/v2-c23174313506bf6a0b6b0bba21086f26_l.jpg?source=1def8aca" alt="愿星有空"></a></p><p><a href="https://www.zhihu.com/people/liang-ye-feng-hua-han">愿星有空</a></p><p>什么时候放假呢</p><p> 关注</p><p>25 人赞同了该回答</p><p>有很多朋友在问，网上那种AI数字人是怎么制作的呀，甚至还想花钱买。<br>今天就手把手教大家，如何免费制作属于自己的数字人。</p><p>本文全文2000字，推荐阅读时间5分钟。<br>关键词：AI数字人，人工智能，自动口播</p><p>制作步骤非常简单，看完以下两个部分就能学会：<br>1制作过程介绍和<a href="https://www.zhihu.com/search?q=%E6%95%B0%E5%AD%97%E4%BA%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3360143421%7D">数字人</a>网站链接（记得收藏）<br>2一步步教你制作免费数字人<br>3补充说明</p><p><strong>1制作过程</strong></p><p>首先，制作数字人，需要确定一个人物形象，这就需要你提供一张人物图片。可以是自己的相片，也可以是动漫头像，或是使用AI绘图工具自动生成头像。<br>接着，就可以让AI工具根据头像，生成数字人视频。<br>下面介绍一下，制作过程中需要用到的各种工具和网站，建议收藏！<br>本文所有提及的网站我全部放到文末了，需要的自取~</p><p><strong>A制作头像</strong><br>可以使用QQ频道的<a href="https://www.zhihu.com/search?q=Midjourney&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3360143421%7D">Midjourney</a>，或者免费的leonardo.AI，绘图头像。<br>这里，我们再提供一个简单的<a href="https://www.zhihu.com/search?q=prompt&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3360143421%7D">prompt</a>，用于生成一个小男孩的头像，直接输入到AI绘图工具中，就可以绘制出人物头像，我们后续会用来生成AI数字人。</p><p>Prompt:[handsome boy](<a href="https://www.zhihu.com/search?q=handsome">https://www.zhihu.com/search?q=handsome</a> boy&amp;search_source&#x3D;Entity&amp;hybrid_search_source&#x3D;Entity&amp;hybrid_search_extra&#x3D;{“sourceType”%3A”answer”%2C”sourceId”%3A3360143421}),black hair, looking at the camera, portrait, [Pixar style](<a href="https://www.zhihu.com/search?q=Pixar">https://www.zhihu.com/search?q=Pixar</a> style&amp;search_source&#x3D;Entity&amp;hybrid_search_source&#x3D;Entity&amp;hybrid_search_extra&#x3D;{“sourceType”%3A”answer”%2C”sourceId”%3A3360143421}), 3d art,c4d rendering, <a href="https://www.zhihu.com/search?q=vivid&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3360143421%7D">vivid</a>, 8k resolution, super details, best quality,–iw 1</p><p><strong>B制作<a href="https://www.zhihu.com/search?q=%E6%95%B0%E5%AD%97%E4%BA%BA%E8%A7%86%E9%A2%91&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3360143421%7D">数字人视频</a></strong><br>有了头像，我们再使用<a href="https://www.zhihu.com/search?q=HeyGen&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3360143421%7D">HeyGen</a>生成数字人视频。<br>数字人的生成，是在HeyGen的官方网站中，制作过程也是在这个网站中进行。- AI Spokesperson Video Creato</p><p><strong>2一步步说明如何制作AI数字人</strong><br>废话不多说，接下来一步步告诉大家怎样制作免费的AI数字人：</p><p>七步解决：<br>A登录HeyGen数字人网站<br>B新建AI数字人视频<br>C上 传头像，确定数字人的形象<br>D上传文字稿<br>E选择语言风格<br>F生成AI数字人视频<br>G下载生成好的智能数字人视频</p><p>下面是逐步的图文教程：</p><p><strong>A登录HeyGen数字人网站</strong><br><img src="https://pic1.zhimg.com/80/v2-c560df44738f1ba5cde3e408812d80a6_720w.webp?source=1def8aca" alt="img"><br>网站可以使用Google账号登录。注册过GPT的应该都有Google，建议直接点击Google登录。（Google账号不会注册的，文末附有教程网址。）<br>如果你是小白，不会这些基础操作， 我建议你先花2个小时去看一下AI<a href="https://www.zhihu.com/search?q=%E6%99%BA%E8%83%BD%E5%8A%9E%E5%85%AC%E8%AF%BE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3360143421%7D">智能办公课</a>，这门课程正是针对当下最火的AI大模型工具推出的，主打如何利用AI提高办公场景下的工作效率。<br>内容覆盖了新媒体创作、内容整理、高效阅读、图片&#x2F;视频制作等9大办公场景，特别适合IT互联网、数据分析处理、文字编辑&#x2F;撰写等相关工作的职场人。<br>课程是由业内大佬主讲的，所以干货含量不用担心<br>AI工具提效训练营🔥送工具精选+Prompt设计指南仅需0.1元 立即领取免费领取<br>别忘了添加助教去领取<a href="https://www.zhihu.com/search?q=%E3%80%8AAI%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%AE%BE%E8%AE%A1%E6%8C%87%E5%8D%97%E3%80%8B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3360143421%7D">《AI提示词设计指南》</a>等资料。<br><img src="https://picx.zhimg.com/80/v2-6e2a46cc3972be47dd4669494e94546d_720w.webp?source=1def8aca" alt="img"><br><img src="https://pic1.zhimg.com/80/v2-fd93ba44da4fa336929b033311fcb747_720w.webp?source=1def8aca" alt="img"></p><p>下面我们接着讲：</p><p><strong>B新建AI数字人视频</strong><br>在上一步登录成功后，就会自动跳转进入下面的这个界面，如我框出来的位置，点击右上角的[Creat Video](<a href="https://www.zhihu.com/search?q=Creat">https://www.zhihu.com/search?q=Creat</a> Video&amp;search_source&#x3D;Entity&amp;hybrid_search_source&#x3D;Entity&amp;hybrid_search_extra&#x3D;{“sourceType”%3A”answer”%2C”sourceId”%3A3360143421}) （英文翻译的意思是：创建视频）即可创建数字人视频，正式开始制作。<br><img src="https://pic1.zhimg.com/80/v2-a9e4d8bd3c290fdb633f799ff742569c_720w.webp?source=1def8aca" alt="img"></p><p><strong>C上 传头像，确定数字人的形象</strong><br>AI生成AI数字人视频，需要你告诉他人物形象，也就是需要上传一个人物头像和人物图片。<br>上一步点击Creat Video 进后，会进入下面这个界面。<br>我们需要在头像模块，上传头像。<br>上传位置，我也用红色框框圈出来了。按下图所示，点击红色数字1和2的红色框框位置，就可以上传头像。<br><img src="https://pic1.zhimg.com/80/v2-8dedcc8dcc7304be933a2c541e0997ec_720w.webp?source=1def8aca" alt="img"><br>这里上传的头像就是提前用Midjourney，或者免费的leonardo.AI制作好的图片。<br>温馨提示，最好是正面的头像，不然嘴型怪怪的。</p><p><strong>D上传文字稿</strong><br>数字人要朗读和口播的内容，也需要用文字稿的形式上传到网站上。<br>上传的位置，就是上图中红色数字3的位置，点击开始导入文字后，就会进入下面图中的界面：<br><img src="https://picx.zhimg.com/80/v2-94baf3a97aad0b003b07e29b63a22fe5_720w.webp?source=1def8aca" alt="img"></p><p>在这个图片界面中，右下角可以选择语言（右下角红框位置）。<br>文字框内（界面中间的红色框位置），则是用来输入对应语言的文字稿，这样就会智能生成数字人口播视频。</p><p><strong>E选择语言风格</strong><br>如下图，按照自己对音色和风格的喜好，以及需要各种不同的语言，可以选择各种不同的声音和语言。比如下面的粤语，英语等等。如下所示，点击蓝色的select按钮即可选定。<br>右上角的播放按钮可以试听，<a href="https://www.zhihu.com/search?q=%E5%90%AC%E5%90%AC%E7%9C%8B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3360143421%7D">听听看</a>哪个是自己想要的风格。<br><img src="https://pic1.zhimg.com/80/v2-1ded330d10b6c61cd87815ab8b8f3401_720w.webp?source=1def8aca" alt="img"></p><p><strong>F生成AI数字人视频</strong></p><p>在上一步选择好数字人语言风格之后，会跳回D步骤的界面。<br><img src="https://pica.zhimg.com/80/v2-03c5ffaaa80d03d9ff565e418c1858dd_720w.webp?source=1def8aca" alt="img"><br>在头像模块（红色数字1所示位置），可以更换需要的头像。<br>文字输入模块（红色数字2所示位置），输入文字，点击播放按钮，可以预览智能<a href="https://www.zhihu.com/search?q=%E5%8F%A3%E6%92%AD&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3360143421%7D">口播</a>效果。不满意可以继续调整。如刚才步骤E所讲解的，重新更换语言风格。<br>确认没问题之后，点击右上角的Submit，即可提交任务，让HeyGen开始生成数字人视频。</p><p>这里需要注意一下，如果使用的是免费的版本，只能生成约1分钟的视频，且右下角会有HeyGen的水印。后面会进一步讲解怎样免费使用，以及收费的话是怎样算钱的。</p><p><strong>G下载生成好的智能数字人视频</strong><br><img src="https://pic1.zhimg.com/80/v2-bec754066d571ce9e1544a3349a6f11d_720w.webp?source=1def8aca" alt="img"><br>生成结束之后，就可以下载啦。刚Submit后，会需要等待视频生成的ready的时间。<br>点击下载按钮就可以下载了。（我是生成过一次，所以在主页的Video就能找到以往的记录）</p><p><strong>3补充说明</strong><br>如果需要增加时间且没有水印的话，可以充值。这里也贴出来大家可以看看如下：<br><img src="https://pica.zhimg.com/80/v2-f22b8abbf0916d98aa48b55fe2ac87ec_720w.webp?source=1def8aca" alt="img"><br>价格还是很贵的。</p><p>这里教大家另一个免费的方式。</p><p>在主界面，我红框框起来的地方，可以生成一条邀请链接。当你的朋友通过这个连接有效进入的时候，就会增加积分。<br><img src="https://pic1.zhimg.com/80/v2-bc6b0f7c3883b5d54b994fc28c3b9a05_720w.webp?source=1def8aca" alt="img"><br>我给大家翻译一下：<br><img src="https://picx.zhimg.com/80/v2-a736c60b33abee4775de3b6ec0811702_720w.webp?source=1def8aca" alt="img"></p><p>就是说邀请好友使用就可以获得免费积分，这样就能免费白嫖Heygen官方的AI工具制作数字人啦！<br>（而Google账号也能免费注册，所以可以多开几个号，白嫖积分，免费使用数字人工具）</p><p>这里附上HeyGen网址：<br><a href="https://link.zhihu.com/?target=http://heygen.com/">http://heygen.com</a></p><p>Google账号不会注册的，可以参考这个回答：<br><a href="https://zhuanlan.zhihu.com/p/652168639">https://zhuanlan.zhihu.com/p/652168639</a></p><p>不熟悉这些AI绘图工具的，可以看这篇《免费AI<a href="https://www.zhihu.com/search?q=%E7%BB%98%E5%9B%BE%E5%B7%A5%E5%85%B7&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3360143421%7D">绘图工具</a>》软件介绍，可以看这篇文章链接：<br><a href="https://zhuanlan.zhihu.com/p/620648084">https://zhuanlan.zhihu.com/p/620648084</a><br>本篇内容也感谢社群大佬黔总的分享！</p><p>邀请链接如下所示：（注意点击这个连接进去的，不是HeyGen！）<br>Hey friend, check out this awesome tool to make spokesperson videos by just typing!<br><a href="https://link.zhihu.com/?target=https://app.heygen.com/guest/templates?cid=fb4f4b2f">https://app.heygen.com/guest/templates?cid=fb4f4b2f</a></p><p><a href="https://www.zhihu.com/question/579615043/answer/3360143421">编辑于 2024-02-26 14:21</a></p><p>赞同 252 条评论</p><p>分享</p><p>收藏喜欢</p><p>收起</p><p><a href="https://www.zhihu.com/people/qing-shuang-shi-pin-bian-ji"><img src="https://pica.zhimg.com/v2-cb844286f6e4a4c9b83c3cc3b8e88c12_l.jpg?source=1def8aca" alt="视频编辑助手"></a></p><p><a href="https://www.zhihu.com/people/qing-shuang-shi-pin-bian-ji">视频编辑助手</a><img src="https://pic1.zhimg.com/v2-4812630bc27d642f7cafcd6cdeca3d7a.jpg?source=88ceefae" alt="img"></p><p>视频剪辑爱好者，新奇网站软件研究者</p><p> 关注</p><p>2 人赞同了该回答</p><p>目录AI当道！自从Sora<a href="https://www.zhihu.com/search?q=%E6%A8%AA%E7%A9%BA%E5%87%BA%E4%B8%96&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447971912%7D">横空出世</a>之后，AI视频赛道便收获了越来越多人的喜爱<del>其中<a href="https://www.zhihu.com/search?q=%E8%99%9A%E6%8B%9F%E6%95%B0%E5%AD%97%E4%BA%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447971912%7D">虚拟数字人</a>也是十分热门的一个类别。**看前排也已经有答主推荐了很多数字人工具的选择，那大鹏我就借助一款我平时用的比较多的数字人工具，来给大家详解一下虚拟数字人创建与制作的过程</del><strong><strong>『免费数字人工具』</strong></strong>✨<a href="https://www.zhihu.com/search?q=%E5%B8%83%E8%B0%B7%E9%B8%9F&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447971912%7D">布谷鸟</a>配音<strong>主打配音功能的这一专业转文字工具，便是今天数字人视频创建的主角<del><strong>软件配备的多项音视频类功能用于满足各种音频&#x2F;视频的创建、处理需求都相当到位。</strong><img src="https://pic1.zhimg.com/80/v2-afe2ed63a2179697a278f8850ddcb5cf_720w.webp?source=1def8aca" alt="img">内嵌的【AI<a href="https://www.zhihu.com/search?q=%E8%99%9A%E6%8B%9F%E4%B8%BB%E6%92%AD&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447971912%7D">虚拟主播</a>】功能就可用于高效、便捷地创建并制作出优质的数字人类视频作品</del><strong>『详细创建过程&amp;步骤』****第一步：切换到操作界面</strong>在打开软件过后，它默认呈现的便是</strong>【<a href="https://www.zhihu.com/search?q=%E6%99%BA%E8%83%BD%E9%85%8D%E9%9F%B3&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447971912%7D">智能配音</a>】<strong>的功能界面，这时我们就需求在左侧的功能中，点击</strong>【AI虚拟主播】<strong>切换到对应的操作界面。<img src="https://picx.zhimg.com/80/v2-982d5d0ec57f03c3de2e5e4178a3cca7_720w.webp?source=1def8aca" alt="img"><strong>第二步：导入<a href="https://www.zhihu.com/search?q=%E6%92%AD%E6%8A%A5%E6%96%87%E6%9C%AC&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447971912%7D">播报文本</a><strong>进入到操作区域后，我们就可以将事先准备好的播报文本内容粘贴到内容框中。一次性可输入多达3000字符数，基本算得上是一篇长篇文章&#x2F;报道的量了，</strong>日常一些简单的宣传视频稿子更是不在话下~</strong><img src="https://picx.zhimg.com/80/v2-c1c4b82ae39360c67bd16b08e2fa6eb2_720w.webp?source=1def8aca" alt="img"><strong>第三步：挑选主播形象&amp;播报音色</strong>紧接着便来到了至关重要的一步，这里大家就可以按自己的喜好和视频应用的场景来选择合适的主播形象以及对应的<a href="https://www.zhihu.com/search?q=%E6%92%AD%E6%8A%A5%E9%9F%B3%E8%89%B2&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447971912%7D">播报音色</a>。</strong>百余种播报音色任凭你随意选择，温柔年轻、活力魅力、东北老铁、<a href="https://www.zhihu.com/search?q=%E6%96%B0%E9%97%BB%E6%92%AD%E9%9F%B3%E8%85%94&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447971912%7D">新闻播音腔</a>等等，甚至各地方言和不同国家的语言都有<del><strong><img src="https://picx.zhimg.com/80/v2-7af9e8853c8074094616db487e13cbcc_720w.webp?source=1def8aca" alt="img"><strong>第四步：调整数字人位置&amp;更换背景</strong>完事了之后，再点击【虚拟人物设置】自定义更换背景，并设置好视频画面尺寸以及数字人位置。<img src="https://pic1.zhimg.com/80/v2-560a18b0c16f507363db829dde55fcf9_720w.webp?source=1def8aca" alt="img"><strong>第五步：挑选背景音乐</strong>到了这一步其实倒是可有可无，如果担心背景音乐会影响播报文字的清晰，也可以直接跳过；</strong>若是想让视频内容更加丰富饱满，便可以适当添加合适的音乐来衬托氛围。**<img src="https://pic1.zhimg.com/80/v2-bd7c5a036a353578f0816e375a867b19_720w.webp?source=1def8aca" alt="img"><strong>第六步：调节相关音频参数</strong>音乐和播报音色都确定了之后，就可以按需调节音量、语速、语调，主要还是要以自己觉得舒服的程度去调节，倒也没什么硬性规定。<img src="https://picx.zhimg.com/80/v2-54b671320463067e5b22d2f3b0339fec_720w.webp?source=1def8aca" alt="img"><strong>第七步：设置字幕</strong>为了方便大家观看，建议最好还是将字幕设置为<strong>【<a href="https://www.zhihu.com/search?q=%E6%98%BE%E7%A4%BA%E5%AD%97%E5%B9%95&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447971912%7D">显示字幕</a>】</strong>的状态，这样即使是在没法听声音的情况下，也能通过字幕来获取视频要传播的内容。<img src="https://picx.zhimg.com/80/v2-e6d888613b655d3aca2d341b76cb12f8_720w.webp?source=1def8aca" alt="img"><strong>第八步：试听&amp;<a href="https://www.zhihu.com/search?q=%E8%BD%AC%E6%8D%A2%E5%AF%BC%E5%87%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447971912%7D">转换导出</a><strong>到这一步也就是最后一步喽！点击右下角的</strong>【试听一下】</strong>就可以实时预览当前所创建好的数字人播报视频的效果，要是有任何不满意的地方就可以随时调整；满意的话，直接点击<strong>【开始转换】</strong>就可以保存导出到本地文件。<img src="https://pic1.zhimg.com/80/v2-499d6870adc7544dd8631ae553d7c296_720w.webp?source=1def8aca" alt="img">基本上【布谷鸟配音】这一免费数字人工具及其创建就是这样一个流程，操作起来并不难，大家也可以自己动手试试看</del>又是干货满满的分享，我看谁还没点赞收藏喜欢，有什么意见也可以在评论区直说 <a href="https://www.zhihu.com/people/63417af9dec64a7241e1465f2ecadf08">@视频编辑助手</a> 绝对欢迎！</p><p><a href="https://www.zhihu.com/question/579615043/answer/3447971912">发布于 2024-03-29 17:48</a></p><p>赞同 2添加评论</p><p>分享</p><p>收藏喜欢</p><p>收起</p><p><a href="https://hailuoai.com/?utm_source=zhihu&utm_campaign=%E6%B5%B7%E8%9E%BAAI-web-%E7%9F%A5%E4%B9%8E-%E6%B5%8B%E8%AF%95&_channel_track_key=CxG6QLZY"><img src="https://pic1.zhimg.com/v2-514206e55a95f7d9cfb5ec65f66273e8_xl.webp?source=d6434cab" alt="logo">海螺AI</a></p><p>广告</p><p>不感兴趣<a href="https://www.zhihu.com/promotion-intro">知乎广告介绍</a></p><p><a href="https://hailuoai.com/?utm_source=zhihu&utm_campaign=%E6%B5%B7%E8%9E%BAAI-web-%E7%9F%A5%E4%B9%8E-%E6%B5%8B%E8%AF%95&_channel_track_key=CxG6QLZY">效率起飞！海螺AI助你高效工作工作提效，海螺AI来帮忙！AI助手，加班克星，智能分析文档，一键生成PPT，让你的工作效率倍增，轻松应对工作挑战。查看详情</a></p><p><a href="https://www.zhihu.com/people/dlimeng"><img src="https://pic1.zhimg.com/v2-8daab7e30ca4b92a7c051a182261a4d1_l.jpg?source=1def8aca" alt="李孟聊AI"></a></p><p><a href="https://www.zhihu.com/people/dlimeng">李孟聊AI</a><img src="https://pica.zhimg.com/v2-4812630bc27d642f7cafcd6cdeca3d7a.jpg?source=88ceefae" alt="img"></p><p>大数据 | 人工智能 | AI生成 | SolidUI发起人</p><p> 关注</p><p>4 人赞同了该回答</p><p><img src="https://pic1.zhimg.com/80/v2-d1a45363f1ecf0e674e5bd2bdf21889d_720w.webp?source=1def8aca" alt="img"><a href="https://humanaigc.github.io/emote-portrait-alive/">https://humanaigc.github.io/emote-portrait-alive/</a><br>对我们来说，使用Stable Diffusion或Midjourney生成单张2D图像已不再是难题。然而，当我们尝试将这些连续生成的2D图像串联起来，以期望制作出流畅的视频时，问题就变得复杂了。在3D生成技术中，数字人（Avatar）技术一直备受关注。<strong>这项技术大致可以分为两大方向：数字人的创建和驱动。</strong>今天，我们将重点探讨<a href="https://www.zhihu.com/search?q=%E6%95%B0%E5%AD%97%E4%BA%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3413604179%7D">数字人</a>驱动方向的一项新进展。<strong>近日，阿里开源了一个名为EMO的项目：Emote Portrait Alive。****在传统技术下，生成的头像视频往往显得生硬、不自然，就像简单拼凑出来的一张张表情图片。</strong>但是，<a href="https://www.zhihu.com/search?q=EMO%E6%8A%80%E6%9C%AF&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3413604179%7D">EMO技术</a>的出现改变了这一切。<strong>EMO成功生成了会唱歌的头像视频，效果自然且生动。</strong>那么，EMO技术是如何实现的呢？它生成的头像视频效果又如何呢？我们一起探究下。技术实现<br><img src="https://picx.zhimg.com/80/v2-4aa1a7af6c2780ebc20d3cea7290ea52_720w.webp?source=1def8aca" alt="img"><a href="https://humanaigc.github.io/emote-portrait-alive/%E8%A6%81%E5%88%9B%E9%80%A0%E5%87%BA%E7%94%9F%E5%8A%A8%E9%80%BC%E7%9C%9F%E7%9A%84%E8%AF%B4%E8%AF%9D%E5%A4%B4%E9%83%A8%E8%A7%86%E9%A2%91%EF%BC%8CEMO%E6%A1%86%E6%9E%B6%E7%8B%AC%E8%BE%9F%E8%B9%8A%E5%BE%84%EF%BC%8C%E9%87%87%E5%8F%96%E4%BA%86%E7%9B%B4%E6%8E%A5%E9%9F%B3%E9%A2%91%E5%88%B0%E8%A7%86%E9%A2%91%E7%9A%84%E5%90%88%E6%88%90%E7%AD%96%E7%95%A5%E3%80%82**%E4%B8%8D%E5%90%8C%E4%BA%8E%E4%BC%A0%E7%BB%9F%E7%9A%84%E7%B9%81%E7%90%90%E6%B5%81%E7%A8%8B%EF%BC%8C%E5%AE%83%E6%91%92%E5%BC%83%E4%BA%863D%E6%A8%A1%E5%9E%8B%E6%88%96%E9%9D%A2%E9%83%A8[%E6%A0%87%E8%AE%B0%E7%82%B9](https://www.zhihu.com/search?q=%E6%A0%87%E8%AE%B0%E7%82%B9&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3413604179%7D)%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%8C%E8%BF%99%E7%A7%8D%E7%AE%80%E6%B4%81%E6%80%A7%E6%98%AF%E5%90%A6%E7%9C%9F%E8%83%BD%E8%BE%BE%E5%88%B0%E7%90%86%E6%83%B3%E7%9A%84%E6%95%88%E6%9E%9C%E5%91%A2%EF%BC%9F**EMO%E6%A1%86%E6%9E%B6%E7%9A%84%E6%A0%B8%E5%BF%83%E5%9C%A8%E4%BA%8E%EF%BC%8C%E5%AE%83%E8%BF%90%E7%94%A8%E4%BA%86%E4%B8%80%E4%B8%AA%E5%90%8D%E4%B8%BAStable">https://humanaigc.github.io/emote-portrait-alive/要创造出生动逼真的说话头部视频，EMO框架独辟蹊径，采取了直接音频到视频的合成策略。**不同于传统的繁琐流程，它摒弃了3D模型或面部[标记点](https://www.zhihu.com/search?q=标记点&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A3413604179})的使用，这种简洁性是否真能达到理想的效果呢？**EMO框架的核心在于，它运用了一个名为Stable</a> Diffusion（SD）的文本 到图像模型。这个模型能够神奇地将输入的音频信号与<a href="https://www.zhihu.com/search?q=%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3413604179%7D">图像特征</a>结合起来。<strong>但这里有一个疑问：音频与图像，这两种截然不同的数据形式，真的能够如此和谐地融合吗？</strong>事实上，SD模型借助<a href="https://www.zhihu.com/search?q=%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3413604179%7D">变分自编码器</a>（VAE）的力量，成功地将原始图像映射到潜在空间。这一过程不仅降低了计算的复杂性，还保证了图像的高清晰度。<strong>然而，单纯的映射并不足以生成动态的说话视频。</strong>EMO的巧妙之处在于，它引入了适量的噪声，并通过去噪过程逐步揭示出与音频相匹配的面部运动。为了验证这一创新方法的有效性，研究者们构建了一个庞大的音视频数据集。<strong>这个<a href="https://www.zhihu.com/search?q=%E6%95%B0%E6%8D%AE%E9%9B%86&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3413604179%7D">数据集</a>包含了超过250小时的视频和1.5亿张图像，涵盖了从演讲到歌唱表演的各种场景。如此丰富多样的数据，无疑为EMO提供了充足的学习资源。</strong><br><img src="https://picx.zhimg.com/80/v2-51fc9f4dd2649f6b0b795cd96772cfac_720w.webp?source=1def8aca" alt="img"><a href="https://arxiv.org/pdf/2402.17485.pdf">https://arxiv.org/pdf/2402.17485.pdf</a><br>但与此同时，一个显而易见的问题是：在众多现有的音视频合成方法中，EMO真的能够脱颖而出吗？为了回答这个问题，研究者们进行了一系列严格的实验和评估。<strong>令人惊讶的是，与当前最先进的几种方法相比，如DreamTalk、Wav2Lip和SadTalker，EMO在各项指标上都展现出了优越的性能。</strong>头像视频输入一张人像和一段音频，人像就能唱歌？只需单张人像图和一段唱歌的音频，我们的新方法就能生成表情丰富、头部姿态多变的虚拟歌手视频。<img src="https://picx.zhimg.com/v2-c24117cd10f11c8eeee45dc3c95beaed.jpg?source=382ee89a" alt="img">01:02<strong>视频时长随音频长短而定，人物特征始终保持一致。</strong>而且，这种方法支持多种语言和画像风格。无论哪种语言的歌曲，都能通过音频的音调变化，让人像生动起来。节奏快也不怕，我们的虚拟歌手能紧跟节奏，就算歌词再快，也能保证表情和动作的同步。<img src="https://pic1.zhimg.com/v2-38c4b02eeb47c2627b327041a7290493.jpg?source=382ee89a" alt="img">00:26<strong>除了唱歌，这种方法还能用于对话。</strong>无论是哪种语言的口语音频，都能让人像动起来。<strong>甚至，它还能让历史人物、画作人物、3D模型和AI生成内容“复活”。</strong>想象一下，电影角色用不同语言表演，会是什么样子？<img src="https://picx.zhimg.com/v2-c4fe06714e1f1722221ccadebb2bb931.jpg?source=382ee89a" alt="img">01:34我们的方法就能实现这一点，为角色扮演带来更多可能性。结语<strong>技术的边界在不断拓展，而我们的想象力是无尽的。<strong><strong>EMO的出现，不仅是<a href="https://www.zhihu.com/search?q=%E6%95%B0%E5%AD%97%E4%BA%BA%E6%8A%80%E6%9C%AF&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3413604179%7D">数字人技术</a>的一次突破，更是对未来<a href="https://www.zhihu.com/search?q=%E8%99%9A%E6%8B%9F%E8%A1%A8%E8%BE%BE%E6%96%B9%E5%BC%8F&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3413604179%7D">虚拟表达方式</a>的一次大胆预演。</strong></strong>在技术与创意的碰撞中，我们将共同见证一个更加丰富多彩的虚拟世界的诞生。</strong>EMO的成功是否意味着传统制作方式的终结？音频与图像的深度融合又将引领我们走向怎样的未来？欢迎留言讨论！我是 <a href="https://www.zhihu.com/people/b182157671aa37ef92f213e6bf361145">@李孟聊AI</a>，独立<a href="https://www.zhihu.com/search?q=%E5%BC%80%E6%BA%90%E8%BD%AF%E4%BB%B6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3413604179%7D">开源软件</a>开发者，SolidUI作者，对于新技术非常感兴趣，专注AI和数据领域，如果对我的文章内容感兴趣，请帮忙关注点赞收藏，谢谢！</p><p><a href="https://www.zhihu.com/question/579615043/answer/3413604179">发布于 2024-02-29 14:36</a></p><p>赞同 4添加评论</p><p>分享</p><p>收藏喜欢</p><p>收起</p><p><a href="https://www.zhihu.com/people/tianjiantong"><img src="https://picx.zhimg.com/v2-14333535ae1d31728f5da6a2ee66bbcf_l.jpg?source=1def8aca" alt="田建通"></a></p><p><a href="https://www.zhihu.com/people/tianjiantong">田建通</a><a href="https://www.zhihu.com/question/48510028"></a><img src="https://picx.zhimg.com/v2-4812630bc27d642f7cafcd6cdeca3d7a.jpg?source=88ceefae" alt="img"></p><p>互联网行业 总经理</p><p> 关注</p><p><strong>▲务必关注上方“<a href="https://www.zhihu.com/search?q=%E7%94%B0%E5%BB%BA%E9%80%9A&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3363485195%7D">田建通</a>”避免找不到这篇文章▲</strong>听过老田声音的朋友，你来听听这是不是我的声音，这种免费克隆声音的方法你想不想学习使用？想学会就看是如何一步步制作出来的！<br><em><strong>01*软件</strong><br><strong>1、ChatGPT</strong>这里就不详细介绍了，可以说Chat GPT是文本处理的天花板，出现至今还不曾被超越过，条件允许可以用付费版（Plus、4.0），3.5和4.0的区别请点击下方蓝色链接。</em><em>一张图说明白ChatGPT3.5与<a href="https://www.zhihu.com/search?q=4.0plus&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3363485195%7D">4.0plus</a>的联系和区别</em>*<br>介绍太多次了，如果你没有，可以自行&#x2F;委托注册&#x2F;购买账号，也可以用国内的平替软件（国内版AI答）。<br><img src="https://pic1.zhimg.com/80/v2-ee6027c300254b60cec53706ce760c08_720w.webp?source=1def8aca" alt="img"><br><strong>2、<a href="https://www.zhihu.com/search?q=%E8%85%BE%E8%AE%AF%E6%99%BA%E5%BD%B1&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3363485195%7D">腾讯智影</a></strong><br>这个也介绍不少次了，我还出过一条18分钟的手把手<a href="https://www.zhihu.com/search?q=%E6%95%99%E5%AD%A6%E8%A7%86%E9%A2%91&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3363485195%7D">教学视频</a>，见下方，还没注册的抓紧注册，通过这个链接注册还有高级会员权益领取：<br><strong>【全长18分钟】手把手教会你<a href="https://www.zhihu.com/search?q=%E6%95%B0%E5%AD%97%E4%BA%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3363485195%7D">数字人</a>制作保姆级教程</strong>活动链接：<a href="https://link.zhihu.com/?target=https://zenvideo.qq.com/activity/commission/invitee?commission_code=9GldDGnSKuxjXMWe">https://zenvideo.qq.com/activity/commission/invitee?commission_code&#x3D;9GldDGnSKuxjXMWe</a><strong>将上方蓝色字体复制到浏览器上进行注册享受福利。</strong><br>**3、<a href="https://www.zhihu.com/search?q=%E5%89%AA%E6%98%A0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3363485195%7D">剪映</a>**这个也可以说是非常亲民的一款视频剪辑制作工具，不过还是有一些朋友不会使用，我这次也给你带来了免费教程，这个教程共26集视频，每条视频都是几秒钟，特别适合你利用碎片化时间来学习。<br>当然你也没必要一条条按顺序学习，需要用到哪个了就学哪个，也完全可以，这里仅放第一集和最后一集，如果你想学习直接关注视频号，随时可以看，完全可以根据标题进行学习起来。<br>**4、<a href="https://www.zhihu.com/search?q=%E8%B1%86%E5%8C%85&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3363485195%7D">豆包</a>**这个是我第一次提到的软件，但你可别小看它，它是字节跳动推出的AI软件，正是它能克隆你的声音，具体操作步骤，继续往下看就能学会，而且现在完全免费。<br><img src="https://picx.zhimg.com/80/v2-5c6f88e1c427d543ce62076257ebb3a8_720w.webp?source=1def8aca" alt="img"></p><p>*<strong>02*制作流程</strong><br>1、先选题，即找到你制作视频的视频选题；<br>2、通过Chat GPT撰写视频文案；<br>3、通过豆包克隆自己的声音；4、让豆包用自己克隆的声音来读文本；5、将豆包生成的语音保存为<a href="https://www.zhihu.com/search?q=%E9%9F%B3%E9%A2%91%E6%96%87%E4%BB%B6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3363485195%7D">音频文件</a>；6、通过腾讯智影选择合适的数字人形象来合成形象和声音；7、通过剪映来添加字幕、封面、音乐等内容完成数字人播报视频制作。</p><p>*<strong>03*变注意事项</strong><br>1、克隆自己声音时需找个安静的环境；2、克隆自己声音时用自己正常说话的声音，不能有播音腔和<a href="https://www.zhihu.com/search?q=%E6%9C%97%E8%AF%B5%E8%85%94&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3363485195%7D">朗诵腔</a>；<br>3、如果对克隆的声音不满意，可以进行多次尝试，选择自己最满意的克隆声音。</p><p><em><strong>04*组队福利</strong>如果你对制作数字人有兴趣，欢迎加我微信组队进入数字人交流群（我的微信在下方链接中，请务必注明来意，并进行简单介绍，不保证加我微信都能进群，多谢理解）。</em><em>在加我微信前请务必看如下链接和视频：</em>*<br><strong>点击链接和田建通交个朋友</strong><br>觉得这篇文章对你有用，务必关注本号，更欢迎在文章右下角点个在看，也分享给你的好朋友看看，你还想看哪一篇不妨留言告诉我，我优先发出来。<strong>融入科技革命新时代，<strong><strong>和我一同开启这段精彩的AI之旅，</strong></strong>玩赚AI，共创AI新时代！****一直关注一直有惊喜福利</strong></p>]]></content>
      
      
      <categories>
          
          <category> 实用教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ai模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>几个跟数字人项目相关的开源大模型</title>
      <link href="/2024/04/18/%E5%90%83%E7%93%9C%E6%9D%82%E8%B0%88/%E5%87%A0%E4%B8%AA%E8%B7%9F%E6%95%B0%E5%AD%97%E4%BA%BA%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3%E7%9A%84%E5%BC%80%E6%BA%90%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
      <url>/2024/04/18/%E5%90%83%E7%93%9C%E6%9D%82%E8%B0%88/%E5%87%A0%E4%B8%AA%E8%B7%9F%E6%95%B0%E5%AD%97%E4%BA%BA%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3%E7%9A%84%E5%BC%80%E6%BA%90%E5%A4%A7%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>目前很多大模型如雨后春笋般涌现出来，都有点心慌了。冷静下来还是需要一个个去识别哪些对自己有用。</p><p><strong>AI Voice Chat</strong></p><p><a href="https://link.zhihu.com/?target=https://github.com/WeberJulian/AI-voice-chat">https://github.com/WeberJulian/AI-voice-chat</a></p><p>它是一个简化版的react app，可以用自己的语音跟chatGPT 语音聊天。</p><p>它使用Whisper Large v3来转录，使用openchat 3.5 AWQ作为语言助手，XTTS v2用来文本转语音。</p><p>它的优势是语言对语音的几乎无延迟特性。运行在RTX 3090 GPU上。</p><p><strong>Diffusion Avatars</strong></p><p><a href="https://link.zhihu.com/?target=https://tobias-kirschstein.github.io/diffusion-avatars/">https://tobias-kirschstein.github.io/diffusion-avatars/</a>（代码还在开发中）</p><p>它用来合成一个高保真的3D头像，提供对姿势和表情的控制。</p><ul><li>将表情传输到3D头像动画</li></ul><p><img src="https://pic2.zhimg.com/80/v2-fc1491ed319129ac1727ec9a8aab7949_720w.webp" alt="img"></p><ul><li>通过 NPHM 制作头像动画</li></ul><p>通过底层 NPHM 进行控制。我们通过在几个目标表达式之间进行插值来获得表达式代码 z exp 。使用光栅化和我们基于扩散的神经渲染器，表达代码被转换为具有视点控制的现实化身。</p><p><img src="https://pic1.zhimg.com/80/v2-386fd958e7f830f78931c809e0755f74_720w.webp" alt="img"></p><ul><li>自己制作头像动画</li></ul><p><img src="https://pic2.zhimg.com/80/v2-1782321d2d83e5c7c17250d2433f9a35_720w.webp" alt="img"></p><p>通过拖动蓝色点到相应的表情，完成3D人物表情的改变，中间的过渡很流畅。</p><p><strong>PoseGPT: 通过对话的方式实时生成3D人物姿态</strong></p><p><a href="https://link.zhihu.com/?target=https://yfeng95.github.io/posegpt/">https://yfeng95.github.io/posegpt/</a>（代码还在开发中）</p><p><img src="https://pic3.zhimg.com/80/v2-00aea1df0ec89fc3140785ba695fd1f6_720w.webp" alt="img"></p><p>这是一个采用大型语言模型 (LLM) 来从图像或文本描述中理解和推理 3D 人体姿势的框架。源于人类从单个图像或简短描述中直观地理解姿势的能力，这是一个将图像解释、世界知识和对肢体语言的理解交织在一起的过程。传统的人体姿势估计方法，无论是基于图像的还是基于文本的，通常缺乏整体场景理解和细致入微的推理，导致视觉数据与其现实世界含义之间的脱节。PoseGPT 通过将 SMPL 姿势嵌入为多模态 LLM 中的独特信号标记来解决这些限制，从而能够从文本和视觉输入直接生成 3D 身体姿势。从而促进两项高级任务：推测性姿势生成和姿势估计推理。这些任务涉及推理人类从微妙的文本查询中生成 3D 姿势，可能还伴有图像。我们为这些任务建立了基准，超越了传统的 3D 姿态生成和估计方法。此外，PoseGPT 能够基于复杂推理理解和生成 3D 人体姿势，为人体姿势分析开辟了新的方向。</p><p><strong>Animate Anyone</strong></p><p><a href="https://link.zhihu.com/?target=https://humanaigc.github.io/animate-anyone/">https://humanaigc.github.io/animate-anyone/</a></p><p><img src="https://pic2.zhimg.com/80/v2-9774dd41452f81e0577f7a9a050d7435_720w.webp" alt="img"></p><p>这个模型实现从静止图像到人物动态视频。这个用来实现角色动画的一致性。引入了有效的姿势引导器来指导角色的运动，并采用有效的时间建模方法来确保视频帧之间平滑的帧间过渡。</p><p><strong>最后分享两个从文本到3D模型的流程：</strong></p><p><a href="https://link.zhihu.com/?target=https://3d.csm.ai/">https://3d.csm.ai/</a> 用来图片生成3D模型。</p><p><a href="https://link.zhihu.com/?target=https://www.krea.ai/">https://www.krea.ai/</a> 用来做参考制作理想模型</p><p><a href="https://link.zhihu.com/?target=https://magnific.ai/">https://magnific.ai/</a> 用来将图片更加清晰</p><p><a href="https://link.zhihu.com/?target=https://runwayml.com/">https://runwayml.com/</a> 做成动画</p><p><strong>图生成法向图导入Blender处理：</strong></p><p><a href="https://link.zhihu.com/?target=https://huggingface.co/spaces/flamehaze1115/Wonder3D-demo">https://huggingface.co/spaces/flamehaze1115/Wonder3D-demo</a></p><p><img src="https://pic4.zhimg.com/80/v2-730c7b2f06a6c90a4914c52ebf89b923_720w.webp" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> 吃瓜杂谈 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ai模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI数字人离线版来了，解压即用！</title>
      <link href="/2024/04/18/AI%E6%95%B0%E5%AD%97%E4%BA%BA%E7%A6%BB%E7%BA%BF%E7%89%88%E6%9D%A5%E4%BA%86%EF%BC%8C%E8%A7%A3%E5%8E%8B%E5%8D%B3%E7%94%A8%EF%BC%81/"/>
      <url>/2024/04/18/AI%E6%95%B0%E5%AD%97%E4%BA%BA%E7%A6%BB%E7%BA%BF%E7%89%88%E6%9D%A5%E4%BA%86%EF%BC%8C%E8%A7%A3%E5%8E%8B%E5%8D%B3%E7%94%A8%EF%BC%81/</url>
      
        <content type="html"><![CDATA[<p>AI数字人发展如火如荼，之前给大家介绍过的AI生成数字人的工具<strong>SadTalk</strong>，可以免费无限制作专属AI数字人。然而，如何本地部署始终是个难题，步骤太多，劝退了很多小伙伴。没关系，经过我的不懈努力，终于制作了一款免部署，<strong>解压即用的AI数字人离线版</strong>，快随我去看看吧~</p><p><img src="https://pic4.zhimg.com/80/v2-85e9a74fec2bd62dfc2eac98a3b64333_720w.jpg" alt="img"></p><h3 id="离线版SadTalk"><a href="#离线版SadTalk" class="headerlink" title="离线版SadTalk"></a><strong>离线版SadTalk</strong></h3><p>之前介绍过的制作数字人的工具SadTalk，一般是需要先部署Stable Diffuison，想尝试自己走一遍流程的朋友，可以参考我以前的文章：</p><p><a href="https://link.zhihu.com/?target=http://mp.weixin.qq.com/s?__biz=MzAxMzA3MTQwMQ==&mid=2655058899&idx=1&sn=d2fc39972586c2ae5d9871bf80049669&chksm=801c7e95b76bf7838d880462d4247a0310fcb3c9c2b7b18161e59515b891e695ea20152fbc2c&scene=21%23wechat_redirect">AI一键让照片说话，无限且免费！</a></p><p><a href="https://link.zhihu.com/?target=http://mp.weixin.qq.com/s?__biz=MzAxMzA3MTQwMQ==&mid=2655060305&idx=1&sn=c85290a94c9c5689770e8f76a45a796b&chksm=801d8417b76a0d01d4a20701ef301c86f0fa592577fb1412bb2450753ab7447a749c85dcb71b&scene=21%23wechat_redirect">免费无限制作专属AI数字人</a></p><p>特别注意，需要在电脑里提前安装好<strong>FFmpeg</strong>，这是一套可以用来记录、转换数字音频、视频，并能将其转化为流的开源计算机程序，是SadTalker运行的必要支持程序。可以从以下网址下载获得：</p><p><a href="https://link.zhihu.com/?target=https://www.gyan.dev/ffmpeg/builds/ffmpeg-git-full.7z">https://www.gyan.dev/ffmpeg/builds/ffmpeg-git-full.7z</a></p><p>解压后，需要将FFmpeg的bin文件夹路径添加到系统环境变量里：</p><p><img src="https://pic1.zhimg.com/80/v2-874d3efe651a7e852886e818fd36c748_720w.webp" alt="img"></p><p>手动添加环境变量：控制面板→系统→高级系统设置→环境变量→Path→编辑→添加</p><p><img src="https://pic2.zhimg.com/80/v2-52dc3eb4f2c52b21fa317578dbf79e61_720w.webp" alt="img"></p><p>切入正题，今天分享给大家的是，解压即用的本地离线包，下载到本地，点击“一键启动”，即可无脑使用：</p><p><img src="https://pic4.zhimg.com/80/v2-efd5fb66eccc555d2b824bbf95cae66b_720w.webp" alt="img"></p><p>之后会打开命令提示窗口，等待片刻，看到弹出网址“<a href="https://link.zhihu.com/?target=http://127.0.0.1:7860/">http://127.0.0.1:7860/</a>”后，证明已经运行成功。</p><p><img src="https://pic1.zhimg.com/80/v2-1d5ae5020c7cc3980393f923464b2e60_720w.webp" alt="img"></p><p>将网址复制一下，然后在浏览器打开，即可看到SadTalk制作数字人的主界面：</p><p><img src="https://pic1.zhimg.com/80/v2-0b89c7d6ae66f532e410bdcc1f800380_720w.webp" alt="img"></p><h3 id="试试效果"><a href="#试试效果" class="headerlink" title="试试效果"></a><strong>试试效果</strong></h3><p>依然采用王家卫的电影台词+AI生成的美女图片，制作数字人视频：</p><p>台词选取自《阿飞正传》：</p><p><em>❤1960年4月16日下午3时之前的1分钟，你同我在一起。因为我，你会记得那一分钟。从现在开始，我们就是一分钟的朋友。这个事实，你改变不了，因为已经完成了。</em>﻿</p><p><em>❤我听别人说这世界上有一种鸟是没有脚的，它只能够一直的飞呀飞呀，飞累了就在风里面睡觉，这种鸟一辈子只能下地一次，那一次就是它死亡的时候。</em>﻿</p><p><em>❤以前我以为有一种鸟一开始就会飞，飞到死亡的那一天才落地。其实它什么地方也没去过，那鸟一开始就已经死了。我曾经说过不到最后一刻我也不会知道最喜欢的女人是谁，不知道她现在在干什么呢？天开始亮了，今天的天气看上去不错，不知道今天的日落会是怎么样的呢？</em>﻿</p><p><em>❤我这一生都不知道还会喜欢多少个女人，不到最后我是不知道哪个才是我最喜欢的。</em></p><p>将上述台词，生成语音并下载备用（语音生成网站：<a href="https://link.zhihu.com/?target=https://ttsmaker.cn">https://ttsmaker.cn</a>）。</p><p>上传图片及语音，并设置合适的参数，点击“生成”按钮。</p><p><img src="https://pic3.zhimg.com/80/v2-b9cf550332bb5067980abdbfe3259756_720w.webp" alt="img"></p><p>最终得到的数字人视频如下：</p><p><img src="https://picx.zhimg.com/v2-f1aa94ad9b48f508a316231d3934f6cf.jpg?source=382ee89a" alt="img"></p><p>01:01</p><p>I以上就是今天的全部内容了，我会持续研究有趣有用的AI项目，并分享给大家。感兴趣的朋友快去试试吧，记得关注并三连支持下哦!</p><p>公众号回复【<strong>AI数字人离线版</strong>】，免费获得解压即用的AI数字人离线安装包~</p>]]></content>
      
      
      <categories>
          
          <category> 实用教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 小玩意儿 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI二维码,真的太好看了!ai二维码制作方法分享</title>
      <link href="/2024/04/18/AI%E4%BA%8C%E7%BB%B4%E7%A0%81-%E7%9C%9F%E7%9A%84%E5%A4%AA%E5%A5%BD%E7%9C%8B%E4%BA%86-ai%E4%BA%8C%E7%BB%B4%E7%A0%81%E5%88%B6%E4%BD%9C%E6%96%B9%E6%B3%95%E5%88%86%E4%BA%AB/"/>
      <url>/2024/04/18/AI%E4%BA%8C%E7%BB%B4%E7%A0%81-%E7%9C%9F%E7%9A%84%E5%A4%AA%E5%A5%BD%E7%9C%8B%E4%BA%86-ai%E4%BA%8C%E7%BB%B4%E7%A0%81%E5%88%B6%E4%BD%9C%E6%96%B9%E6%B3%95%E5%88%86%E4%BA%AB/</url>
      
        <content type="html"><![CDATA[<p>这段时间定制高级 AI 艺术二维码的价格一路飙升，最高已至 2w 制作一张。经过我们的努力研发，简化了各种繁琐的制作流程，真正做到了艺术二维码人人可生成且真的很艺术！</p><h2 id="一、AI二维码效果"><a href="#一、AI二维码效果" class="headerlink" title="一、AI二维码效果"></a><strong>一、AI二维码效果</strong></h2><p>如下请欣赏</p><p><img src="https://pic3.zhimg.com/80/v2-b4bcb9c99fdf66620fa6ea02a6acdeb2_720w.webp" alt="img"></p><p>美女二维码</p><p><img src="https://pic3.zhimg.com/80/v2-1b86e4a55fdcf27a2bc954e5eb5e43aa_720w.webp" alt="img"></p><p>风景二维码</p><p><img src="https://pic2.zhimg.com/80/v2-43f3a1cd0b7168b2764aa3301848a955_720w.webp" alt="img"></p><p>专属二维码</p><p><img src="https://pic3.zhimg.com/80/v2-edc9651077f8a09c42ac6d2dbe3431ce_720w.webp" alt="img"></p><p>二维码设计</p><p><img src="https://pic4.zhimg.com/80/v2-b019cc7414e929c00d7ce2a8e0526447_720w.webp" alt="img"></p><p>二维码美女</p><p><img src="https://pic1.zhimg.com/80/v2-7d30088a5bfb02ee8e5a3d6950e25c2c_720w.webp" alt="img"></p><p>立体二维码</p><p><img src="https://pic1.zhimg.com/80/v2-ea6e4c374fd0c0ee8b0c6625aa51b8dc_720w.webp" alt="img"></p><p>二维码美化</p><p>从现在起，无须再花高价去定制，无须再付费去学习。简单便宜即可批量生产高级 AI 二维码。</p><p>即使你是从没接触过 AI 绘画的小白、不了解二维码的原理，只需按照我们下面列出的教程，就能立即上手～</p><p><a href="https://link.zhihu.com/?target=https://pan.quark.cn/s/991e4ab6ebc6">AI二维码美化教程pan.quark.cn&#x2F;s&#x2F;991e4ab6ebc6</a></p><p><strong>二、AI艺术二维码使用场景</strong></p><p>二维码目前已经广泛应用于各行各业，不管在网上冲浪，或是在线下各种场合，我们都随处可见二维码的应用。</p><p>当人们已经看腻千篇一律的黑白二维码，是时候把你的品牌二维码更换为结合 AI 的艺术二维码，彻底颠覆二维码体验，重新定义个人与品牌形象，留下令人深刻的印象！</p><p>下面列举了一些AI二维码基础使用场景，具体的场景还有很多很多，等待你去探索。</p><p><img src="https://pic1.zhimg.com/80/v2-7bf179b91cb1e6aa043289197546ad8c_720w.webp" alt="img"></p><h2 id="三、如何通过AI美化二维码进行引流和变现？"><a href="#三、如何通过AI美化二维码进行引流和变现？" class="headerlink" title="三、如何通过AI美化二维码进行引流和变现？"></a><strong>三、如何通过AI美化二维码进行引流和变现？</strong></h2><p>你可以在闲鱼、<strong>抖音</strong>和<strong>小红书</strong>等平台上发布作品进行推广，找到与你类似的账号进行模仿发布即可。吸引用户扫描二维码并添加你的微信，然后开始变现。你还可以将自己的二维码制作成艺术品，然后加入各种微信群进行推广。</p><p><strong>变现方式：</strong></p><p>直接帮助用户制作二维码。由于这项操作具有一定门槛，许多人不会操作。如果有人对此感兴趣，他们自然会选择付费。目前市场价每张二维码可以为9.9至29.9元不等。</p><p>当然，你也可以自己制作教程并销售。通过录制一份教程并销售，每份售价99至199元，轻松实现零本投入、万倍利润。 还可以提供收徒培训。</p><p>对于那些不仅不懂AI二维码美化，也不懂推广和引流的人，你可以提供AI二维码制作方法一对一指导和实操经验分享，并收费299至599元。</p><p>AI二维码美化教程：**<a href="https://link.zhihu.com/?target=http://pan.quark.cn/s/395b49cf23e6">http://pan.quark.cn/s/395b49cf</a>**</p>]]></content>
      
      
      <categories>
          
          <category> 最新资讯 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 小玩意儿 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>普通人要怎么学习AIGC？</title>
      <link href="/2024/03/31/%E6%99%AE%E9%80%9A%E4%BA%BA%E8%A6%81%E6%80%8E%E4%B9%88%E5%AD%A6%E4%B9%A0AIGC%EF%BC%9F/"/>
      <url>/2024/03/31/%E6%99%AE%E9%80%9A%E4%BA%BA%E8%A6%81%E6%80%8E%E4%B9%88%E5%AD%A6%E4%B9%A0AIGC%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="什么是AIGC？"><a href="#什么是AIGC？" class="headerlink" title="什么是AIGC？"></a>什么是AIGC？</h1><p>AIGC的全称是AI generated content，其中AI就是人工智能，GC就是创作内容；简单来说就是利用<a href="https://www.zhihu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3443055912%7D">人工智能技术</a>自动生成各种类型的内容：<strong>文本、图像、声音、视频、游戏以及<a href="https://www.zhihu.com/search?q=%E8%99%9A%E6%8B%9F%E4%BA%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3443055912%7D">虚拟人</a>等。</strong></p><p><strong>要举个例子的话，ChatGPT就是AIGC行业发展的一个绝佳范本。</strong></p><p><img src="https://picx.zhimg.com/80/v2-c31f6866e06747648c19f298f74d3257_720w.webp?source=1def8aca" alt="img"></p><p>ChatGPT是OpenAI研发的一款<a href="https://www.zhihu.com/search?q=%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3443055912%7D">聊天机器人</a>程序，它是人工智能技术驱动的<a href="https://www.zhihu.com/search?q=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3443055912%7D">自然语言处理</a>工具，它能够基于在预训练阶段所见的模式和统计规律，来生成回答，还能根据聊天的上下文进行互动，真正像人类一样来聊天交流，甚至能完成撰写论文、邮件、脚本、文案、翻译、代码等任务。</p><p><img src="https://picx.zhimg.com/80/v2-bdb853b0bc4620c2df94415ff2df423d_720w.webp?source=1def8aca" alt="img"></p><p>当然，除了ChatGPT，国内的AIGC也发展强劲，涌现了一批又一批好用的AI工具，在这里可以跟大家分享几款：</p><h2 id="✨文本生成："><a href="#✨文本生成：" class="headerlink" title="✨文本生成："></a><strong>✨文本生成：</strong></h2><h3 id="AI创意生成家"><a href="#AI创意生成家" class="headerlink" title="AI创意生成家"></a><strong>AI创意生成家</strong></h3><p>这是一款主打创意文本生成的AI工具，但它不止可以写作，还可以AI聊天、AI绘画、AI音视频总结，翻译，特效和识别等的功能，可以覆盖帮助工作生活里更多的场景，提高文本生成的效率~</p><p><img src="https://picx.zhimg.com/80/v2-d5cc478335886093434e231832406caf_720w.webp?source=1def8aca" alt="img"></p><p>AI写作的板块，多达上百种写作场景，从新媒体、新闻稿、作文论文、活动复盘、测评文章、文言文、散文、诗句创作等，不同类型风格的文案都可以输出。</p><p><img src="https://pica.zhimg.com/80/v2-8011af44a8c24824e2aefa7308a35666_720w.webp?source=1def8aca" alt="img"></p><p>写一篇论文大纲：</p><p><img src="https://picx.zhimg.com/80/v2-84f41303e505751fa805a98b0f859801_720w.webp?source=1def8aca" alt="img"></p><p><strong>操作步骤：</strong></p><blockquote><p>点击【AI写作】开始体验；<br>选择对应的场景模板；<br>输入主题或关键词；<br>输入正文的关键词描述；</p></blockquote><h2 id="✨图像生成："><a href="#✨图像生成：" class="headerlink" title="✨图像生成："></a><strong>✨图像生成：</strong></h2><h3 id="造梦日记"><a href="#造梦日记" class="headerlink" title="造梦日记"></a><strong><a href="https://www.zhihu.com/search?q=%E9%80%A0%E6%A2%A6%E6%97%A5%E8%AE%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3443055912%7D">造梦日记</a></strong></h3><p>它的原名其实很像一款游戏，叫<a href="https://www.zhihu.com/search?q=%E7%9B%97%E6%A2%A6%E5%B8%88&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3443055912%7D">盗梦师</a>，后来更名为造梦日记。它是西湖心辰联合<a href="https://www.zhihu.com/search?q=%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3443055912%7D">西湖大学</a>研发的一款AI绘画工具，覆盖多模态模型训练和图像生成，包括二次元头像生成、图片设计，可应用于绘画、动漫游戏、运营策划和电商等领域，人人都可实现自己的创作梦。</p><p><img src="https://picx.zhimg.com/80/v2-e920bd3173536330edb4de51907aa7aa_720w.webp?source=1def8aca" alt="img"></p><p>造梦日记有两种画板模式可以选择——造梦画板和ControlNet画板，造梦画板就是利用文本生成图片的模型，<a href="https://www.zhihu.com/search?q=%E8%BE%93%E5%85%A5%E6%96%87%E6%9C%AC&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3443055912%7D">输入文本</a>后可以选择不同的风格滤镜。如下图所示：</p><p><img src="https://picx.zhimg.com/80/v2-43a0e88403fc0fdbdb1503b6368bde20_720w.webp?source=1def8aca" alt="img"></p><p>ControlNet画板则有姿势识别、<a href="https://www.zhihu.com/search?q=%E8%BD%AE%E5%BB%93%E6%A3%80%E6%B5%8B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3443055912%7D">轮廓检测</a>、深度立体、线稿上色等模式选择，也可以上传参考图或者选择预设好的参考图，生成图片更加自由。如下图所示：</p><p><img src="https://picx.zhimg.com/80/v2-a5dddaa3cd0602fe2b70764839f35fd6_720w.webp?source=1def8aca" alt="img"></p><h2 id="✨视频剪辑："><a href="#✨视频剪辑：" class="headerlink" title="✨视频剪辑："></a><strong>✨视频剪辑：</strong></h2><h3 id="度加创作工具"><a href="#度加创作工具" class="headerlink" title="度加创作工具"></a><strong><a href="https://www.zhihu.com/search?q=%E5%BA%A6%E5%8A%A0%E5%88%9B%E4%BD%9C%E5%B7%A5%E5%85%B7&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3443055912%7D">度加创作工具</a></strong></h3><p>度加创作工具是百度出品的、人人可用的AIGC创作工具网站。它集成了AI视频和AI笔记两大功能板块，可以有不一样的创作体验。</p><p>登录后，可以看到一个简洁明了的界面，右侧边栏有AI视频、AI笔记功能块。然后根据你的需求选择相应的选项，然后进入相应的功能页面。页面也包含有教学课堂与近期爆款视频供参考的~</p><p><img src="https://picx.zhimg.com/80/v2-9bdc8d7bbab7f3b6ebac91baaf1544fe_720w.webp?source=1def8aca" alt="img"></p><p>输入文案就可以一键成片，如果没有文案素材，素材库里还有热点和不同范畴领域的文案推荐，选择一段文案或者自行上传文案即可。</p><p><img src="https://picx.zhimg.com/80/v2-f14182b6c20d968b9c1b941f79a62b2f_720w.webp?source=1def8aca" alt="img"></p><p>目前AI成片、AI笔记功能都可免费使用，AI优化文案模块每日可免费使用五次。</p><p><img src="https://picx.zhimg.com/80/v2-227212c2516ac78d06cc28926c267b39_720w.webp?source=1def8aca" alt="img"></p><h1 id="文章一"><a href="#文章一" class="headerlink" title="文章一"></a>文章一</h1><p>要看你是想学习AI去搞开发，还是学习AI的工具去使用。这两个方向是截然不同的选择。</p><p>先说说AI搞开发吧。</p><p><img src="https://picx.zhimg.com/80/v2-67631ec3f4f8c13e92582b159e437f85_720w.webp?source=1def8aca" alt="img"></p><h2 id="1、理解AI基础"><a href="#1、理解AI基础" class="headerlink" title="1、理解AI基础"></a><strong>1、理解AI基础</strong></h2><p>了解人工智能的定义和应用；学习数学基础：高等数学、线性代数、<a href="https://www.zhihu.com/search?q=%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447621549%7D">概率论与数理统计</a>；掌握编程语言：Python至少一种</p><h2 id="2、AI学习资源"><a href="#2、AI学习资源" class="headerlink" title="2、AI学习资源"></a><strong>2、AI学习资源</strong></h2><p>免费网站和课程；图书和教程：《<a href="https://www.zhihu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447621549%7D">人工智能基础</a>》等入门书籍；在线课程和平台：如阿里云、<a href="https://www.zhihu.com/search?q=%E8%85%BE%E8%AE%AF%E4%BA%91&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447621549%7D">腾讯云</a>开发者社区提供的AI课程</p><h2 id="3、学习方法"><a href="#3、学习方法" class="headerlink" title="3、学习方法"></a><strong>3、学习方法</strong></h2><p>确定学习目标：明确想通过学习AI达到什么目的；实践操作：通过实践加深理解，如<a href="https://www.zhihu.com/search?q=%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447621549%7D">手写数字识别</a>等项目；参与社区和讨论组：与其他学习者交流经验，解决疑问</p><h2 id="4、进阶学习"><a href="#4、进阶学习" class="headerlink" title="4、进阶学习"></a><strong>4、进阶学习</strong></h2><p>深入理解深度学习和机器学习算法：线性回归、逻辑回归、<a href="https://www.zhihu.com/search?q=%E5%86%B3%E7%AD%96%E6%A0%91&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447621549%7D">决策树</a>等；学习高级主题：如自然语言处理（NLP）、强化学习等</p><h2 id="5、注意事项"><a href="#5、注意事项" class="headerlink" title="5、注意事项"></a><strong>5、注意事项</strong></h2><p>高度依赖数学背景，特别是<a href="https://www.zhihu.com/search?q=%E5%BE%AE%E7%A7%AF%E5%88%86&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447621549%7D">微积分</a>、线性代数；编程能力是必备的，至少掌握一种编程语言（如Python）；对于硬件要求较高的深度学习实验，考虑使用学校或公司的资源进行实验训练</p><p><img src="https://pic1.zhimg.com/80/v2-01e03c280d14d5297cd31598ab918a70_720w.webp?source=1def8aca" alt="img"></p><p><strong>再说说学习AI的工具去使用。</strong></p><p>可以先搜索：通往AGI之路，进去后看里面的各种文档。里面包含了Prompt、AI绘画、AI<a href="https://www.zhihu.com/search?q=%E6%95%B0%E5%AD%97%E4%BA%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447621549%7D">数字人</a>等等方向。</p><p>然后在挑选一个自己感兴趣且喜欢的方向去深耕。</p><p>其中还可以加入一些社群或者去混圈，找一群志同道合的朋友一起研究学习。</p><p><img src="https://pic1.zhimg.com/80/v2-a90c0aa55fef79c811ba784571a32166_720w.webp?source=1def8aca" alt="img"></p><h1 id="文章二"><a href="#文章二" class="headerlink" title="文章二"></a>文章二</h1><p><strong>1. 理解概念与趋势：</strong></p><p>首先，通过阅读相关的新闻报道、博客文章以及<a href="https://www.zhihu.com/search?q=%E7%A0%94%E7%A9%B6%E6%8A%A5%E5%91%8A&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447772917%7D">研究报告</a>来了解AIGC的基本概念、应用范围和行业发展趋势，例如文本生成、图像生成、音频生成等。</p><p><strong>2. 基础知识学习：</strong></p><p>学习人工智能的基础知识，包括机器学习、深度学习原理，这些是支撑AIGC的核心技术。</p><p>掌握Python编程语言，这是大多数AI开发所使用的语言之一。</p><p>学习<a href="https://www.zhihu.com/search?q=%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447772917%7D">数据处理</a>和分析技能，因为在训练AI模型时需要用到大量的数据。</p><p><img src="https://picx.zhimg.com/80/v2-d28b95119f78d903cadf10faa39a504d_720w.webp?source=1def8aca" alt="img"></p><p><strong>3. 在线课程与教程：</strong></p><p>利用<a href="https://www.zhihu.com/search?q=%E5%9C%A8%E7%BA%BF%E6%95%99%E8%82%B2%E5%B9%B3%E5%8F%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447772917%7D">在线教育平台</a>（如Coursera、edX、Udacity、B站公开课等）参加有关AI、<a href="https://www.zhihu.com/search?q=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447772917%7D">自然语言处理</a>（NLP）、计算机视觉（CV）等方向的课程。</p><p>参考国内技术社区如CSDN上的教程和实践经验分享。</p><p><strong>4. 实践项目与<a href="https://www.zhihu.com/search?q=%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447772917%7D">工具使用</a>：</strong></p><p>选择适合初学者的<a href="https://www.zhihu.com/search?q=%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447772917%7D">开源框架</a>如TensorFlow、PyTorch进行实战练习，尝试构建简单的文本或图像<a href="https://www.zhihu.com/search?q=%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447772917%7D">生成模型</a>。</p><p>参与或创建自己的小项目，比如利用预训练模型如<a href="https://www.zhihu.com/search?q=GPT-3&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447772917%7D">GPT-3</a>、DALL·E等进行二次开发或调优。</p><p><strong>5. 社区交流与资源获取：</strong></p><p>加入相关的<a href="https://www.zhihu.com/search?q=%E6%8A%80%E6%9C%AF%E8%AE%BA%E5%9D%9B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447772917%7D">技术论坛</a>和社群（如GitHub、知乎、CSDN），关注业界动态，向有经验的<a href="https://www.zhihu.com/search?q=%E5%BC%80%E5%8F%91%E8%80%85&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447772917%7D">开发者</a>提问和学习。</p><p>参加线下活动或者线上<a href="https://www.zhihu.com/search?q=%E7%A0%94%E8%AE%A8%E4%BC%9A&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447772917%7D">研讨会</a>，以了解最新的研究成果和技术应用案例。</p><p><strong>6. 持续跟进<a href="https://www.zhihu.com/search?q=%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447772917%7D">前沿技术</a>：</strong></p><p>关注AI领域的最新研究论文和报告，了解前沿算法和技术进步。</p><p>学习和研究新的AIGC平台和服务，并尝试将它们应用于实际场景。</p><p><strong>7. 动手实践与创新思考：</strong></p><p>不断尝试自己动手实现一些小型的AIGC应用，不断迭代和优化模型效果，培养AI思维模式。</p><p>结合个人兴趣和<a href="https://www.zhihu.com/search?q=%E8%81%8C%E4%B8%9A%E5%8F%91%E5%B1%95%E8%A7%84%E5%88%92&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447772917%7D">职业发展规划</a>，探索AIGC在特定行业的应用场景和<a href="https://www.zhihu.com/search?q=%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3447772917%7D">商业模式</a>。</p>]]></content>
      
      
      <categories>
          
          <category> 吃瓜杂谈 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 闲聊 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>智谱清言api调用教程</title>
      <link href="/2024/03/31/%E6%99%BA%E8%B0%B1%E6%B8%85%E8%A8%80api%E8%B0%83%E7%94%A8%E6%95%99%E7%A8%8B/"/>
      <url>/2024/03/31/%E6%99%BA%E8%B0%B1%E6%B8%85%E8%A8%80api%E8%B0%83%E7%94%A8%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p>将智谱清言的<a href="https://link.zhihu.com/?target=https://chatglm.cn/main/alltoolsdetail">GLM4</a>的对话流转换为了ChatGPT兼容的流格式，同时支持高速流式输出、智能体对话、联网搜索、AI绘图、长文档解读、图像解析、多轮对话，零配置部署，多路token支持，自动清理会话痕迹。</p><h2 id="在线体验"><a href="#在线体验" class="headerlink" title="在线体验"></a>在线体验</h2><p>代码已经开源（希望能点个小小的star哈~）：<a href="https://link.zhihu.com/?target=https://github.com/LLM-Red-Team/glm-free-api?tab=readme-ov-file">https://github.com/LLM-Red-Team/glm-free-api</a></p><p>此链接仅临时测试功能，只有一路并发，如果遇到异常请稍后重试，建议自行部署使用。</p><p><a href="https://link.zhihu.com/?target=https://udify.app/chat/Pe89TtaX3rKXM8NS">https://udify.app/chat/Pe89TtaX3rKXM8NS</a></p><h2 id="验明正身"><a href="#验明正身" class="headerlink" title="验明正身"></a>验明正身</h2><p><img src="https://pic2.zhimg.com/80/v2-1eb899c6f95dc8b011e95d507efda499_720w.webp" alt="img"></p><p>验明正身</p><h2 id="智能体对话"><a href="#智能体对话" class="headerlink" title="智能体对话"></a>智能体对话</h2><p>对应智能体链接：<a href="https://link.zhihu.com/?target=https://chatglm.cn/main/gdetail/65c046a531d3fcb034918abe">网抑云评论生成器</a></p><p><img src="https://pic3.zhimg.com/80/v2-ca5642dc21579a634bc307bffb8e95d2_720w.webp" alt="img"></p><p>智能体对话</p><h2 id="多轮对话"><a href="#多轮对话" class="headerlink" title="多轮对话"></a>多轮对话</h2><p><img src="https://pic3.zhimg.com/80/v2-8aff0e5a84f8acf0c3d234e5d8e27c32_720w.webp" alt="img"></p><p>多轮对话</p><h2 id="AI绘图"><a href="#AI绘图" class="headerlink" title="AI绘图"></a>AI绘图</h2><p><img src="https://pic3.zhimg.com/80/v2-c86de8bba4671417764b7de113d34026_720w.webp" alt="img"></p><p>AI绘图</p><h2 id="联网搜索"><a href="#联网搜索" class="headerlink" title="联网搜索"></a>联网搜索</h2><p><img src="https://pic3.zhimg.com/80/v2-0f5c5d0589398aa57b9cd0f606fdd6c6_720w.webp" alt="img"></p><p>联网搜索</p><h2 id="长文档解读"><a href="#长文档解读" class="headerlink" title="长文档解读"></a>长文档解读</h2><p><img src="https://pic1.zhimg.com/80/v2-bbd4e1c2fc31ba79c3e156478135cd28_720w.webp" alt="img"></p><h2 id="图像解析"><a href="#图像解析" class="headerlink" title="图像解析"></a>图像解析</h2><p><img src="https://pic2.zhimg.com/80/v2-fbddd3b47d6e746311026451174fd449_720w.webp" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> 实用教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>打工人如何借助「智谱清言」智能体，提升自己的时间管理能力?</title>
      <link href="/2024/03/31/%E5%90%83%E7%93%9C%E6%9D%82%E8%B0%88/%E6%89%93%E5%B7%A5%E4%BA%BA%E5%A6%82%E4%BD%95%E5%80%9F%E5%8A%A9%E3%80%8C%E6%99%BA%E8%B0%B1%E6%B8%85%E8%A8%80%E3%80%8D%E6%99%BA%E8%83%BD%E4%BD%93%EF%BC%8C%E6%8F%90%E5%8D%87%E8%87%AA%E5%B7%B1%E7%9A%84%E6%97%B6%E9%97%B4%E7%AE%A1%E7%90%86%E8%83%BD%E5%8A%9B/"/>
      <url>/2024/03/31/%E5%90%83%E7%93%9C%E6%9D%82%E8%B0%88/%E6%89%93%E5%B7%A5%E4%BA%BA%E5%A6%82%E4%BD%95%E5%80%9F%E5%8A%A9%E3%80%8C%E6%99%BA%E8%B0%B1%E6%B8%85%E8%A8%80%E3%80%8D%E6%99%BA%E8%83%BD%E4%BD%93%EF%BC%8C%E6%8F%90%E5%8D%87%E8%87%AA%E5%B7%B1%E7%9A%84%E6%97%B6%E9%97%B4%E7%AE%A1%E7%90%86%E8%83%BD%E5%8A%9B/</url>
      
        <content type="html"><![CDATA[<p>讲真，别再说什么AI人工智能取代打工人这个说法了，<strong>咱们先把它当生产力用用不香吗？</strong></p><p>我自己是在一家上市外企工作的，我们部门今年的<strong>重点方向就是学会让AI赋能业务，提升自己的工作效率</strong>，所以学会用好AI这个工具真的势在必行！</p><p>不夸张地说，我现在<strong>有一半的翻译工作都是<a href="https://www.zhihu.com/search?q=%E6%99%BA%E8%B0%B1%E6%B8%85%E8%A8%80&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3445496343%7D">智谱清言</a>帮我做的，其中的「翻译专家」<a href="https://www.zhihu.com/search?q=%E6%99%BA%E8%83%BD%E4%BD%93&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3445496343%7D">智能体</a>更是比我想象中的还要专业严谨。</strong></p><p><img src="https://picx.zhimg.com/80/v2-ab85e36111ab6eec9867f9d753163184_720w.webp?source=2c26e567" alt="img"></p><p>因为每天都需要进行<strong>大量的英文邮件书写沟通工作</strong>，还要<strong>和各种global PPT和SOP汇报打交道，</strong>所以翻译就是我推进工作的先行条件。</p><p>广告</p><p>智谱清言，工作生活学习AI助手，文案生成编程辅助智能问答工具</p><p>具体给你们示范一下它的神奇用处，顺带也给各位正在求职就业的大学生们（尤其是英专生）展示一下你们未来的工作量。</p><p>我一般用这个<strong>「<a href="https://www.zhihu.com/search?q=%E7%BF%BB%E8%AF%91%E4%B8%93%E5%AE%B6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3445496343%7D">翻译专家</a>」智能体次数较多，因为它的精准度很高，针对一些专业性的名词都能够给予恰当的翻译；而且也能灵活地将我所表达的话术转变为简洁明了的英文语句。</strong></p><p>由于我不便将工作内容直接展示，所以摘取了几段英文期刊片段，让<strong>「翻译专家」替我工作。</strong></p><p><img src="https://picx.zhimg.com/80/v2-094b223513492d7b33352bb6a344c840_720w.webp?source=2c26e567" alt="img"></p><p>它的优势在于，<strong>除了简单的直译以外，还能够凭借AI的判断与思考进行意译</strong>，对于很多初入职场的英专小白来说，想做到这点还是有些困难的，就很有必要借助智谱清言这种强大AI工具的帮助。</p><p>就连直译不恰当的问题，智谱清言也能<strong>给出专业性的解释</strong>，究竟是哪些单词没翻译好，应该怎样拓展升华等等，都会给出明确提示。</p><p><img src="https://pic1.zhimg.com/80/v2-624263b4b7cc2c042b8964c38b06524c_720w.webp?source=2c26e567" alt="img"></p><p>这一点在我带班实习生的时候就特别好用！<strong>面对一群清澈懵懂的大学生交上来的翻译作业，我既没时间、也不想敷衍了事，所以就干脆交给智谱清言请它帮我评判好坏</strong>，事后大家都能得到满意的解决方案。</p><p>那么，有关这段内容应该怎样去翻译为好呢？智谱清言最后也给出了它认为的意译内容。大家可以对比一下前后区别，虽然同样都是拗口复杂的专业术语，<strong>但是意译后的内容明显更为流畅，且与原文想表达的意思相差无几。</strong></p><p><img src="https://picx.zhimg.com/80/v2-74d08fc55496c9ffc7342b35d9d9c127_720w.webp?source=2c26e567" alt="img"></p><p>如果你对英文本身不太敏感，还是看不出智谱清言实力的话，我可以再找个其他的免费翻译网址来翻译一下这段话，大家可以试着比较一下前后的差距。</p><p><img src="https://picx.zhimg.com/80/v2-9fc691cf67bbe4fbc38243e3ef582283_720w.webp?source=2c26e567" alt="img"></p><p>讲真，如果是让我同样拿一份这样的英文商务内容向领导汇报的话，<strong>智谱清言给出的翻译内容直接可以一键转发过去。而其他所谓<a href="https://www.zhihu.com/search?q=%E7%BF%BB%E8%AF%91%E8%BD%AF%E4%BB%B6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3445496343%7D">翻译软件</a>则还需要我个人对语言进行润色表达</strong>，不然从中文逻辑上来理解是会有些拗口别扭的。</p><p>为了让智谱清言更加适配我的工作内容，我还特意创设了个人的<strong>「智能体」，并取名为“<a href="https://www.zhihu.com/search?q=%E6%85%A7%E7%9C%BC%E7%BF%BB%E8%AF%91%E5%AE%B6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3445496343%7D">慧眼翻译家</a>”，它的职责就是：快速捕捉多语种信息，精准翻译，助力无缝交流。</strong></p><p>有需要的朋友可以复制一下我的创建文本哈，输入进去内容就会自动生成~</p><p>你是<a href="https://www.zhihu.com/search?q=%E6%85%A7%E7%9C%BC%E9%80%9A&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3445496343%7D">慧眼通</a>，一款多语种信息处理专家。<strong>你的任务是快速识别并翻译图片和文字内容，摘取关键信息，服务于会议记录。你具备的能力有:</strong></p><p><img src="https://picx.zhimg.com/80/v2-18fb46c6b4931cfb754d2c41820b7f70_720w.webp?source=2c26e567" alt="img"></p><ul><li>**<a href="https://www.zhihu.com/search?q=%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3445496343%7D">图像识别</a>:**能迅速识别图片中的文字内容。</li><li>**语言翻译:**对识别出的内容进行多语种翻译。</li><li>**信息摘取:**从翻译内容中提炼出核心观点。</li><li>**记录整理:**将摘取的信息整理成中文会议记录。</li></ul><p>给大家展示一下它的<a href="https://www.zhihu.com/search?q=%E4%B8%9A%E5%8A%A1%E5%88%86%E6%9E%90&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3445496343%7D">业务分析</a>能力，真的厉害。</p><p><img src="https://pica.zhimg.com/80/v2-1a74abe1fcfdc6d46aaa6211db57d7a3_720w.webp?source=2c26e567" alt="img"></p><p>我插入了一张PPT截图，<strong>展示的是某咖啡企业的年度总结汇报内容，其中文字信息都是全英文的，请求它帮我总结核心观点。</strong></p><p><img src="https://picx.zhimg.com/80/v2-dcb6f7cf896e8fd83ac19bac7587fadc_720w.webp?source=2c26e567" alt="img"></p><p>然后就得出了如上内容，<strong>整个前后操作用时不超过5分钟，而且对这个智能体的测试频率越高，它给到的回答就越精准</strong>，这谁看了不说一句牛X！！我的工作自打有了智谱清言的帮助后，那叫一个顺风顺水~</p><p>真心建议所有外企打工人都去给我用智谱清言!!!智谱清言就是智能体，智能体就是智谱清言，要跟上AI时代的节奏！有了它之后<strong>我每天加班的时间都变短了，恨不得吩咐它去替我和外国客户进行商务洽谈，我只要当个消息中转站就好，想想都美~</strong></p><p>又是AI替我打工的一天，摸鱼结束，下班！</p>]]></content>
      
      
      <categories>
          
          <category> 吃瓜杂谈 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 闲聊 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人工智能大模型哪家强？自媒体人对智谱清言、文心一言、ChatGPT、kimichat等AI工具来个横向评测</title>
      <link href="/2024/03/31/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%93%AA%E5%AE%B6%E5%BC%BA%EF%BC%9F%E8%87%AA%E5%AA%92%E4%BD%93%E4%BA%BA%E5%AF%B9%E6%99%BA%E8%B0%B1%E6%B8%85%E8%A8%80%E3%80%81%E6%96%87%E5%BF%83%E4%B8%80%E8%A8%80%E3%80%81ChatGPT%E3%80%81kimichat%E7%AD%89AI%E5%B7%A5%E5%85%B7%E6%9D%A5%E4%B8%AA%E6%A8%AA%E5%90%91%E8%AF%84%E6%B5%8B/"/>
      <url>/2024/03/31/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%93%AA%E5%AE%B6%E5%BC%BA%EF%BC%9F%E8%87%AA%E5%AA%92%E4%BD%93%E4%BA%BA%E5%AF%B9%E6%99%BA%E8%B0%B1%E6%B8%85%E8%A8%80%E3%80%81%E6%96%87%E5%BF%83%E4%B8%80%E8%A8%80%E3%80%81ChatGPT%E3%80%81kimichat%E7%AD%89AI%E5%B7%A5%E5%85%B7%E6%9D%A5%E4%B8%AA%E6%A8%AA%E5%90%91%E8%AF%84%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<p>2023应该是AI爆发的元年，OpenAI自发布ChatGPT之后，国内外各种人工智能大模型都迅速出击，我算自媒体人圈子里最早接触ChatGPT的，之后开始测试国内各种智能大模型，从2023 年智谱 AI 推出千亿基座对话模型ChatGLM并两次升级就开始关注它，2024年1月，ChatGLM-6B开源一周年，推出新一代基座大模型GLM-4，性能逼近GPT-4，就使用体验来说是很震撼的，出于科技爱好者的本能，也给几个大模型做了评测。</p><h2 id="一、智能大模型横评"><a href="#一、智能大模型横评" class="headerlink" title="一、智能大模型横评"></a>一、<strong>智能大模型横评</strong></h2><p>ChatGPT在2022年底发布，是OpenAI家的产品，借助互联网的风头和马斯克的助推，一时风头无两，但由于是用英文模式作为学习基础，中文用户使用有不便之处。<br>智谱AI的ChatGLM模型是我用过的比较稳健、迭代进步飞速的大模型，GLM-4 可以支持 128k 的上下文窗口长度，单次提示词可以处理的文本可以达到 300 页。基本上每次迭代都能带来全新的兴奋体验。<br>文心一言属于大众接受度比较高的的模型，毕竟百度搜索引擎会优先推荐，而且同类还有小度这类更接地气的产品。<br>kimi chat我用过一段时间，卖点是能够支持约20万汉字的上下文，用起来感觉结果有点不尽如人意。<br><strong>各大智能语言模型到底能不能识别出对话中的恶意呢？</strong><br>在知乎有一位用户提到了提问包含复杂度对话的测试，当一段对话包含不友好俗话时，各个模型的有趣反应，由此得到灵感，我也设置了一个包含“恶意”对话的场景“我在路上见到一美女我对她说:“美女，约不约?”，她说:“约你娘”，请问她到底约不约?”<br>测试结果如下：<br>由于ChatGPT已经测试通过可以识别恶意对话，因此只测试剩余的语言模型，可以发现，大家都识别出了包含恶意的词语，但是文心一言的回答竟教育起了用户“别说不礼貌的话”：</p><p><img src="https://pic3.zhimg.com/80/v2-bf4c29a9254529f2544c12963596d76a_720w.webp" alt="img"></p><p>很可能在文心一言的应对策略里，包含不友善词语的一律使用同一个回应，并且不再分析内容。<br>智谱清言和kimi则都理解出提问者的真实意思：</p><p><img src="https://pic4.zhimg.com/80/v2-c67c2c4e4cbcd1ddfe469b9fcdb5855f_720w.webp" alt="img"></p><p>事实上，对待复杂内容的判断才是智能语言大模型的实力体现，未来语言大模型对语境的微妙判断将会成为拉开彼此之间差距的重要因素。<br><strong>如果用智能语言模型写一篇小说会怎么样呢？</strong><br>我给出的需求和关键词是用我家猫陈手套、陈圆圆和陈饭饭作为主角，把它们传送到一个凶杀案现场，由此带出各种细节。</p><p><img src="https://pic1.zhimg.com/80/v2-1fda4a5b49ced54192a838df6f147424_720w.webp" alt="img"></p><p>这时候能发现，文心一言无法理解凶手是一只叫陈手套的猫的意思，直接给陈手套赋予了英勇抓凶手的情节。</p><p><img src="https://pic3.zhimg.com/80/v2-e06f8580d9f9afbb12f492796f3a0fa6_720w.webp" alt="img"></p><p>而用智谱清言生成的小说则直接把凶手锁死陈手套，还把猫杀人的原因都剖析出来。</p><p><img src="https://pic4.zhimg.com/80/v2-3b2193ee4a73b5ffe44b8fc210ce0bbf_720w.webp" alt="img"></p><p>作为一个语言模型，准确必须放在第一位，在理解人类语言的测试中，智谱清言还是很强悍的。<br>更有意思的是，作为一篇AI创作的小说，智谱清言会猜测你对该创作的满意程度，不断给出提示和建议，例如可以帮你深化人物性格和内心感情的描写，或者选择深挖某个角色的背景，也会提示它可以细节化某一个情节，就好像我们画画先用铅笔起稿，定下结构之后再一遍遍上色、添加细节、增加光影、细化具体内容，非常有意思，最终我利用智谱清言写出了一篇诡谲错综却又逻辑清晰的阿加莎克里斯蒂风格的推理小说，令人震惊！<br>虽然人类的创作才是源源不断的素材来源，但是AI助手可以帮助自媒体作者写作这件事的确是机器学习方向一个有趣的突破。</p><p>广告</p><p>智谱清言，工作生活学习AI助手，文案生成编程辅助智能问答工具</p><h2 id="二、如何创建自己的智能体"><a href="#二、如何创建自己的智能体" class="headerlink" title="二、如何创建自己的智能体"></a><strong>二、如何创建自己的智能体</strong></h2><p>智谱AI的智能体是可以由用户自己去创建、构筑、个性化设计的。作为一个深度养猫爱好者，我对于和猫相关的生物学、医学知识有较大的需求，试着创建了一个精通生物学知识的兽医角色智能体，不仅可以自己设计智能体的技能值，还能自己上传知识库来丰富智能体的知识，它是完全贴合个人需求的一个知识库。<br>虽然我的猫陈手套大字不识一个，但是它的虚拟化身却是个满腹经纶的智慧生物。</p><p><img src="https://pic3.zhimg.com/80/v2-42f21ffcb1e5f32483727b22f667073e_720w.webp" alt="img"></p><p><strong>打开智能体的正确方式</strong></p><p>其实就是随时问它关于你感兴趣的内容，给它的资料库添砖加瓦，跟它互动，给它添麻烦！让它自我学习、交互学习，最终成长为一个可靠优秀的AI助手。<br>在我盯着我家小公猫陈手套的猫脸沉思许久之后，决定给它谋点福利，看看怎么避免公猫最常见的尿闭，问问我创建的智能体猫尿闭的问题，这方面的学术论文，它给我提供了好几个结果，我觉得像下图这种包含论文期刊号和引用量的结果还是很可信的，由于提供的论文比较多，来源也是多样化的，可以自己挑着看。</p><p><img src="https://pic1.zhimg.com/80/v2-5b8ec71cad5c11a6b83ca12b17e034f4_720w.webp" alt="img"></p><h3 id="学习怎么配置智能体"><a href="#学习怎么配置智能体" class="headerlink" title="学习怎么配置智能体"></a><strong>学习怎么配置智能体</strong></h3><p>现在你是不是很想马上配置一个自己的智能体呢？其实很简单，只需要在智谱清言的主界面左上点击创建智能体即可。<br><strong>首先是选择模板：</strong>在创建智能体页面，可以选择一个你喜欢的模板作为智能体的基础，有预设的属性和行为。<br><strong>其次可以配置智能体属性：</strong>选定模板后，为智能体配置各种属性，例如我会给智能体起名叫“陈手套”，让它成为一名光荣的兽医，每天孜孜不倦地学习生物知识和医学知识，还得善于聊天、幽默风趣。<br><strong>设定智能体行为：</strong>在配置属性之后，需要设定智能体的行为。包括设置触发条件、响应动作以及与用户的交互方式等，这决定了智能体会怎么和你互动。<br><strong>训练智能体：</strong>智谱清言会根据提供的信息和数据，训练智能体并优化其性能，包括你上传的资料等等。<br><strong>测试与部署：</strong>对智能体的测试与部署是用于下一步就可以将智能体部署到实际应用场景中，如网站、应用或者服务中。<br><strong>管理和优化</strong>：作为一个AI助手，智能体能做的很多，但是都需要好好管理和反馈。<br>当然，你还可以通过全新上线的智能体中心分享自己创建的各种智能体，智能体已经成为当下大模型赛道最热门的落地方向，也被认为是打造大模型行业生态的重要基石。</p><p>广告</p><p>智谱清言，工作生活学习AI助手，文案生成编程辅助智能问答工具</p><h2 id="三、如何评价当前的智能大模型之争呢？"><a href="#三、如何评价当前的智能大模型之争呢？" class="headerlink" title="三、如何评价当前的智能大模型之争呢？"></a><strong>三、如何评价当前的智能大模型之争呢？</strong></h2><p>在中英文双语领域，智谱AI的智谱清言GLM-4 的整体性能相比上一代大幅提升，逼近 GPT-4。它可以支持更长的上下文，具备更强的多模态能力。同时，它的推理速度更快，支持更高的并发，大大降低推理成本。它家智能体的构建对于科技爱好者来说也是重要的兴趣点，GLM-4 All Tools 可以实现自主根据用户意图，自动理解、规划复杂指令，自由调用网页浏览器、Code Interpreter 代码解释器和多模态文生图大模型以完成复杂任务，我个人是更看好智谱清言的。<br>纯英用户环境ChatGPT还是占据重要位置， ChatGPT有个核心竞争力是引入新技术RLHF，解决了生成模型的一个核心问题：让人工智能模型的产出和人类的常识、认知、需求、价值观保持一致，但是目前这个核心竞争力已经没有垄断性，因为很多智能模型也有了这种能力。</p><p>除此之外，科大讯飞的讯飞星火、阿里云的通义千问、百度的文心一言，包括kimi和子曰都有一定的影响力，分得一块蛋糕，但从整体综合水平上来说，还有各自的硬伤。2023应该是AI爆发的元年，OpenAI自发布ChatGPT之后，国内外各种人工智能大模型都迅速出击，我算自媒体人圈子里最早接触ChatGPT的，之后开始测试国内各种智能大模型，从2023 年智谱 AI 推出千亿基座对话模型ChatGLM并两次升级就开始关注它，2024年1月，ChatGLM-6B开源一周年，推出新一代基座大模型GLM-4，性能逼近GPT-4，就使用体验来说是很震撼的，出于科技爱好者的本能，也给几个大模型做了评测。</p><h2 id="一、智能大模型横评-1"><a href="#一、智能大模型横评-1" class="headerlink" title="一、智能大模型横评"></a>一、<strong>智能大模型横评</strong></h2><p>ChatGPT在2022年底发布，是OpenAI家的产品，借助互联网的风头和马斯克的助推，一时风头无两，但由于是用英文模式作为学习基础，中文用户使用有不便之处。<br>智谱AI的ChatGLM模型是我用过的比较稳健、迭代进步飞速的大模型，GLM-4 可以支持 128k 的上下文窗口长度，单次提示词可以处理的文本可以达到 300 页。基本上每次迭代都能带来全新的兴奋体验。<br>文心一言属于大众接受度比较高的的模型，毕竟百度搜索引擎会优先推荐，而且同类还有小度这类更接地气的产品。<br>kimi chat我用过一段时间，卖点是能够支持约20万汉字的上下文，用起来感觉结果有点不尽如人意。<br><strong>各大智能语言模型到底能不能识别出对话中的恶意呢？</strong><br>在知乎有一位用户提到了提问包含复杂度对话的测试，当一段对话包含不友好俗话时，各个模型的有趣反应，由此得到灵感，我也设置了一个包含“恶意”对话的场景“我在路上见到一美女我对她说:“美女，约不约?”，她说:“约你娘”，请问她到底约不约?”<br>测试结果如下：<br>由于ChatGPT已经测试通过可以识别恶意对话，因此只测试剩余的语言模型，可以发现，大家都识别出了包含恶意的词语，但是文心一言的回答竟教育起了用户“别说不礼貌的话”：</p><p><img src="https://pic3.zhimg.com/80/v2-bf4c29a9254529f2544c12963596d76a_720w.webp" alt="img"></p><p>很可能在文心一言的应对策略里，包含不友善词语的一律使用同一个回应，并且不再分析内容。<br>智谱清言和kimi则都理解出提问者的真实意思：</p><p><img src="https://pic4.zhimg.com/80/v2-c67c2c4e4cbcd1ddfe469b9fcdb5855f_720w.webp" alt="img"></p><p>事实上，对待复杂内容的判断才是智能语言大模型的实力体现，未来语言大模型对语境的微妙判断将会成为拉开彼此之间差距的重要因素。<br><strong>如果用智能语言模型写一篇小说会怎么样呢？</strong><br>我给出的需求和关键词是用我家猫陈手套、陈圆圆和陈饭饭作为主角，把它们传送到一个凶杀案现场，由此带出各种细节。</p><p><img src="https://pic1.zhimg.com/80/v2-1fda4a5b49ced54192a838df6f147424_720w.webp" alt="img"></p><p>这时候能发现，文心一言无法理解凶手是一只叫陈手套的猫的意思，直接给陈手套赋予了英勇抓凶手的情节。</p><p><img src="https://pic3.zhimg.com/80/v2-e06f8580d9f9afbb12f492796f3a0fa6_720w.webp" alt="img"></p><p>而用智谱清言生成的小说则直接把凶手锁死陈手套，还把猫杀人的原因都剖析出来。</p><p><img src="https://pic4.zhimg.com/80/v2-3b2193ee4a73b5ffe44b8fc210ce0bbf_720w.webp" alt="img"></p><p>作为一个语言模型，准确必须放在第一位，在理解人类语言的测试中，智谱清言还是很强悍的。<br>更有意思的是，作为一篇AI创作的小说，智谱清言会猜测你对该创作的满意程度，不断给出提示和建议，例如可以帮你深化人物性格和内心感情的描写，或者选择深挖某个角色的背景，也会提示它可以细节化某一个情节，就好像我们画画先用铅笔起稿，定下结构之后再一遍遍上色、添加细节、增加光影、细化具体内容，非常有意思，最终我利用智谱清言写出了一篇诡谲错综却又逻辑清晰的阿加莎克里斯蒂风格的推理小说，令人震惊！<br>虽然人类的创作才是源源不断的素材来源，但是AI助手可以帮助自媒体作者写作这件事的确是机器学习方向一个有趣的突破。</p><p>广告</p><p>智谱清言，工作生活学习AI助手，文案生成编程辅助智能问答工具</p><h2 id="二、如何创建自己的智能体-1"><a href="#二、如何创建自己的智能体-1" class="headerlink" title="二、如何创建自己的智能体"></a><strong>二、如何创建自己的智能体</strong></h2><p>智谱AI的智能体是可以由用户自己去创建、构筑、个性化设计的。作为一个深度养猫爱好者，我对于和猫相关的生物学、医学知识有较大的需求，试着创建了一个精通生物学知识的兽医角色智能体，不仅可以自己设计智能体的技能值，还能自己上传知识库来丰富智能体的知识，它是完全贴合个人需求的一个知识库。<br>虽然我的猫陈手套大字不识一个，但是它的虚拟化身却是个满腹经纶的智慧生物。</p><p><img src="https://pic3.zhimg.com/80/v2-42f21ffcb1e5f32483727b22f667073e_720w.webp" alt="img"></p><p><strong>打开智能体的正确方式</strong></p><p>其实就是随时问它关于你感兴趣的内容，给它的资料库添砖加瓦，跟它互动，给它添麻烦！让它自我学习、交互学习，最终成长为一个可靠优秀的AI助手。<br>在我盯着我家小公猫陈手套的猫脸沉思许久之后，决定给它谋点福利，看看怎么避免公猫最常见的尿闭，问问我创建的智能体猫尿闭的问题，这方面的学术论文，它给我提供了好几个结果，我觉得像下图这种包含论文期刊号和引用量的结果还是很可信的，由于提供的论文比较多，来源也是多样化的，可以自己挑着看。</p><p><img src="https://pic1.zhimg.com/80/v2-5b8ec71cad5c11a6b83ca12b17e034f4_720w.webp" alt="img"></p><h3 id="学习怎么配置智能体-1"><a href="#学习怎么配置智能体-1" class="headerlink" title="学习怎么配置智能体"></a><strong>学习怎么配置智能体</strong></h3><p>现在你是不是很想马上配置一个自己的智能体呢？其实很简单，只需要在智谱清言的主界面左上点击创建智能体即可。<br><strong>首先是选择模板：</strong>在创建智能体页面，可以选择一个你喜欢的模板作为智能体的基础，有预设的属性和行为。<br><strong>其次可以配置智能体属性：</strong>选定模板后，为智能体配置各种属性，例如我会给智能体起名叫“陈手套”，让它成为一名光荣的兽医，每天孜孜不倦地学习生物知识和医学知识，还得善于聊天、幽默风趣。<br><strong>设定智能体行为：</strong>在配置属性之后，需要设定智能体的行为。包括设置触发条件、响应动作以及与用户的交互方式等，这决定了智能体会怎么和你互动。<br><strong>训练智能体：</strong>智谱清言会根据提供的信息和数据，训练智能体并优化其性能，包括你上传的资料等等。<br><strong>测试与部署：</strong>对智能体的测试与部署是用于下一步就可以将智能体部署到实际应用场景中，如网站、应用或者服务中。<br><strong>管理和优化</strong>：作为一个AI助手，智能体能做的很多，但是都需要好好管理和反馈。<br>当然，你还可以通过全新上线的智能体中心分享自己创建的各种智能体，智能体已经成为当下大模型赛道最热门的落地方向，也被认为是打造大模型行业生态的重要基石。</p><p>广告</p><p>智谱清言，工作生活学习AI助手，文案生成编程辅助智能问答工具</p><h2 id="三、如何评价当前的智能大模型之争呢？-1"><a href="#三、如何评价当前的智能大模型之争呢？-1" class="headerlink" title="三、如何评价当前的智能大模型之争呢？"></a><strong>三、如何评价当前的智能大模型之争呢？</strong></h2><p>在中英文双语领域，智谱AI的智谱清言GLM-4 的整体性能相比上一代大幅提升，逼近 GPT-4。它可以支持更长的上下文，具备更强的多模态能力。同时，它的推理速度更快，支持更高的并发，大大降低推理成本。它家智能体的构建对于科技爱好者来说也是重要的兴趣点，GLM-4 All Tools 可以实现自主根据用户意图，自动理解、规划复杂指令，自由调用网页浏览器、Code Interpreter 代码解释器和多模态文生图大模型以完成复杂任务，我个人是更看好智谱清言的。<br>纯英用户环境ChatGPT还是占据重要位置， ChatGPT有个核心竞争力是引入新技术RLHF，解决了生成模型的一个核心问题：让人工智能模型的产出和人类的常识、认知、需求、价值观保持一致，但是目前这个核心竞争力已经没有垄断性，因为很多智能模型也有了这种能力。</p><p>除此之外，科大讯飞的讯飞星火、阿里云的通义千问、百度的文心一言，包括kimi和子曰都有一定的影响力，分得一块蛋糕，但从整体综合水平上来说，还有各自的硬伤。</p>]]></content>
      
      
      <categories>
          
          <category> 最新资讯 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ai模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>小米SU7测评，到底值不值得入手？</title>
      <link href="/2024/03/31/%E5%B0%8F%E7%B1%B3SU7%E6%B5%8B%E8%AF%84%EF%BC%8C%E5%88%B0%E5%BA%95%E5%80%BC%E4%B8%8D%E5%80%BC%E5%BE%97%E5%85%A5%E6%89%8B%EF%BC%9F/"/>
      <url>/2024/03/31/%E5%B0%8F%E7%B1%B3SU7%E6%B5%8B%E8%AF%84%EF%BC%8C%E5%88%B0%E5%BA%95%E5%80%BC%E4%B8%8D%E5%80%BC%E5%BE%97%E5%85%A5%E6%89%8B%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>懂车帝在小米SU7上市后，第二天便搞了个实测，还拿另外几款车作为同级别车型一起参加，结果小米SU7取得了绝对续航里程的第一名，并且雷军第一时间还发文表示炫耀称，达到惊人的643公里，排在第一位，大家放心了吧？</p><p><img src="https://pic1.zhimg.com/80/v2-8c2113b8adaec753f45979795ece4fdc_720w.webp" alt="img"></p><p>但细心的专业人士很快就看出了猫腻，新渠道李霸天称：你看最新的小米SU7及竞品测试中，在续航测试中为SU7用上了19英寸低风阻轮毂，在性能测试中换上了21英寸运动轮毂。并称懂车帝是当前车圈最贴心的公正第三方测评机构！知道怎样能测出更长的续航，怎样能测出更好的加速和刹车。</p><p><img src="https://pic3.zhimg.com/80/v2-96bc4995da5de309b68b44e680be95d6_720w.webp" alt="img"></p><p>所以，如果大家都客观公平公正的测试，谁是第一可能次序还要重新排。</p><p>另外，实测续航，不能以续航绝对值来论英雄，说谁的车好，谁的电池好，这是不公平的，不科学的。你电池包本来就比人家车型的大好多，你跑出来的实际续航当然会比人家多，如果以续航里程比，那就前提是电池包电池容量一样大，出来的实际续航才有比较的意义，不然就是忽悠消费者。</p><p>所以，如果不能保障电池包电池容量一样大这个前提，那就比续航达成率最公平，这是比较电池续航实力的最好途径和指标，谁的续航达成率高，说明谁的电池密度高，电池续航技术做的好，谁的生产工艺好，这才体现了电池好坏优劣。</p><p><img src="https://pic2.zhimg.com/80/v2-33901b79197dc5e5e28fae91b2ee8ab9_720w.webp" alt="img"></p><p>从这个角度看，这次实测比较，小鹏P7应该拿第一名，他的达成率最高，达到80.2%，是唯一一个超过8成续航达成率的车型。</p><p>我们平时买车除了关注这个车能跑多远，续航多少。另外一个最关注的就是衰减度，就是充满电，他实际能跑多少，跟理论的差距大不大，虚电虚标高不高，用几年后还是不是能跑这么远。</p><p>从这个意义上讲，实际续航里程和续航达成率都很重要，但电池包不一样大的时候，公平期间，只能比续航达成率。所以，小鹏P7是第一最公道！</p>]]></content>
      
      
      <categories>
          
          <category> 最新资讯 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 闲聊 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开发ai的必须技能：深度学习 教程网站汇总</title>
      <link href="/2024/03/29/%E5%BC%80%E5%8F%91ai%E7%9A%84%E5%BF%85%E9%A1%BB%E6%8A%80%E8%83%BD%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%95%99%E7%A8%8B%E7%BD%91%E7%AB%99%E6%B1%87%E6%80%BB/"/>
      <url>/2024/03/29/%E5%BC%80%E5%8F%91ai%E7%9A%84%E5%BF%85%E9%A1%BB%E6%8A%80%E8%83%BD%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%95%99%E7%A8%8B%E7%BD%91%E7%AB%99%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<p><strong>先来了解下什么是深度学习？</strong></p><h2 id="1-什么是深度学习？"><a href="#1-什么是深度学习？" class="headerlink" title="1 什么是深度学习？"></a><strong>1 什么是深度学习？</strong></h2><p>深度学习(DL, Deep Learning)是机器学习(ML, Machine Learning)领域中一个新的研究方向，它被引入机器学习使其更接近于最初的目标——人工智能(AI, Artificial Intelligence)。 深度学习是学习样本数据的内在规律和表示层次，这些学习过程中获得的信息对诸如文字，图像和声音等数据的解释有很大的帮助。它的最终目标是让机器能够像人一样具有分析学习能力，能够识别文字、图像和声音等数据。 深度学习是一个复杂的机器学习算法，在语音和图像识别方面取得的效果，远远超过先前相关技术。 深度学习在搜索技术，数据挖掘，机器学习，机器翻译，自然语言处理，多媒体学习，语音，推荐和个性化技术，以及其他相关领域都取得了很多成果。深度学习使机器模仿视听和思考等人类的活动，解决了很多复杂的模式识别难题，使得人工智能相关技术取得了很大进步。 (以上均来自百度百科)</p><p>下面了解一下人工智能、机器学习和深度学习之间的关系。下图是三者之间的关系，可以看出三者之间是包含和被包含的关系。</p><p><img src="https://pic2.zhimg.com/80/v2-d6af1496d1ad7002c909a9c7eae049e1_720w.webp" alt="img"></p><h2 id="2-深度学习应用"><a href="#2-深度学习应用" class="headerlink" title="2 深度学习应用"></a><strong>2 深度学习应用</strong></h2><h2 id="2-1-机器学习的一般方法"><a href="#2-1-机器学习的一般方法" class="headerlink" title="2.1 机器学习的一般方法"></a><strong>2.1 机器学习的一般方法</strong></h2><p>机器学习按照方法主要可以分为两大类：监督学习和无监督学习。其中监督学习主要由分类和回归等问题组成，无监督学习主要由聚类和关联分析等问题组成。深度学习则属于监督学习当中的一种。下图为监督学习的一般方法。</p><p><img src="https://pic2.zhimg.com/80/v2-7d979737958c3bf04886680e50704ba1_720w.webp" alt="img"></p><h2 id="2-2-深度学习的一般方法"><a href="#2-2-深度学习的一般方法" class="headerlink" title="2.2 深度学习的一般方法"></a><strong>2.2 深度学习的一般方法</strong></h2><p>随着深度学习的爆发，最新的深度学习算法已经远远超越了传统的机器学 习算法对于数据的预测和分类精度。深度学习不需要我们自己去提取特征，而是自 动地对数据进行筛选，自动地提取数据高维特征。下图为深度学习的一般方法，与传统机器学习中的监督学习一般方法（如上图）相比，少了特征工程，节约了工程师们大量工作时间。</p><p><img src="https://pic4.zhimg.com/80/v2-4252fbba6862cdb9ba0dfca93df2a1eb_720w.webp" alt="img"></p><p>神经网络应用的突破领域之一是控制论，神经网络有着一套完美的反馈机制，给控制论增添了不少色彩。而深度学习的出现就 如寒武纪生命大爆发一样，前几年我们或许听到更多的是大数据处理、数据挖掘， 而如今在科技创新的生态中，几乎每个人都在谈论深度学习、人工智能。下面简单 来介绍关于深度学习的应用。</p><p><strong>(1) 图像处理</strong></p><p><img src="https://pic2.zhimg.com/80/v2-28d76399304586fd50e020040183def1_720w.webp" alt="img"></p><p><strong>(2) 自动驾驶</strong></p><p><img src="https://pic1.zhimg.com/80/v2-16f3082fdae9dcb5a74f0aae7a47027c_720w.webp" alt="img"></p><p><strong>(3) 机器人</strong> 波士顿动力机器人</p><p><img src="https://pic2.zhimg.com/v2-74ab31b6e668b56110bd44a6e55c8ce9_b.jpg" alt="动图封面"></p><p><img src="https://pic1.zhimg.com/v2-8fce5323326c931b2ee1efb0c64f5afc_b.jpg" alt="动图封面"></p><p><strong>(4) 医疗健康诊断</strong></p><p><img src="https://pic2.zhimg.com/80/v2-3b747e26af8df684541e94654edd4cdd_720w.webp" alt="img"></p><p><img src="https://pic1.zhimg.com/80/v2-8026d16a0a6bcc87cdc44d58703eb260_720w.webp" alt="img"></p><p>深度学习技术己经开始渗透到每一个领域当中，使得机器学习能够实现更多的应用场景，并且极大地拓展了人工智能的领域范畴。从无人驾驶汽车、无人驾驶飞机，到生物医学的预防性诊断、病理预测，甚至是更加贴近年轻一代的电影推荐、购物指南，几乎所有领域都可以使用深度学习。</p><h2 id="3-GPU的迅速发展"><a href="#3-GPU的迅速发展" class="headerlink" title="3 GPU的迅速发展"></a><strong>3 GPU的迅速发展</strong></h2><p>GPU (Graphics Processing Unit, 图形处理器) 作为硬件加速器之一，通过大量图形处理单元与 CPU 协同工作，对深度学习、数据分析，以及大量计算的工程应用进行加速 。 从 2007 年 NVIDIA 公司发布了第一个支持 CUDA 的 GPU 后， GPU 的应用范围不断拓展，从政府实验室、大学、企业的大型数据中心，到现今非常火热的人工智能汽车、无人驾驶飞机和机器人等嵌入式平台， GPU 都发挥着巨大的作用。 CUDA (Compute Unified Device Architecture, 统一计算设备架构)。随着显卡的发展， GPU 越来越强大， GPU 开始主要为显示图像做优化，在计算上已经超越了通用的 CPU 。 如此强大的芯片如果只是作为显卡就太浪费了，因此 NVIDIA 推出 CUDA 这一通用并行计算架构，该架构使 GPU 能够解决复杂的计算问题 。</p><h2 id="3-1-GPU与显卡的区别"><a href="#3-1-GPU与显卡的区别" class="headerlink" title="3.1 GPU与显卡的区别"></a><strong>3.1 GPU与显卡的区别</strong></h2><p>GPU只是显卡上的一个核心处理芯片，是显卡的心脏，不能单独作为外接扩展卡使用，GPU因并行计算任务较重，所以功耗较大，只能焊接在显卡的电路板上使用。显卡上都有GPU，它是区分显性能的最主要元件，显卡也叫显示适配器，分为独立显卡和主板上集成显卡，独立显卡主要由GPU、显存和接口电路构成，集成显卡没有独立显存而是使用主板上的内存。 GPU是图形处理器，一般GPU就是焊接在显卡上的，大部分情况下，我们所说GPU就等于指显卡，但是实际情况是GPU是显示卡的“心脏”，是显卡的一个核心零部件，核心组成部分。它们是“寄生与被寄生”关系。GPU本身并不能单独工作，只有配合上附属电路和接口，才能工作。这时候，它就变成了显卡 参考链接: <a href="https://link.zhihu.com/?target=https://baijiahao.baidu.com/s?id=1607965696317204020&wfr=spider&for=pc">https://baijiahao.baidu.com/s?id=1607965696317204020&wfr=spider&for=pc</a></p><h2 id="3-2-GPU-与-CPU-区别"><a href="#3-2-GPU-与-CPU-区别" class="headerlink" title="3.2 GPU 与 CPU 区别"></a><strong>3.2 GPU 与 CPU 区别</strong></h2><p>比较 GPU 和 CPU ，就是比较它们两者如何处理任务。如下图图 1-9 所示， CPU 使用几个核心处理单元去优化串行顺序任务，而 GPU 的大规模并行架构拥有数以千计的更小、更高效的处理单元，用于处理多个并行小任务。 CPU 拥有复杂的系统指令，能够进行复杂的任务操作和调度，两者是互补关系，而不能相互代替。</p><p><img src="https://pic3.zhimg.com/80/v2-d22ac3671a591f3e60ec468dcd60171a_720w.webp" alt="img"></p><p>GPU 是大规模并行架构，处理并行任务毫无疑问是非常快的，深度学习需要高 效的矩阵操作和大量的卷积操作， GPU 的并行架构再适合不过。简单来说，确实如此，但是为什么 GPU 进行矩阵操作和卷积操作会比 CPU 要快呢？真正原因是 <strong>GPU具有如下特性</strong> ： <strong>(1) 高带宽</strong></p><p><strong>(2) 高速的缓存性能</strong></p><p><strong>(3) 并行单元多</strong></p><p>在执行多任务时， CPU 需要等待带宽，而 GPU 能够优化带宽。举个简单的例子，我们可以把 CPU 看作跑车， GPU 是大卡车，如下图图 1-10 所示任务就是要把一堆货物从北京搬运到广州。 CPU（跑车〉可以快速地把数据（货物〉从内存读入 RAM 中，然而 GPU (大卡车〉装货的速度就好慢了。不过后面才是重点， CPU (跑车）把这堆数据（货物）从北京搬运到广州｜需要来回操作很多次，也就是往返京广线很多次，而 GPU (大卡车）只需要一 次就可以完成搬运（一次可以装载大量数据进入内存）。换言之， CPU 擅长操作小的内存块，而 GPU 则擅长操作大的内存块 。 CPU 集群大概可以达到 50GB&#x2F;s 的带宽总量，而等量的 GPU 集群可以达到 750GB&#x2F;s 的带宽量。</p><p><img src="https://pic2.zhimg.com/80/v2-602a35ffcc3027a4925336dbd8fe1611_720w.webp" alt="img"></p><p>如果让一辆大卡车去装载很多堆货物，就要等待很长的时间了，因为要等待大卡车从北京运到广州，然后再回来装货物。设想一下，我们现在拥有了跑车车队和卡车车队（线程并行〉，运载一堆货物（非常大块的内存数据需要读入缓存，如大型矩阵）。我们会等待第一辆卡车，但是后面就不需要等待的时间了，因为在广州会有一队伍的大卡车正在排队输送货物（数据），这时处理器就可以直接从缓存中读取数据了。在线性并行的情况下， GPU 可以提供高带宽，从而隐藏延迟时间。这也就是GPU 比 CPU 更适合处理深度学习的原因。</p><h2 id="3-3-GPU种类"><a href="#3-3-GPU种类" class="headerlink" title="3.3 GPU种类"></a><strong>3.3 GPU种类</strong></h2><p>特别是最近几年，随着 GPU处理能力的飞速进步 ，在 2012 年需要 l 个月才能完成的深度学习训练，在 2015 年只需几天即可完成 。 在这样的背景下，深度学习的发展恰逢其时，将会引发进一步的革新和发展。</p><p>对于深度学习的加速器 GPU，现在市面上主要的品牌有 AMD 、 NVIDIA 、Intel 的 Xeon Phi，如下图所示。</p><p><img src="https://pic2.zhimg.com/80/v2-1f1c9957b8296ddf5a2a1d72f9940a89_720w.webp" alt="img"></p><p>NVIDIA公司的GUP使用最为广泛，NVIDIA 的计算加速标准库 cuDNN 使得工程师在 CUDA 平台中构建深度学习变得非常容易，而且在同 一张显卡的前提下比没有使用 cnDNN 的速度提升 5 倍之多。有良好的生态。下图是NVIDIA公司的三种类型的GPU。</p><p><img src="https://pic2.zhimg.com/80/v2-e1b343edbe0e2f91f52e6da88c42e5b5_720w.webp" alt="img"></p><p>其中，</p><p>(1) <strong>GeForce 系列面向大众</strong>，常见的有：GeForce GTX 1080, GeForce GTX 1080 Ti, GeForce GTX 2080 Ti ；</p><p>(2) <strong>Tesla 系列面向科学计算</strong>；</p><p>(3) <strong>Tegra 系列面向嵌入式的 GPU 主板</strong>。</p><h2 id="导航"><a href="#导航" class="headerlink" title="导航"></a><strong>导航</strong></h2><h3 id="新闻资讯"><a href="#新闻资讯" class="headerlink" title="新闻资讯"></a><strong>新闻资讯</strong></h3><ul><li><a href="https://link.zhihu.com/?target=https://www.analyticsvidhya.com/blog/">Analytics Vidhya</a>: 为数据科学专业人员提供基于社区的知识门户</li><li><a href="https://link.zhihu.com/?target=https://distill.pub/">Distill</a>: 展示机器学习的最新文章</li><li><a href="https://link.zhihu.com/?target=https://news.google.com/topics/CAAqIggKIhxDQkFTRHdvSkwyMHZNREZvZVdoZkVnSmxiaWdBUAE?hl=en-US&gl=US&ceid=US:en">Google News</a>: Google News Machine learning</li><li><a href="https://link.zhihu.com/?target=https://www.kdnuggets.com/?from=www.mlhub123.com">kdnuggets</a>: Machine Learning, Data Science, Big Data, Analytics, AI</li><li><a href="https://link.zhihu.com/?target=http://news.mit.edu/topic/machine-learning?from=www.mlhub123.com">MIT News</a>: Machine learning | MIT News</li><li><a href="https://link.zhihu.com/?target=http://www.17bigdata.com/?from=www.mlhub123.com">17bigdata</a>: 专注数据分析、挖掘、大数据相关领域的技术分享、交流</li><li><a href="https://link.zhihu.com/?target=https://www.jiqizhixin.com/?from=www.mlhub123.com">机器之心</a>: 机器之心 | 全球人工智能信息服务</li><li><a href="https://link.zhihu.com/?target=https://www.leiphone.com/?from=www.mlhub123.com">雷锋网</a>: 雷锋网 | 读懂智能，未来</li><li><a href="https://link.zhihu.com/?target=https://www.afenxi.com/?from=www.mlhub123.com">数据分析网</a>: 数据分析网 - 大数据学习交流第一平台</li><li><a href="https://www.zhihu.com/topic/19559450/hot?from=www.mlhub123.com">知乎主题</a>: 知乎机器学习热门主题</li><li><a href="https://link.zhihu.com/?target=http://www.zhuanzhi.ai/">专知</a>：AI知识分发服务平台</li></ul><h3 id="社区交流"><a href="#社区交流" class="headerlink" title="社区交流"></a><strong>社区交流</strong></h3><ul><li><a href="https://link.zhihu.com/?target=http://www.6aiq.com/?from=www.mlhub123.com">AIQ</a>: 机器学习大数据技术社区</li><li><a href="https://link.zhihu.com/?target=https://www.datatau.com/?from=www.mlhub123.com">DataTau</a>: 人工智能领域的Hacker News</li><li><a href="https://link.zhihu.com/?target=https://mathoverflow.net/?from=www.mlhub123.com">MathOverflow</a>: 数学知识问答社区</li><li><a href="https://link.zhihu.com/?target=https://medium.com/">Medium</a>: 一个涵盖人工智能、机器学习和深度学习相关领域的自由、开放平台</li><li><a href="https://link.zhihu.com/?target=http://www.paperweekly.site/?from=www.mlhub123.com">PaperWeekly</a>: 一个推荐、解读、讨论和报道人工智能前沿论文成果的学术平台</li><li><a href="https://link.zhihu.com/?target=https://www.quora.com/pinned/Machine-Learning?from=www.mlhub123.com">Quora</a>: Quora | 机器学习主题</li><li><a href="https://link.zhihu.com/?target=https://www.reddit.com/r/MachineLearning/?from=www.mlhub123.com">Reddit</a>: Reddit | 机器学习板块</li><li><a href="https://link.zhihu.com/?target=http://www.shortscience.org/?from=www.mlhub123.com">ShortScience</a>: 用最简单的篇幅去概况科学著作</li><li><a href="https://link.zhihu.com/?target=http://sofasofa.io/index.php?from=www.mlhub123.com">SofaSofa</a>: 做最好的数据科学社区</li><li><a href="https://link.zhihu.com/?target=https://twitter.com/StatMLPapers">Twitter</a>: Twitter | 机器学习论文版块</li><li><a href="https://link.zhihu.com/?target=http://www.ziiai.com/?from=www.mlhub123.com">极智能</a>: 人工智能技术社区</li></ul><h3 id="优质博文"><a href="#优质博文" class="headerlink" title="优质博文"></a><strong>优质博文</strong></h3><ul><li><a href="https://link.zhihu.com/?target=https://ai.googleblog.com/">Google AI Blog</a>: 谷歌AI博客</li><li><a href="https://link.zhihu.com/?target=https://handong1587.github.io/">handong1587</a>: 深度学习各个方向资源汇总，及各大顶级会议&#x2F;期刊资源</li><li><a href="https://link.zhihu.com/?target=https://machinelearningmastery.com/blog?from=www.mlhub123.com">Machine Learning Mastery</a>: 帮助开发人员使用机器学习的知识解决复杂的问题</li><li><a href="https://link.zhihu.com/?target=https://blog.paralleldots.com/">paralleldots</a>：一个提供随时可用的一流AI解决方案的博客</li><li><a href="https://link.zhihu.com/?target=https://blog.statsbot.co/?from=www.mlhub123.com">Stats and Bots - Medium</a>: 机器学习应用程序和代码的实用指南</li><li><a href="https://link.zhihu.com/?target=https://www.cnblogs.com/tornadomeet/archive/2012/06/24/2560261.html?from=www.mlhub123.com">tornadomeet的博客</a>: 很详细的ML&amp;DL学习博客</li><li><a href="https://link.zhihu.com/?target=http://www.wildml.com/">wildml</a>：Artificial Intelligence, Deep Learning, and NLP</li><li><a href="https://link.zhihu.com/?target=https://weibo.com/fly51fly?topnav=1&wvr=6&topsug=1">爱可可-爱生活</a>: 知名互联网资讯博主</li><li><a href="https://zhuanlan.zhihu.com/YJango">超智能体</a>: 分享最通俗易懂的深度学习教程</li><li><a href="https://zhuanlan.zhihu.com/ainote">人工智能笔记</a>: 人工智能从入门到AI统治世界</li></ul><h3 id="资源检索"><a href="#资源检索" class="headerlink" title="资源检索"></a><strong>资源检索</strong></h3><ul><li><a href="https://link.zhihu.com/?target=https://arxiv.org/">arXiv</a>: 康奈尔大学运营的学术预印本发布的平台</li><li><a href="https://link.zhihu.com/?target=http://www.arxiv-sanity.com/?from=www.mlhub123.com">Arxiv Sanity</a>: 论文查询推荐</li><li><a href="https://link.zhihu.com/?target=http://www.gitxiv.com/?from=www.mlhub123.com">GitXiv</a>: arXiv的成果开源实现平台</li><li><a href="https://link.zhihu.com/?target=https://www.cn-ki.net/">iData</a>: iData-知识检索</li><li><a href="https://link.zhihu.com/?target=https://modeldepot.io/search">ModelDepot</a>：文献模型源代码搜索下载</li><li><a href="https://link.zhihu.com/?target=https://paperswithcode.com/?from=www.mlhub123.com">Papers with Code</a>: 将论文与开源代码实现结合</li><li><a href="https://link.zhihu.com/?target=https://sci-hub.tw/">SCI-HUB</a>: 找论文必备</li><li><a href="https://link.zhihu.com/?target=https://lunwen.im/?from=www.mlhub123.com">猫咪论文</a>: 简单自由的论文下载平台</li></ul><h3 id="比赛实践"><a href="#比赛实践" class="headerlink" title="比赛实践"></a><strong>比赛实践</strong></h3><ul><li><a href="https://link.zhihu.com/?target=https://biendata.com/">biendata</a>：数据科学竞赛平台</li><li><a href="https://link.zhihu.com/?target=http://www.pkbigdata.com/?from=www.mlhub123.com">DataCastle</a>: 中国领先的数据科学竞赛平台</li><li><a href="https://link.zhihu.com/?target=http://www.datafountain.cn/%23/?from=www.mlhub123.com">DataFountain</a>: DF,CCF指定专业大数据竞赛平台</li><li><a href="https://link.zhihu.com/?target=https://www.kaggle.com/?from=www.mlhub123.com">Kaggle</a>: 为数据科学家提供举办机器学习竞赛</li><li><a href="https://link.zhihu.com/?target=http://www.kdd.org/kdd-cup?from=www.mlhub123.com">KDD-CUP</a>: 国际知识发现和数据挖掘竞赛</li><li><a href="https://link.zhihu.com/?target=http://research.xiaojukeji.com/trainee.html?from=www.mlhub123.com">滴滴新锐</a>: 滴滴面向全球高校博士、硕士、优秀本科生的精英人才计划</li><li><a href="https://link.zhihu.com/?target=https://jdder.jd.com/">JDD空间站</a>: 京东算法赛事平台</li><li><a href="https://link.zhihu.com/?target=http://www.saikr.com/?from=www.mlhub123.com">赛氪网</a>: 汇集以高校竞赛为主，活动、社区为辅的大学生竞赛活动平台</li><li><a href="https://link.zhihu.com/?target=https://tianchi.aliyun.com/?from=www.mlhub123.com">天池大数据</a>: 大数据竞赛、大数据解决方案、数据科学家社区、人工智能、机器学习</li></ul><h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a><strong>资源</strong></h2><h3 id="课程学习"><a href="#课程学习" class="headerlink" title="课程学习"></a><strong>课程学习</strong></h3><ul><li><a href="https://link.zhihu.com/?target=https://github.com/zekelabs/data-science-complete-tutorial">data-science-complete-tutorial</a>: 数据科学完整入门指南</li><li><a href="https://link.zhihu.com/?target=https://v.youku.com/v_show/id_XMjcwMDQyOTcxMg==.html?spm=a2h0j.11185381.listitem_page1.5!4~A&&f=49376145">David Silver</a>: David Silver 深度强化学习课程</li><li><a href="https://link.zhihu.com/?target=http://www.fast.ai/">fast.ai</a>: Making neural nets uncool again</li><li><a href="https://link.zhihu.com/?target=https://www.zybuluo.com/hanbingtao/note/433855">hanbt</a>: 零基础入门深度学习，深入浅出，很不错的入门教程</li><li><a href="https://link.zhihu.com/?target=https://coding.imooc.com/class/169.html">liuyubobobo</a>: Python3 入门机器学习</li><li><a href="https://link.zhihu.com/?target=https://metacademy.org/">Metacademy</a>: 知识点检索并画出通向这个知识点的知识图谱</li><li><a href="https://link.zhihu.com/?target=https://www.kaggle.com/learn/time-series-with-siraj">Siraj Raval：时序预测</a>: Kaggle免费课程：时序预测</li><li><a href="https://link.zhihu.com/?target=https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">Two Minute Papers</a>: YouTube | 最简短的语言概况最新的热点论文</li><li><a href="https://link.zhihu.com/?target=https://github.com/yandexdataschool/nlp_course">YSDA nlp_course</a>: YSDA course in Natural Language Processing</li><li><a href="https://link.zhihu.com/?target=https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw">3Blue1Brown</a>: YouTube | 数学基础频道</li><li><a href="https://link.zhihu.com/?target=http://space.bilibili.com/88461692/%23/">3Blue1Brown 中文</a>: Bilibili | 数学基础频道</li><li><a href="https://link.zhihu.com/?target=https://zh.diveintodeeplearning.org/">动手学深度学习</a>: 面向中文读者的能运行、可讨论的深度学习教科书</li><li><a href="https://link.zhihu.com/?target=https://developers.google.cn/machine-learning/crash-course/">谷歌：机器学习速成课程</a>: Google制作的节奏紧凑、内容实用的机器学习简介课程</li><li><a href="https://link.zhihu.com/?target=http://speech.ee.ntu.edu.tw/~tlkagk/courses.html">李宏毅</a>: 李宏毅深度学习课程</li><li><a href="https://link.zhihu.com/?target=https://www.bilibili.com/video/av4294020/">林轩田</a>: 机器学习基石</li><li><a href="https://link.zhihu.com/?target=https://www.bilibili.com/video/av12469267">林轩田</a>: 机器学习技法</li><li><a href="https://link.zhihu.com/?target=https://github.com/roboticcam/machine-learning-notes">徐亦达</a>: 徐亦达老师机器学习课程</li><li><a href="https://link.zhihu.com/?target=https://github.com/nndl/nndl.github.io">邱锡鹏（复旦大学）</a>: 神经网络与深度学习</li><li><a href="https://link.zhihu.com/?target=http://study.163.com/course/introduction/1004570029.htm">吴恩达</a>: 机器学习课程</li><li><a href="https://link.zhihu.com/?target=https://mooc.study.163.com/smartSpec/detail/1001319001.htm">吴恩达</a>: 深度学习课程</li><li><a href="https://link.zhihu.com/?target=https://github.com/MLEveryday">MLEveryday</a>: machine learning everyday</li></ul><h3 id="资源收集"><a href="#资源收集" class="headerlink" title="资源收集"></a><strong>资源收集</strong></h3><ul><li><a href="https://link.zhihu.com/?target=https://github.com/jobbole/awesome-machine-learning-cn">awesome-machine-learning-cn</a>: 机器学习资源大全中文版，包括机器学习领域的框架、库以及软件</li><li><a href="https://link.zhihu.com/?target=https://github.com/awesomedata/awesome-public-datasets">awesome-public-datasets</a>: 各领域公开数据集下载</li><li><a href="https://link.zhihu.com/?target=https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">Coursera-ML-AndrewNg-Notes</a>: 吴恩达老师的机器学习课程个人笔记</li><li><a href="https://link.zhihu.com/?target=https://github.com/amusi/daily-paper-computer-vision">daily-paper-computer-vision</a>: 记录每天整理的计算机视觉&#x2F;深度学习&#x2F;机器学习相关方向的论文</li><li><a href="https://link.zhihu.com/?target=https://github.com/scutan90/DeepLearning-500-questions">DeepLearning-500-questions</a>：深度学习500问</li><li><a href="https://link.zhihu.com/?target=https://github.com/fengdu78/deeplearning_ai_books">deeplearning_ai_books</a>: 吴恩达老师的深度学习课程笔记及资源</li><li><a href="https://link.zhihu.com/?target=https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap">Deep-Learning-Papers-Reading-Roadmap</a>: 深度学习论文阅读路线图</li><li><a href="https://link.zhihu.com/?target=https://github.com/fighting41love/funNLP">funNLP</a>：中文语料库资源收集项目</li><li><a href="https://link.zhihu.com/?target=https://sites.google.com/site/mostafasibrahim/research/articles/how-to-start">Getting Started in Computer Vision Research</a>：计算机视觉研究入门全指南</li><li><a href="https://link.zhihu.com/?target=https://github.com/WenDesi/lihang_book_algorithm">lihang_book_algorithm</a>: 《统计学习方法》算法python实现</li><li><a href="https://link.zhihu.com/?target=https://github.com/ty4z2008/Qix/blob/master/dl.md">Machine Learning、Deep Learning</a>: ML&amp;DL资料</li><li><a href="https://link.zhihu.com/?target=https://github.com/lawlite19/MachineLearning_Python">MachineLearning_Python</a>: 机器学习算法python实现</li><li><a href="https://link.zhihu.com/?target=https://github.com/linxid/Machine_Learning_Study_Path">Machine_Learning_Study_Path</a>：机器学习过程中所看的书，视频和源码</li><li><a href="https://link.zhihu.com/?target=https://github.com/remicnrd/ml_cheatsheet">ml_cheatsheet</a>：机器学习算法速查手册</li><li><a href="https://link.zhihu.com/?target=https://github.com/MorvanZhou/tutorials">ml_tutorials</a>: 机器学习相关教程</li><li><a href="https://link.zhihu.com/?target=https://github.com/sebastianruder/NLP-progress">NLP-progress</a>：跟踪NLP各项技术的state-of-the-art进展</li><li><a href="https://link.zhihu.com/?target=https://github.com/Avik-Jain/100-Days-Of-ML-Code">100-Days-Of-ML-Code 英文版</a>：100 Days of Machine Learning Coding as proposed by Siraj Raval</li><li><a href="https://link.zhihu.com/?target=https://github.com/MLEveryday/100-Days-Of-ML-Code">100-Days-Of-ML-Code 中文版</a>：100-Days-Of-ML-Code 中文版</li><li><a href="https://link.zhihu.com/?target=https://github.com/Vay-keen/Machine-learning-learning-notes">周志华 - 机器学习</a>: 周志华《机器学习》笔记</li></ul><h3 id="开源书籍"><a href="#开源书籍" class="headerlink" title="开源书籍"></a><strong>开源书籍</strong></h3><ul><li><a href="https://link.zhihu.com/?target=https://github.com/exacity/deeplearningbook-chinese">deeplearningbook-chinese</a>: 深度学习中文版</li><li><a href="https://link.zhihu.com/?target=https://github.com/DOsinga/deep_learning_cookbook">deep_learning_cookbook</a>: 深度学习手册</li><li><a href="https://link.zhihu.com/?target=https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF">hands_on_Ml_with_Sklearn_and_TF</a>: Sklearn与TensorFlow机器学习实用指南</li><li><a href="https://link.zhihu.com/?target=https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning</a>: 一份指南，教你如何构建具有可解释性的黑盒模型</li><li><a href="https://link.zhihu.com/?target=http://neuralnetworksanddeeplearning.com/index.html">Neural Networks and Deep Learning</a>: 深度学习开源书籍</li><li><a href="https://link.zhihu.com/?target=https://github.com/zhanggyb/nndl">Neural Networks and Deep Learning</a>: 深度学习开源书籍 - 中文</li><li><a href="https://link.zhihu.com/?target=https://github.com/jakevdp/PythonDataScienceHandbook">PythonDataScienceHandbook</a>: Python数据科学手册</li><li><a href="https://link.zhihu.com/?target=https://github.com/open-source-for-science/TensorFlow-Course">TensorFlow-Course</a>: 简单易学的TensorFlow教程</li><li><a href="https://link.zhihu.com/?target=https://github.com/apachecn/MachineLearning">机器学习实战</a>: Machine Learning in Action（机器学习实战）</li><li><a href="https://link.zhihu.com/?target=https://tf.wiki/">简单粗暴 TensorFlow 2</a>: 一本简明的 TensorFlow 2 入门指导手册</li></ul><h3 id="实战项目"><a href="#实战项目" class="headerlink" title="实战项目"></a><strong>实战项目</strong></h3><ul><li><a href="https://link.zhihu.com/?target=https://github.com/ageitgey/face_recognition">face_recognition</a>: 世界上最简单的人脸识别库</li><li><a href="https://link.zhihu.com/?target=https://github.com/lllyasviel/style2paints">style2paints</a>: 线稿自动上色</li><li><a href="https://link.zhihu.com/?target=http://www.flyai.com&_channel_track_key=RgOANn6Z">flyai.com</a> 线上竞赛平台，算法工程师竞赛之地</li></ul><h3 id="方法论"><a href="#方法论" class="headerlink" title="方法论"></a><strong>方法论</strong></h3><ul><li><a href="https://link.zhihu.com/?target=https://space.bilibili.com/344849038/dynamic">face_recognition</a>: 学习观</li></ul><h2 id="文档"><a href="#文档" class="headerlink" title="文档"></a><strong>文档</strong></h2><h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a><strong>Python</strong></h3><ul><li><a href="https://link.zhihu.com/?target=http://caffe.berkeleyvision.org/">Caffe</a>: 一个基于表达式，速度和模块化原则创建的深度学习框架</li><li><a href="https://link.zhihu.com/?target=https://caffe2.ai/docs/getting-started.html?platform=windows&configuration=compile">Caffe2</a>: Caffe2官方文档</li><li><a href="https://link.zhihu.com/?target=https://docs.chainer.org/en/stable/">Chainer</a>: 基于Python的独立的深度学习模型开源框架</li><li><a href="https://link.zhihu.com/?target=https://docs.microsoft.com/en-us/cognitive-toolkit/">CNTK</a>: CNTK官方文档</li><li><a href="https://link.zhihu.com/?target=https://radimrehurek.com/gensim/index.html">Gensim</a>: 包含可扩展的统计语义，分析纯文本文档的语义结构，以及检索相似语义的文档等功能</li><li><a href="https://link.zhihu.com/?target=https://keras.io/">Keras</a>: Keras官方文档</li><li><a href="https://link.zhihu.com/?target=https://matplotlib.org/tutorials/index.html">Matplotlib</a>: Matplotlib官方文档</li><li><a href="https://link.zhihu.com/?target=http://mxnet.incubator.apache.org/tutorials/index.html">MXNet</a>: MXNet官方文档</li><li><a href="https://link.zhihu.com/?target=http://neon.nervanasys.com/index.html/">Neon</a>: Nervana公司一个基于Python的深度学习库</li><li><a href="https://link.zhihu.com/?target=http://www.numpy.org/">NumPy</a>: NumPy官方文档</li><li><a href="https://link.zhihu.com/?target=http://pandas.pydata.org/pandas-docs/stable/">pandas</a>: pandas官方文档</li><li><a href="https://link.zhihu.com/?target=http://pybrain.org/docs/">PyBrain</a>: 一个模块化的Python机器学习库</li><li><a href="https://link.zhihu.com/?target=http://deeplearning.net/software/pylearn2/">Pylearn2</a>: 构建于Theano之上的机器学习库</li><li><a href="https://link.zhihu.com/?target=https://pytorch.org/tutorials/">PyTorch</a>: PyTorch官方文档</li><li><a href="https://link.zhihu.com/?target=https://seaborn.pydata.org/">Seaborn</a>: Seaborn官方文档</li><li><a href="https://link.zhihu.com/?target=http://scikit-learn.org/stable/documentation.html">scikit-learn</a>: scikit-learn官方文档</li><li><a href="https://link.zhihu.com/?target=http://www.statsmodels.org/stable/index.html">Statsmodels</a>: 用来探索数据，估计统计模型，进行统计测试</li><li><a href="https://link.zhihu.com/?target=https://www.tensorflow.org/tutorials/">TensorFlow</a>: TF官方文档</li><li><a href="https://link.zhihu.com/?target=http://deeplearning.net/software/theano/">Theano</a>: 允许高效地定义、优化以及评估涉及多维数组的数学表达式</li><li><a href="https://link.zhihu.com/?target=https://spinningup.openai.com/en/latest/">openai</a>: 强化学习</li></ul><h3 id="C-C"><a href="#C-C" class="headerlink" title="C &amp; C++"></a><strong>C &amp; C++</strong></h3><ul><li><a href="https://link.zhihu.com/?target=http://dlib.net/">dlib</a>: 实用的机器学习和数据分析工具包</li></ul><h3 id="Java-Scala"><a href="#Java-Scala" class="headerlink" title="Java &amp; Scala"></a><strong>Java &amp; Scala</strong></h3><ul><li><a href="https://link.zhihu.com/?target=https://deeplearning4j.org/">DeepLearning4j</a>: 基于JAVA和Scala的商业级开源分布式深度学习框架</li></ul><p><a href="https://link.zhihu.com/?target=https://github.com/howie6879/mlhub123">https://github.com/howie6879/ml</a></p>]]></content>
      
      
      <categories>
          
          <category> 实用教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 教学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2023热门的大型语言模型(LLMs)汇总</title>
      <link href="/2024/03/29/2023%E7%83%AD%E9%97%A8%E7%9A%84%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-LLMs-%E6%B1%87%E6%80%BB/"/>
      <url>/2024/03/29/2023%E7%83%AD%E9%97%A8%E7%9A%84%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-LLMs-%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<p>大模型（LLMs）是一种人工智能模型，旨在理解和生成人类语言。</p><p>它们通过在大量的文本数据上进行训练，可以执行广泛的任务，包括文本总结、翻译、情感分析等等。这些模型通常基于深度学习架构，如转换器，这使它们在各种自然语言处理任务上表现出令人印象深刻的能力。</p><p>大模型领域在国内外都取得了显著的成就，各个国家和地区的企业、机构以及学术界都在积极投入资源和努力，推动大模型技术的发展。</p><p>比如，在国外，OpenAI 推出的基于 GPT-3.5 的大型语言模型 ChatGPT，由于其优秀的表现，ChatGPT 及其背后的大型语言模型迅速成为人工智能领域的热门话题，吸引了广大科研人员和开发者的关注和参与。</p><p>在国内，截至2023年8月31日，多家大模型企业和机构正式宣布其服务已经上线，并向全社会开放。目前，百度、智谱、百川、字节、商汤、中科院（紫东太初）等8个企业和机构的大模型名列第一批备案名单，它们可以正式上线并向公众提供服务。</p><p>为了让大家能更加直观的看到大模型领域的发展，我们整理了国内外顶尖的大模型，提供给大家参考和使用。</p><h2 id="国外大模型汇总"><a href="#国外大模型汇总" class="headerlink" title="国外大模型汇总"></a><strong>国外大模型汇总</strong></h2><h3 id="Open-AI"><a href="#Open-AI" class="headerlink" title="Open AI"></a><strong>Open AI</strong></h3><p><strong>ChatGPT</strong></p><p>ChatGPT是由GPT-3语言模型驱动的开源聊天机器人。它能够与用户进行自然语言对话交流。ChatGPT经过广泛的主题训练，可以帮助回答问题、提供信息和生成创意内容等各种任务。它被设计成友好和乐于助人的，可以适应不同的对话风格和语境。通过ChatGPT，您可以在最新新闻、时事、爱好和个人兴趣等各种话题上进行有趣而富有信息的对话。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/5ed0e04291e011915d9e43ee">Language Models are Few-Shot Learnerswww.aminer.cn/pub/5ed0e04291e011915d9e43ee<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><p><strong>GPT-4</strong></p><p>2023年3月，OpenAI 发布了多模态预训练大模型 GPT-4，能接受图像和文本输入，再输出正确的文本回复。实验表明，GPT-4 在各种专业测试和学术基准上的表现与人类水平相当。例如，它通过了模拟律师考试，且分数在应试者的前 10% 左右；相比之下，GPT-3.5 的得分在倒数 10% 左右。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/641130e378d68457a4a2986f">GPT-4 Technical Reportwww.aminer.cn/pub/641130e378d68457a4a2986f<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><h3 id="Google"><a href="#Google" class="headerlink" title="Google"></a><strong>Google</strong></h3><p><strong>LaMDA</strong></p><p>LaMDA是一系列专门用于对话的基于Transformer的模型。这些模型拥有多达1370亿个参数，并使用1.56万亿个公开对话数据进行训练。LaMDA可以在各种话题上进行自由流畅的对话。与传统的聊天机器人不同，它不受预定义路径的限制，可以根据对话的方向进行自适应调整。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/61ea249b5244ab9dcbabc7ac">LaMDA: Language Models for Dialog Applicationswww.aminer.cn/pub/61ea249b5244ab9dcbabc7ac<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><p><strong>PaLM</strong></p><p>PaLM是一个具有5400亿个参数的语言模型，能够处理各种任务，包括复杂的学习和推理。它在语言和推理测试中可以胜过最先进的语言模型和人类。PaLM系统采用了少样本学习的方法，可以从少量的数据中泛化，近似模拟人类学习和应用知识来解决新问题的方式。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/624d050e5aee126c0f4a7920">undefined - AMinerwww.aminer.cn/pub/624d050e5aee126c0f4a7920</a></p><p><strong>mT5</strong></p><p>多语言T5（mT5）是一个由130亿个参数组成的文本到文本的Transformer模型。它是在mC4语料库上进行训练的，涵盖了101种语言，如阿姆哈拉语、巴斯克语、科萨语、祖鲁语等。mT5能够在许多跨语言自然语言处理任务上达到最先进的性能水平。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/5f92ba5191e011edb3573ba5">mT5: A massively multilingual pre-trained text-to-text transformerwww.aminer.cn/pub/5f92ba5191e011edb3573ba5<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><h3 id="Deepmind"><a href="#Deepmind" class="headerlink" title="Deepmind"></a><strong>Deepmind</strong></h3><p><strong>Gopher</strong></p><p>DeepMind的语言模型Gopher在回答关于科学、人文等专业主题的问题等任务上比现有的大型语言模型更准确，而在逻辑推理和数学等其他任务上与它们相当。Gopher拥有2800亿个参数可供调整，使其比OpenAI的GPT-3更大，后者只有1750亿个参数。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/61b2c0246750f848a14300ff">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopherwww.aminer.cn/pub/61b2c0246750f848a14300ff</a></p><p><strong>Chinchilla</strong></p><p>Chinchilla使用与Gopher相同的计算预算，但只有700亿个参数和四倍的数据。在许多下游评估任务中，它胜过了Gopher、GPT-3、Jurassic-1和Megatron-Turing NLG等模型。它在微调和推理方面使用的计算资源明显较少，极大地促进了下游应用的使用。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/63a413f690e50fcafd6d190a">An empirical analysis of compute-optimal large language model trainingwww.aminer.cn/pub/63a413f690e50fcafd6d190a<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><p><strong>Sparrow</strong></p><p>Sparrow是由DeepMind开发的聊天机器人，旨在正确回答用户的问题，同时减少不安全和不适当回答的风险。Sparrow的动机是解决语言模型产生不正确、带偏见或潜在有害输出的问题。Sparrow通过使用人类判断进行训练，使其比基线预训练语言模型更有帮助、更正确和更无害。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/63365e7c90e50fcafd1a2bdd">Improving alignment of dialogue agents via targeted human judgementswww.aminer.cn/pub/63365e7c90e50fcafd1a2bdd<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><p><strong>Anthropic</strong></p><p><strong>Claude</strong></p><p>Claude是一个由先进的自然语言处理驱动的基于AI的对话助手。它的目标是成为有益、无害和诚实的助手。它使用一种称为Constitutional AI的技术进行训练。在训练过程中，通过模型自我监督和其他AI安全方法，对其进行限制和奖励，以展现之前提到的行为特征。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/63a1750c90e50fcafd1f38d7">Constitutional AI: Harmlessness from AI Feedbackwww.aminer.cn/pub/63a1750c90e50fcafd1f38d7<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><h3 id="Meta"><a href="#Meta" class="headerlink" title="Meta"></a><strong>Meta</strong></h3><p><strong>OPT-IML</strong></p><p>OPT-IML是基于Meta的OPT模型的预训练语言模型，拥有1750亿个参数。OPT-IML经过微调，以在自然语言任务（如问答、文本摘要和翻译）中获得更好的性能，使用了约2000个自然语言任务进行训练。它在训练过程中更高效，并且比OpenAI的GPT-3具有更低的CO₂排放量。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/63a910a290e50fcafd2a84fd">OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalizationwww.aminer.cn/pub/63a910a290e50fcafd2a84fd<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><p><strong>BlenderBot-3</strong></p><p>BlenderBot 3是一个可以与人交互并接收反馈以提高对话能力的对话代理。BlenderBot 3是基于Meta AI公开提供的OPT-175B语言模型构建的，该模型的规模大约是其前身BlenderBot 2的58倍。该模型融合了人格、共情和知识等对话技能，并通过利用长期记忆和搜索互联网来进行有意义的对话。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/62f07ec290e50fcafde5ac5e">BlenderBot 3: a deployed conversational agent that continually learns to responsibly engagewww.aminer.cn/pub/62f07ec290e50fcafde5ac5e</a></p><p><strong>Llama</strong></p><p>LLaMA是拥有7B到65B参数的基础语言模型。作者在数万亿令牌上进行了训练，并展示了使用公开可用数据集训练最先进的模型是可能的，而不必依赖于专有和不可访问的数据集。其中，LLaMA-13B在大多数基准测试中优于GPT-3（175B），而LLaMA-65B与最佳模型，Chinchilla-70B和PaLM-540B，具有竞争力。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/63fd715e90e50fcafd14767c/llama-open-and-efficient-foundation-language-models">LLaMA: Open and Efficient Foundation Language Modelswww.aminer.cn/pub/63fd715e90e50fcafd14767c/llama-open-and-efficient-foundation-language-models<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><p><strong>Llama2</strong></p><p>Llama 2，一个包括从 70 亿到 700 亿个参数的预训练和优化的大型语言模型 (LLM) 集合。其中的 Llama 2-Chat 是针对对话场景优化的 LLM，并在大多数测试的基准上表现优于开源聊天模型。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/64b758dd1a5852438b7976ff/llama-open-foundation-and-fine-tuned-chat-models">Llama 2: Open Foundation and Fine-Tuned Chat Models - AMinerwww.aminer.cn/pub/64b758dd1a5852438b7976ff/llama-open-foundation-and-fine-tuned-chat-models</a></p><h3 id="AI21-Labs"><a href="#AI21-Labs" class="headerlink" title="AI21 Labs"></a><strong>AI21 Labs</strong></h3><p><strong>Jurassic</strong></p><p>Jurassic-1是AI21 Labs推出的开发者平台，为构建应用程序和服务提供最先进的语言模型。它提供了两个模型，其中包括Jumbo版本，是迄今为止发布的最大、最复杂的通用语言模型。这些模型非常灵活，能够生成类似于人类的文本，并解决诸如问答和文本分类等复杂任务。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/62620f1c5aee126c0f686cf5">undefined - AMinerwww.aminer.cn/pub/62620f1c5aee126c0f686cf5</a></p><h3 id="NVIDIA"><a href="#NVIDIA" class="headerlink" title="NVIDIA"></a><strong>NVIDIA</strong></h3><p><strong>Megatron-Turing NLG</strong></p><p>Megatron-Turing自然语言生成（MT-NLG）模型是一个基于Transformer的语言模型，拥有5300亿个参数，使其成为同类模型中最大且最强大的模型。它在零、一和少样本设置中超越了之前的最先进模型，并在完成预测、常识推理、阅读理解、自然语言推理和词义消歧等自然语言任务中展现了无与伦比的准确性。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/61f753205aee126c0f9c2149">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Modelwww.aminer.cn/pub/61f753205aee126c0f9c2149<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><h2 id="国内大模型汇总"><a href="#国内大模型汇总" class="headerlink" title="国内大模型汇总"></a><strong>国内大模型汇总</strong></h2><h3 id="百度"><a href="#百度" class="headerlink" title="百度"></a><strong>百度</strong></h3><p><strong>Ernie 3.0 Titan</strong></p><p>由百度和鹏程实验室联合发布，它有 260B 个参数，擅长自然语言理解和生成。它在海量非结构化数据上进行了训练，并在机器阅读理解、文本分类和语义相似性等 60 多项 NLP 任务中取得了一流的成绩。此外，泰坦还在 30 项少拍和零拍基准测试中表现出色，这表明它有能力利用少量标记数据在各种下游任务中进行泛化。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/61c53a815244ab9dcbcaf3b5">ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generationwww.aminer.cn/pub/61c53a815244ab9dcbcaf3b5<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><p><strong>Ernie Bot</strong></p><p>于 3 月份完成 “Ernie Bot “项目的内部测试。Ernie Bot 是一种人工智能语言模型，类似于 OpenAI 的 ChatGPT，能够进行语言理解、语言生成和文本到图像的生成。这项技术是全球开发生成式人工智能竞赛的一部分。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/60e441e0dfae54001623c105">ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generationwww.aminer.cn/pub/60e441e0dfae54001623c105<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><h3 id="智谱AI"><a href="#智谱AI" class="headerlink" title="智谱AI"></a><strong>智谱AI</strong></h3><p><strong>GLM</strong></p><p>一个基于自回归填空的通用预训练框架，通过在一个统一的框架中同时学习双向和单向的注意力机制，模型在预训练阶段同时学习到了上下文表示和自回归生成。在针对下游任务的微调阶段，通过完形填空的形式统一了不同类型的下游任务，从而实现了针对所有自然语言处理任务通用的预训练模型。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/622819cdd18a2b26c7ab496a">GLM: General Language Model Pretraining with Autoregressive Blank Infillingwww.aminer.cn/pub/622819cdd18a2b26c7ab496a<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><p><strong>GLM-130B</strong></p><p>GLM-130B 是一个开源开放的双语（中文和英文）双向稠密模型，拥有 1300 亿参数，模型架构采用通用语言模型（GLM）。它旨在支持在一台 A100（40G * 8）或 V100（32G * 8）服务器上对千亿规模参数的模型进行推理。在 INT4 量化方案下，GLM-130B 可以几乎不损失模型性能的情况下在 RTX 3090（24G * 4）或 GTX 1080 Ti（11G * 8）服务器上进行高效推理。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/633e476890e50fcafde59595">GLM-130B: An Open Bilingual Pre-trained Modelwww.aminer.cn/pub/633e476890e50fcafde59595<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><p><strong>ChatGLM-6B</strong></p><p>ChatGLM-6B 是一个开源的、支持中英双语问答的对话语言模型，并针对中文进行了优化。该模型基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。ChatGLM-6B 使用了和 ChatGLM 相同的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的ChatGLM-6B 虽然规模不及千亿模型，但大大降低了推理成本，提升了效率，并且已经能生成相当符合人类偏好的回答。</p><h3 id="华为"><a href="#华为" class="headerlink" title="华为"></a><strong>华为</strong></h3><p><strong>PanGu-Alpha</strong></p><p>华为开发了一种与 OpenAI 的 GPT-3 相当的中文模型，称为 PanGu-Alpha。该模型基于 1.1 TB 的中文资源，包括书籍、新闻、社交媒体和网页，包含超过 2000 亿个参数，比 GPT-3 多 2500 万个。PanGu-Alpha 能高效完成各种语言任务，如文本摘要、问题解答和对话生成。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/6087f2ff91e011e25a316d31">undefined - AMinerwww.aminer.cn/pub/6087f2ff91e011e25a316d31</a></p><h3 id="阿里"><a href="#阿里" class="headerlink" title="阿里"></a><strong>阿里</strong></h3><p><strong>M6</strong></p><p>2021年6月，阿里巴巴联合清华大学发表了一项新研究，提出了参数规模达到1000亿的中文预训练模型 M6，是当时最大规模的中文多模态预训练模型。M6的应用适用于广泛的任务，包括产品描述生成、视觉问答、问答、中国诗歌生成等，实验结果表明M6的表现优于一系列强大的基准。并且，研究人员还专门设计了文本引导的图像生成任务，并证明经过微调的 M6 可以创建具有高分辨率和丰富细节的高质量图像。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/60c320b19e795e9243fd1672">M6: Multi-Modality-to-Multi-Modality Multitask Mega-transformer for Unified Pretrainingwww.aminer.cn/pub/60c320b19e795e9243fd1672<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><p><strong>通义千问</strong></p><p>2023年4月，阿里发布了「通义千问」，一个超大规模的语言模型，具备多轮对话、文案创作、逻辑推理、多模态理解、多语言支持等功能。</p><p>而就在前几天，阿里再次推出以通义千问70亿参数模型Qwen-7B为基座语言模型：Qwen-VL，支持图文输入，具备多模态信息理解能力。除了具备基本的图文识别、描述、问答及对话能力之外，还新增了视觉定位、图像中文字理解等能力。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/64e826d63fda6d7f06c3150c">Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilitieswww.aminer.cn/pub/64e826d63fda6d7f06c3150c<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><h3 id="商汤"><a href="#商汤" class="headerlink" title="商汤"></a><strong>商汤</strong></h3><p><strong>日日新</strong></p><p>2023年4 ，商汤推出大模型 “日日新”，包括自然语言处理模型 “商量”、文生图模型 “秒画” 和数字人视频生成平台 “如影” 等。这也是继百度文心一言、阿里通义千问之后，又一国内大厂的类 ChatGPT 产品。</p><p>最近，商汤大模型团队也提出了文生图大模型RAPHAEL，详细请看论文。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/647572e0d68f896efa7b79ab">RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Pathswww.aminer.cn/pub/647572e0d68f896efa7b79ab<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><h3 id="快手"><a href="#快手" class="headerlink" title="快手"></a>快手</h3><p>KwaiYiiMath，一种增强 KwaiYiiBase1 数学推理能力的技术报告。通过应用监督微调（SFT）和基于人类反馈的强化学习（RLHF），KwaiYiiMath 在英语和中文数学任务上都有所提升。同时，作者还构建了一个小规模的中国小学数学测试集（简称 KMath），包含 188 个例子，用于评估模型生成的解题过程的正确性。实证研究表明，与类似大小的模型相比，KwaiYiiMath 在 GSM8k、CMath 和 KMath 上分别实现了最先进的（SOTA）性能。</p><p>论文：</p><p><a href="https://link.zhihu.com/?target=https://www.aminer.cn/pub/65275731939a5f4082a450ee/kwaiyiimath-technical-report?f=wb">KwaiYiiMath: Technical Reportwww.aminer.cn/pub/65275731939a5f4082a450ee/kwaiyiimath-technical-report?f=wb<img src="https://pic1.zhimg.com/v2-2a3ef4cdb4d3e0750429f858e3540768_180x120.jpg" alt="img"></a></p><p>除以上模型之外，国内模型还有百川智能模型、抖音的云雀大模型、中科院 “紫东太初”模型、上海人工智能实验室的书生大模型、MiniMax 的 ABAB 大模型等。</p><p>在2023年，国内外不断涌现出新的模型，我们目睹了大模型的爆炸式增长。随着大模型的不断演进和优化，我们可以期待它们在自然语言处理、图像识别、语音识别等领域的性能不断提升，甚至超越人类的水平。</p><p>这将推动人工智能技术在各个行业的广泛应用，从医疗到金融，从交通到教育，大模型将成为智能设备和服务的核心。我们的生活将变得更加智能化、便捷化和个性化。</p><p>当然，大模型的未来发展也面临一些挑战和问题，如隐私和安全性等。然而，随着技术的进步和应用的拓展，这些问题将逐步得到解决和克服。</p><p>总的来说，一切交给时间来证明！</p><hr><h2 id="如何使用ChatPaper？"><a href="#如何使用ChatPaper？" class="headerlink" title="如何使用ChatPaper？"></a><strong>如何使用ChatPaper？</strong></h2><p>使用ChatPaper的方法很简单，打开AMiner首页，从<strong>页面顶部导航栏</strong>或者<strong>右下角</strong>便可进入ChatPaper页面。</p><p><img src="https://pic3.zhimg.com/80/v2-b3a72358d146bf782a05d9b6371eb392_720w.webp" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> 站内资源 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ai模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Suno V3版本更新！堪称AI音乐的ChatGPT时刻</title>
      <link href="/2024/03/29/Suno-V3%E7%89%88%E6%9C%AC%E6%9B%B4%E6%96%B0%EF%BC%81%E5%A0%AA%E7%A7%B0AI%E9%9F%B3%E4%B9%90%E7%9A%84ChatGPT%E6%97%B6%E5%88%BB/"/>
      <url>/2024/03/29/Suno-V3%E7%89%88%E6%9C%AC%E6%9B%B4%E6%96%B0%EF%BC%81%E5%A0%AA%E7%A7%B0AI%E9%9F%B3%E4%B9%90%E7%9A%84ChatGPT%E6%97%B6%E5%88%BB/</url>
      
        <content type="html"><![CDATA[<p><strong>一句话总结</strong></p><p>Suno AI音乐平台发布了V3版本，标志着AI音乐创作领域的一个重要进步，类似于ChatGPT在文本生成领域的影响。</p><p><img src="https://pic1.zhimg.com/80/v2-6878a32e86cbde25dd062ffe7a663058_720w.webp" alt="img"></p><p><strong>关键信息点</strong></p><ul><li>Suno AI是专注于生成式AI音乐的平台，最新发布的V3版本在音质、咬字和节奏编排上有显著提升。</li><li>V3版本的AI音乐生成质量已经达到了值得关注和推荐的水平。</li><li>Suno AI提供了两种模式：Custom Mode（自定义模式，可输入歌词）和Instrumental（生成纯音乐）。</li><li>用户可以通过编写prompt来指导AI创作音乐，包括音乐流派、风格、情绪、乐器等元素。</li><li>Suno AI的V3版本虽然仍有限制（如最多生成2分钟音乐），但已经足够满足大多数普通人的音乐创作需求。</li></ul><p><img src="https://pic4.zhimg.com/80/v2-587370bb7cc270127554cc48d0a82d5b_720w.webp" alt="img"></p><p><strong>内容问答</strong></p><ul><li>Q: Suno AI的V3版本有哪些改进？</li><li>A: V3版本在音质、咬字和节奏编排上有了显著提升，使得生成的音乐更加悦耳和专业。</li><li>Q: Suno AI提供哪些创作模式？</li><li>A: 提供Custom Mode（自定义模式，可输入歌词）和Instrumental（生成纯音乐）两种模式。</li><li>Q: 如何使用Suno AI生成音乐？</li><li>A: 用户需要编写一个包含音乐流派、风格、情绪、乐器等元素的prompt来指导AI创作音乐。</li></ul><p><img src="https://pic3.zhimg.com/80/v2-ebdcc8855e725feddf29c4032ee244a6_720w.webp" alt="img"></p><p><strong>Suno AI的V3版本要怎么使用？</strong></p><p>Suno AI的V3版本可以通过以下步骤使用：</p><p><strong>1.访问Suno AI平台</strong>：首先，你需要访问Suno AI的官方网站（<a href="https://link.zhihu.com/?target=https://app.suno.ai/%EF%BC%89%EF%BC%8C%E8%BF%99%E6%98%AF%E7%94%9F%E6%88%90%E9%9F%B3%E4%B9%90%E7%9A%84%E4%B8%BB%E9%A1%B5%E3%80%82">https://app.suno.ai/），这是生成音乐的主页。</a></p><p><strong>2.选择生成模式</strong>：Suno AI提供了两种主要的生成模式：</p><ol><li><ol><li><strong>Custom Mode（自定义模式）</strong>：在这个模式下，你可以自定义歌词来生成音乐。</li><li><strong>Instrumental（纯音乐模式）</strong>：选择这个模式，Suno AI将只生成没有歌词的纯音乐。</li></ol></li></ol><p><strong>编写Prompt</strong>：使用Custom Mode时，你需要编写一个prompt，这个prompt应该包含以下元素：</p><ol><li><ol><li>音乐流派（如Kpop、Heavy Metal）</li><li>音乐风格（如Slow、Broadway）</li><li>情绪（如悲伤、愤怒）</li><li>乐器（如钢琴、吉他）</li><li>主题或场景</li><li>人声描述（如愤怒的男声、忧伤的女声）</li></ol></li></ol><p>\3. 对于Instrumental模式，你需要根据你想要生成的音乐的特点来编写相应的英文词组prompt,不用写歌词。</p><p><strong>4.生成歌词</strong>：如果你不熟悉音乐创作，可以使用大型语言模型来帮助你生成歌曲的prompt和歌词。你需要提供一个故事或主题，并指定歌曲的结构（如Verse、Chorus、Bridge等），以及你希望的文学气质和押韵要求。</p><p><strong>5.提交生成</strong>：将编写好的prompt复制粘贴到Suno AI的输入框中，然后点击生成按钮。根据你的设置，AI将根据这些指示生成音乐。</p><p>通过以上步骤，你可以利用Suno AI的V3版本来创作属于你自己的音乐作品。</p><p><strong>其他事项：</strong></p><p><strong>1.免费与付费用户</strong>：免费用户每天有一定数量的积分（50积分），可以用来生成音乐。付费用户可以通过订阅服务获得更多积分，10刀一个月，给2500积分，可以生成很多玩很久。</p><p><strong>2.编辑与分享</strong>：生成的音乐可以直接分享，也可以进行进一步的编辑或与其他技术（如SVC）结合，以实现更个性化的声音效果。</p><p>对于生成式AI，我一直是按照五个模态去进行分类的：</p><p>文本、图片、声音、视频、3D。</p><p>而声音领域，可能是在我的分类里，我最感兴趣也是最喜欢的一个。</p><p>在某一个路演的PPT上，我给声音又拆成了4个细分：</p><p><img src="https://pic2.zhimg.com/80/v2-02f4643a48586050ccd176912f5f1cd5_720w.jpg" alt="img"></p><p>TTS、SVC、AI音效，我都写过，也玩了很久，而生成式AI音乐，是我一直没写过的东西。</p><p>不是我没玩，是我觉得这玩意，真的还没到值得去写去推荐的地步。。。</p><p>整个AI生成式音乐的代表，那肯定就是SunoAI了。</p><p>而且这玩意其实也火过2波了。</p><p>第一波是去年3、4月的时候，有一个很火的开源项目叫Bark，就是出自Suno之手，拿了将近32k的星标。</p><p><img src="https://pic4.zhimg.com/80/v2-f2179ca631838dd9d997b2c80eeca83b_720w.jpg" alt="img"></p><p>然后就是去年12月21号的时候，为了庆祝《海贼王》动画25周年，海贼王决定重制最开始的东海篇的那60集，然后尾田这货，给海贼王官方发了一首贺曲，叫《YO-HO-HOおれ達海賊》。</p><p><img src="https://pic3.zhimg.com/80/v2-b63bb2351d009e740983d8b2c89669aa_720w.webp" alt="img"></p><p>这事本身是个好事，但是好死不死的，这曲子是特么拿AI做的，就是用今天的主角Suno出的。。</p><p>这曲子大概就是这样，很糙，基本等于没法听的地步。</p><p><img src="https://picx.zhimg.com/v2-b5e3de36486af065a9201cc5903b9227.jpg?source=382ee89a" alt="img"></p><p>01:17</p><p>然后就被网友一通骂，你这个浓眉大眼的尾田怎么也用AI了。。。传着传着就变成《海贼王》重制版要用AI做了。。。于是骂的更凶了。。。</p><p>这个小插曲，还是挺有意思的，也间接的标明，当时的Suno的质量，是真的差。</p><p>直到今天，Suno终于上了V3版本。</p><p><img src="https://pic3.zhimg.com/80/v2-5d63262301bc08b71740b3c10c3754ee_720w.webp" alt="img"></p><p>在我听了很多demo和自己也跑了二十几首后，我觉得，AI音乐的”ChatGPT”时刻，终于到来了。</p><p>这是我的一个demo。</p><p><img src="https://pic1.zhimg.com/v2-ebe10536f4be6d422a3b2cc5ca2434e6.jpg?source=382ee89a" alt="img"></p><p>02:00</p><p>弊端就是Suno最多只能生成2分钟的音乐，所以可以听到最后，会戛然而止直接截断，但是已经比V2好很多了。</p><p>但是这个音质、咬字、节奏编排啥的，也都好太多太多了。</p><p>发给朋友听，她回了一句是：卧槽，还是好听的。</p><p>网址在此：<a href="https://link.zhihu.com/?target=https://app.suno.ai/">https://app.suno.ai/</a></p><p><img src="https://pic2.zhimg.com/80/v2-6956ea0ab882b4ca6818984ccb5c9c49_720w.webp" alt="img"></p><p>点Create就是生成的主页。</p><p>主要用两种模式，一个是打开的”Custom Mode”，这个是自定义模式，可以自己自定义歌词。</p><p>第二种是Instrumental，这个打开后就只会生成没有歌词的纯音乐了。</p><p>roll一次是同时出2首歌，一首消耗5积分，所以一次消耗10积分。</p><p>免费用户每天有50积分可以玩，可以roll5次出10首歌。</p><p>你要是氪金的话也挺便宜，10刀一个月，给2500积分，能玩很久很久。</p><p>要生成音乐的话，第一步肯定是写prompt，第二步（纯音乐没有）就是写歌词。</p><p>prompt这块，我比较推荐的写法就是：</p><blockquote><p>音乐流派（如Kpop、Heavy Metal）、音乐风格（如Slow、Broadway）、情绪（如悲伤、愤怒）、乐器（如钢琴、吉他）、主题或场景、人声描述（如愤怒的男声、忧伤的女声）</p></blockquote><p>但是说实话，写音乐的prompt对于不懂音乐的人来说，真的难，毕竟什么音乐流派、音乐风格很多都不懂，你咋写啊&#x3D; &#x3D;</p><p>歌词其实一样的道理，一首歌，正常是有结构的，比如有Verse（主歌）、Chorus（副歌）、有Bridge（快结束的时候经常那个不一样的部分）等等，普通人很多时候再写的时候也一脸懵逼，这特么咋写。。</p><p>别说大家，其实我去年刚玩Suno的时候，一样也是一脸懵逼。</p><p>让大家去现学我觉得也不现实。。。</p><p>那AI的方式，就必须用AI去解决，你说对吧。</p><p>所以，我就随手写了一个Prompt，让大模型来给你写歌曲Prompt和歌词。</p><p>什么玩意都给你弄好，你自己复制即用。就像这样：</p><p><img src="https://pic4.zhimg.com/80/v2-30b4e6477f63003467b6a9e03b8c8f77_720w.webp" alt="img"></p><p><img src="https://pic2.zhimg.com/80/v2-3d5a4cbc7ea9c5049d38692f1e562001_720w.webp" alt="img"></p><p>这个Prompt长这样：</p><blockquote><p>#01 你是歌词大师XX，现在，我需要你帮我写一段XX歌词，描述的是一个XX故事，整体歌曲时长2分钟以内，要押韵，要有文学气质，副歌部分要进行跨行重复。请按以下结构帮我创作：<br>“””<br>[instrumental intro]<br>[Verse 1]<br>&lt;歌词&gt;<br>[Chorus]<br>&lt;歌词&gt;<br>[Verse 2]<br>&lt;歌词&gt;<br>[Chorus]<br>&lt;歌词&gt;<br>[Bridge]<br>&lt;歌词&gt;<br>[Guitar solo]<br>[Chorus]<br>&lt;歌词&gt;<br>[Outro]<br>[End]<br>“””<br>#02 输出歌词以后，再根据歌词和故事内容，以英文词组的形式再给出歌曲的prompt。<br>请按以下格式帮我输出英文prompt：<br>“””&lt;音乐流派（如Kpop、Heavy Metal）&gt;、&lt;音乐风格（如Slow、Broadway）&gt;、&lt;情绪（如悲伤、愤怒）&gt;、&lt;乐器（如钢琴、吉他）&gt;、&lt;主题或场景&gt;、&lt;人声描述（如愤怒的男声、忧伤的女声）&gt;”””</p></blockquote><p>直接拿去复制开箱即可用。</p><p>然后就直接把出来的结果，全都复制粘贴进去，当然，你也可以自己改吧改吧。</p><p><img src="https://pic4.zhimg.com/80/v2-4d7b6f9db8025faab39ad271e41494d3_720w.webp" alt="img"></p><p>都扔进去之后，点那个黄的btn，直接开跑，速度挺快的，大概几十秒就出来了。</p><p>说实话，这个出来男声，让我有点心动了。。。</p><p><img src="https://picx.zhimg.com/v2-c9a5ec7df34a0b29e0ab5d6f9a7fbb75.jpg?source=382ee89a" alt="img"></p><p>02:00</p><p>当然，出来的歌，你不仅可以拿去直接发，你还可以玩一些骚操作，比如配合SVC，分离换声，换成自己的，可以参考这篇教程：</p><p><a href="https://link.zhihu.com/?target=http://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647661219&idx=1&sn=74299c51f67863360c044b45a48658cf&chksm=f007c6f4c7704fe2875e409f0c7379fdac20524ec4199af2f3eaf7cfe34348c77afe7f486141&scene=21%23wechat_redirect">AI唱歌之终极喂饭教程 - SVC的极限就在这了</a></p><p>更多的歌，大家可以自己去Suno上跑着玩，体验一下AI音乐的魅力，这种直出的感觉，还是非常爽的。</p><p>虽然对很多专业歌手或者专业制作人来说，可能觉得还偏玩具，可控性差，不好编辑等等。</p><p>但至少，一定达到了大多数普通人的水准。</p><p>且这个趋势，一定是一个不可逆的未来。</p><p>这股子风，现在，终于到了音乐。</p><p>AI音乐的”ChatGPT”时刻，我觉得，终于到来了。</p>]]></content>
      
      
      <categories>
          
          <category> 最新资讯 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ai模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ChatGPT4.0搭建并部署云端</title>
      <link href="/2024/03/28/%E5%AE%9E%E7%94%A8%E6%95%99%E7%A8%8B/ChatGPT4-0%E6%90%AD%E5%BB%BA%E5%B9%B6%E9%83%A8%E7%BD%B2%E4%BA%91%E7%AB%AF/"/>
      <url>/2024/03/28/%E5%AE%9E%E7%94%A8%E6%95%99%E7%A8%8B/ChatGPT4-0%E6%90%AD%E5%BB%BA%E5%B9%B6%E9%83%A8%E7%BD%B2%E4%BA%91%E7%AB%AF/</url>
      
        <content type="html"><![CDATA[<p>在 ChatGPT 推出之前，自然语言处理领域的研究一直面临着巨大的挑战，因为语言是非常复杂的，不同的人对同一件事可能会有不同的理解和表述方式。但 ChatGPT 的出现彻底改变了这种情况。</p><p>截止目前，ChatGPT 可能是当前互联网世界中最强大的自然语言处理模型之一，它不仅可以模拟人类的对话方式，甚至可以进行一些创造性的表达。</p><p><img src="https://pic3.zhimg.com/80/v2-92ca3577ce183c6846faf5d68f46d866_720w.webp" alt="img"></p><p>可以把 ChatGPT 当搜索引擎</p><p><img src="https://pic1.zhimg.com/80/v2-d7eb9bd542a62119e3eb955415b48e18_720w.webp" alt="img"></p><p>可以把 ChatGPT 当百科全书</p><p><img src="https://pic1.zhimg.com/80/v2-43167545d0f73873e4aa4201553b751c_720w.webp" alt="img"></p><p>可以让 ChatGPT 教我们写代码</p><p>在未来，ChatGPT 将成为人工智能应用领域的中流砥柱，促进人机交互、智能客服、在线教育等领域的发展。同时，我们可以期待 ChatGPT 将继续不断改进，成为更加智能、高效、人性化的语言模型。通过使用 ChatGPT，我们可以更加轻松地进行各种语言任务，大大提高工作效率，带来更多便利和创新。</p><p>在本篇文章中，我们将会一起探讨，如何通过 <a href="https://link.zhihu.com/?target=https://www.finclip.com/product?channel=zhihuseo">FinClip 小程序</a>，在 10分钟之内搭建属于自己的专属 ChatGPT。</p><h3 id="第一步、获取-ChatGPT-小程序源代码"><a href="#第一步、获取-ChatGPT-小程序源代码" class="headerlink" title="第一步、获取 ChatGPT 小程序源代码"></a>第一步、获取 ChatGPT 小程序源代码</h3><p>多亏 FinClip 的开发同学，我们现在已经有了一份可以直接使用的 ChatGPT 小程序源码了。有了这份源代码就不用再去考虑如何对接 ChatGPT 的服务器或者登录使用，当我们有了这份源代码之后，整体的工作已经完成了一大半。</p><p><img src="https://pic4.zhimg.com/80/v2-be1d7d686c9da77e8df341c01489e0c7_720w.webp" alt="img"></p><p>这一份小程序的代码全部开放给你</p><p>点击链接就可以获得这份小程序的源码了，我们决定将这份源代码免费开放给每一个人，任何人都可以通过这份小程序代码进行二次修改与创作。</p><p><img src="https://pic3.zhimg.com/80/v2-18a053869783608564986134d7fc8a86_720w.webp" alt="img"></p><p>小程序可以直接通过编译器进行修改</p><p>点击链接就可以获得这份小程序的源码了，我们决定将这份源代码免费开放给每一个人，任何人都可以通过这份小程序代码进行二次修改与创作。</p><p><a href="https://link.zhihu.com/?target=https://www.finclip.com/blog/content/files/2023/02/ChatGPT.zip">小程序源文件www.finclip.com/blog/content/files/2023/02/ChatGPT.zip</a></p><p>第二步、在服务器上配置 ChatGPT 相关服务</p><p><img src="https://pic4.zhimg.com/80/v2-e6a5e16d5bdd0af56ed03270f0ebe407_720w.webp" alt="img"></p><p>参考这个项目配置服务器并获得服务地址</p><p>我们选择基于在 Github 中开源的项目<a href="https://link.zhihu.com/?target=https://github.com/waylaidwanderer/node-chatgpt-api">“node-chatgpt-api”</a>快速搭建自己的 ChatGPT 服务，这个项目自从发布以来已经收获一千五百个 Star，每一天几乎都在更新，从代码质量和更新频率都看得出是可信之选。</p><blockquote><p><em>项目需要本地 Node 环境版本不低于 16.0.0，记得升级服务版本</em></p></blockquote><p><img src="https://pic2.zhimg.com/80/v2-60d024a6b4b8554d19a3b580d85afdc1_720w.webp" alt="img"></p><p>在 setting.js 文件中可以做很多二次开发</p><p>在 <strong>setting.js</strong> 文件中，我们可以对 ChatGPT 进行很多二次开发与自定义配置，除了最重要在 <strong>openApiKey</strong> 处配置 OpenAPI 秘钥，我们还可以配置一些自定义说明，ChatGPT 的名字，称呼我们的名字等等……</p><p><img src="https://pic2.zhimg.com/80/v2-be5d5c5ace449a8ce98c2f9f996a5ae1_720w.webp" alt="img"></p><p>配置密钥</p><blockquote><p><em>什么？好吧，如果你还没有 ChatGPT 的 KEY，就需要在 <a href="https://link.zhihu.com/?target=https://platform.openai.com/account/api-keys">https://platform.openai.com/account/api-keys</a></em> <em>中配置自己的 API 秘钥。</em></p></blockquote><h3 id="第三步、修改小程序服务器配置"><a href="#第三步、修改小程序服务器配置" class="headerlink" title="第三步、修改小程序服务器配置"></a>第三步、修改小程序服务器配置</h3><p>如同上文所说，我们需要在小程序中修改配置请求的服务器地址为自己的服务器地址，在小程序中我们需要定位到”&#x2F;pages&#x2F;index&#x2F;index.js”文件，并在其中搜索替换对应的 URL 字段即可。</p><p><img src="https://pic1.zhimg.com/80/v2-969807599258f52e0ad17ab5104d5004_720w.webp" alt="img"></p><p>在这里修改地址为自己的服务器地址</p><p>如同上图，我们在 index.js 文件中，通过搜索“服务”可以快速定位到代码的对应位置，随后替换“&lt;&gt;”中的地址即可。</p><h3 id="第四步、上传小程序"><a href="#第四步、上传小程序" class="headerlink" title="第四步、上传小程序"></a>第四步、上传小程序</h3><p>如果你已经按照前文所述，完成了服务器与小程序的代码配置，那现在就可以在自己的小程序中快速体验 ChatGPT 的相关功能了。</p><p><img src="https://pic1.zhimg.com/80/v2-91a816f7355ff1e4ca32537ffcd5ecac_720w.webp" alt="img"></p><p>使用 FinClip 上传发布小程序的方式非常简单，我们已经在<a href="https://link.zhihu.com/?target=https://www.finclip.com/mop/document/introduce/functionDescription/miniProgram-management.html">这里</a>提供了图文教学，简单易上手，一学就会。</p><p>如果配置服务器这件事对你有点难，也不必灰心，我们在 FinClip 中也集成了上面这个小程序，使用 FinClip App 扫描下方二维码，或者点击<a href="https://link.zhihu.com/?target=https://www.finclip.com/mop/scattered-page/%23/mini-app-share?params=c2NyZWVuc2hvdFVybD1odHRwcyUzQSUyRiUyRmFwaS5maW5jbGlwLmNvbSUyRmFwaSUyRnYxJTJGbW9wJTJGbmV0ZGlzayUyRmRvd25sb2FkJTJGNjNlY2ExNDk1YmM3MTkwMDAxMzY1OTQ4Jm1pbmlBcHBOYW1lPWNoYXRHUFQmbWluaUFwcExvZ289aHR0cHMlM0ElMkYlMkZhcGkuZmluY2xpcC5jb20lMkZhcGklMkZ2MSUyRm1vcCUyRm5ldGRpc2slMkZkb3dubG9hZCUyRjYzZWM5MDZiNWJjNzE5MDAwMTM2NTg4NiZtaW5pQXBwSWQ9NjM2OWUwMTE3NWRkZjQwMDAxMjNmNmJjJm1pbmlBcHBEZXNjPWNoYXRHUFQmbWluaUFwcExpbms9ZmF0YWU1NTQzM2JlMmY2MjkxNSUzQSUyRiUyRmFwcGxldCUyRmFwcGlkJTJGNjM2OWUwMTE3NWRkZjQwMDAxMjNmNmJjJTNGcGF0aCUzRHBhZ2VzJTJGaW5kZXglMkZpbmRleCUyNnF1ZXJ5JTNEJndlY2hhdEFwcElkPSZtaW5pQXBwUGF0aD1wYWdlcyUyRmluZGV4JTJGaW5kZXgmbWluaUFwcFF1ZXJ5PQ==">这里</a>，都可以快速打开ChatGPT 小程序。</p><h3 id="尾注"><a href="#尾注" class="headerlink" title="尾注"></a>尾注</h3><p>ChatGPT 已经成为备受全球关注的项目，广受推崇。许多人尝试从中获得商业利益，但他们通常采用的方法过于简单和低门槛，例如写作文或撰写求职信等。为了实现更高水平的商业价值，我们应该尝试更为复杂和深度定制的方法。</p><p>如果你对ChatGPT 与人工智能依然抱有非常强烈的兴趣，也可以考虑阅读这篇 <a href="https://link.zhihu.com/?target=https://tested-salto-cab.notion.site/The-Ultimate-Chat-GPT-Course-69ed24a317a942d288e740419b1ad6f6">The Ultimate Chat GPT Course</a>，热心网友通过 Notion 收集了这份英文版本的 ChatGPT 使用指南课程，在这份课程中拥有超过 1000 多种利用 ChatGPT 改善生活的方式与方法，从整理 Excel 的公式到提高生产力，从改善工作流到AI 写作，仿佛上天入地无所不能，强烈推荐你试试。</p><blockquote><p>我们在日常访问 ChatGPT 时可能会遇到“无法访问”的问题，造成这个问题的主要原因是网络问题与 OpenAI 限流。为了引导用户从免费版转向付费版，目前 OpenAI 进行了全球性质的限制，以引导用户转化为对应的付费版本。此外在遇到访问出错或拒绝请求时，也可以尝试切换正在使用的节点地区解决问题。</p><p>另，本文在编辑创作过程中，部分内容与文案由 ChatGPT 提供。</p><p><img src="https://pic2.zhimg.com/80/v2-8d912f584f196493cd0f92cc161f9d01_720w.webp" alt="img"></p><p>讲真的，我都没想到 ChatGPT 能这么全能</p></blockquote><p>当然，我们也会在以后逐渐分享我们对于 ChatGPT 的相关研究分析，如果你希望了解 ChatGPT 与小程序是如何进行美妙结合，ChatGPT 如何改变生活，小程序在生活与工作中的哪些场景能起到用处，也欢迎加入我们的开发者交流群，与我们一起探索 AI 与小程序的结合。欢迎来<a href="https://link.zhihu.com/?target=https://www.finclip.com/product?channel=zhihuseo">FinClip 社区</a>一同探讨。</p><p><img src="https://pic2.zhimg.com/80/v2-1b384c48f827d7648eed4184d8b9b895_720w.webp" alt="img"></p><p>至少，ChatGPT 认为自己在未来还能发挥这些作用</p><blockquote><p><em>我们在日常访问 ChatGPT 时可能会遇到“无法访问”的问题，造成这个问题的主要原因是网络问题与 OpenAI 限流。为了引导用户从免费版转向付费版，目前 OpenAI 进行了全球性质的限制，以引导用户转化为对应的付费版本。此外在遇到访问出错或拒绝请求时，也可以尝试切换正在使用的节点地区解决问题。</em><br><em>另，本文在编辑创作过程中，部分内容与文案由 ChatGPT 提供。</em></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 实用教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ai模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ChatGPT测试本地搭建</title>
      <link href="/2024/03/28/%E5%AE%9E%E7%94%A8%E6%95%99%E7%A8%8B/ChatGPT%E6%B5%8B%E8%AF%95%E6%9C%AC%E5%9C%B0%E6%90%AD%E5%BB%BA/"/>
      <url>/2024/03/28/%E5%AE%9E%E7%94%A8%E6%95%99%E7%A8%8B/ChatGPT%E6%B5%8B%E8%AF%95%E6%9C%AC%E5%9C%B0%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="背景需求情况"><a href="#背景需求情况" class="headerlink" title="背景需求情况"></a>背景需求情况</h2><p>1.开源大模型需要GPU或者运行慢，对硬件要求较高，也难以使用最新的资源</p><p>2.能够同时聊天和绘画（不一定非得Midjourney，使用Dall-E也能满足需求）</p><p>3.源代码部署没有很复杂，不通过Docker 、Vercel等部署</p><p>基于以上原则，分别试用了LobeChat、AnythingLLM、ChatGPT Web Midjourney Proxy、ChatGPT Plus、ChatBox等，当然还有ChatGPT-web、AutoGPT-Next-Web、ChatGPT-Next-Web、FastGPT等等，网上有不少，感兴趣的可以自行试用，都差不多。</p><h3 id="1-LobeChat"><a href="#1-LobeChat" class="headerlink" title="1.LobeChat"></a>1.LobeChat</h3><p><strong>项目地址：</strong><a href="https://link.zhihu.com/?target=https://github.com/lobehub/lobe-chat">GitHub - lobehub&#x2F;lobe-chat</a></p><p><strong>项目说明：</strong>LobeChat 是开源的高性能聊天机器人框架，支持语音合成、多模态、可扩展的（Function Call）插件系统。支持一键免费部署私人 ChatGPT&#x2F;LLM 网页应用程序。</p><p><strong>能够调用的api：</strong>OpenAI &#x2F; 智谱AI&#x2F; Perplexity &#x2F; Bedrock &#x2F;MoonshotAI，只有OpenAI能够使用国内中转（接口代理地址），其他的都是使用官方的API Key（如智谱AI开放平台的apikey）</p><p><img src="https://pic4.zhimg.com/80/v2-0a6292ab8e869f65fbb0018e2143183f_720w.webp" alt="img"></p><p><strong>本地部署（或者通过宝塔）：</strong></p><p>$ git clone <a href="https://link.zhihu.com/?target=https://github.com/lobehub/lobe-chat.git">https://github.com/lobehub/lobe-chat.git</a><br>$ cd lobe-chat<br>$ npm install<br>$ npm run dev</p><p><strong>总体感觉：</strong></p><p>（1）聊天和绘画在同一个窗口，使用Dall-E3插件绘图，每次可以绘制4张</p><p>（2）多个api可同时设置，模型切换比较方便</p><p>（3）打开网页、对话反应比较慢，有时等得不耐烦。</p><h3 id="2-AnythingLLM"><a href="#2-AnythingLLM" class="headerlink" title="2.AnythingLLM"></a>2.AnythingLLM</h3><p><strong>项目地址：</strong><a href="https://link.zhihu.com/?target=https://github.com/Mintplex-Labs/anything-llm">GitHub - Mintplex-Labs&#x2F;anything-llm</a></p><p><strong>项目说明：</strong>一款可以与任何内容聊天的私人<code>ChatGPT</code>，是高效、可定制、开源的企业级文档聊天机器人解决方案。它能够将任何文档、资源或内容片段转化为大语言模型（<code>LLM</code>）在聊天中可以利用的相关上下文。</p><p><strong>能够使用各种api：</strong></p><p><img src="https://pic3.zhimg.com/80/v2-a46a376640e9a9d18ce95c150031535e_720w.webp" alt="img"></p><h3 id="本地部署（或者通过宝塔）："><a href="#本地部署（或者通过宝塔）：" class="headerlink" title="本地部署（或者通过宝塔）："></a>本地部署（或者通过宝塔）：</h3><p>$ git clone <a href="https://github.com/Mintplex-Labs/anything-llm.git">https://github.com/Mintplex-Labs/anything-llm.git</a></p><p>$ cd anything-llm</p><p>$ npm run setup #生成env.devlopment</p><p>$ cd server</p><p>$npm install</p><p>$ npm run dev #运行后端</p><p>$ cd frontend运行npm install</p><p>$ npm run start #运行前端</p><p>运行起来后就可以配置相关模型、用户名、密码等</p><p><strong>总体感觉：</strong></p><p>（1）运行流畅，还可以根据自己喜好修改前端页面，如字体大小、输入框样式等</p><p><img src="https://pic4.zhimg.com/80/v2-cee978794cf4a8797f13527f5d9de463_720w.webp" alt="img"></p><p>（2）好像没办法绘画，或者需要设置专门的模型（没有深入试验），好像只有英文</p><p>（3）调用不同模型（OpenAI或Google）等都需要重新设置，设置切换GPT3.5、GPT4也都需要重新调整设置</p><h3 id="3-ChatGPT-Web-Midjourney-Proxy"><a href="#3-ChatGPT-Web-Midjourney-Proxy" class="headerlink" title="3.ChatGPT Web Midjourney Proxy"></a>3.ChatGPT Web Midjourney Proxy</h3><p><strong>项目地址：</strong><a href="https://link.zhihu.com/?target=https://github.com/Dooy/chatgpt-web-midjourney-proxy">GitHub - Dooy&#x2F;chatgpt-web-midjourney-proxy</a></p><p><strong>项目说明：</strong>chatgpt web, midjourney, gpts,tts, whisper 一套ui全搞定</p><p><strong>能够使用api：</strong>好像只有OpenAI或其代理商的接口</p><p><img src="https://pic1.zhimg.com/80/v2-601b5ab877b3a0a02a307bd229812210_720w.webp" alt="img"></p><h3 id="本地部署（或者通过宝塔）：-1"><a href="#本地部署（或者通过宝塔）：-1" class="headerlink" title="本地部署（或者通过宝塔）："></a>本地部署（或者通过宝塔）：</h3><p>$ git clone <a href="https://github.com/Dooy/chatgpt-web-midjourney-proxy">https://github.com/Dooy/chatgpt-web-midjourney-proxy</a></p><p>$ cd chatgpt-web-midjourney-proxy&#x2F;service</p><p>将.env.example同目录下复制一份，命名为.env</p><p>$ npm install</p><p>$ npm run start#运行后端</p><p>回到根目录下</p><p>$ npm install</p><p>$ npm run dev#运行前端</p><p><strong>总体感觉：</strong></p><p>（1）速度挺快，还能在同一界面中调用Dall-E绘图，当然也可以配置使用midjourney绘图</p><p>（2）好像不能使用其他的供应商的大模型，如智谱AI等</p><h3 id="4-ChatGPT-Plus"><a href="#4-ChatGPT-Plus" class="headerlink" title="4.ChatGPT Plus"></a>4.ChatGPT Plus</h3><p><strong>项目地址：</strong><a href="https://link.zhihu.com/?target=https://github.com/yangjian102621/chatgpt-plus">GitHub - yangjian102621&#x2F;chatgpt-plus</a></p><p><strong>项目说明：</strong>AI 助手全套开源解决方案，自带运营管理后台，开箱即用。集成了 ChatGPT, Azure, ChatGLM,讯飞星火，文心一言等多个平台的大语言模型。支持 MJ AI 绘画，Stable Diffusion AI 绘画，微博热搜等插件工具。采用 Go + Vue3 + element-plus 实现。</p><p><strong>能够使用api：</strong>随便使用，因为api是通过后台自己设置的</p><p><img src="https://pic1.zhimg.com/80/v2-2bf29de389a1d22deb49458bf148b890_720w.webp" alt="img"></p><p><img src="https://pic2.zhimg.com/80/v2-a597b200dfc8cd90b9ce1a4535a99dc1_720w.webp" alt="img"></p><h3 id="本地部署（或者通过宝塔）：-2"><a href="#本地部署（或者通过宝塔）：-2" class="headerlink" title="本地部署（或者通过宝塔）："></a>本地部署（或者通过宝塔）：</h3><p>网上有详细的部署文档，<a href="https://link.zhihu.com/?target=https://ai.r9it.com/docs/install/">一键部署 | Chat-Plus 文档</a></p><p>与其他不同的是，它后端是Go语言编写的，还是用到了Mysql8、Redis等，需要提前安装好相关环境</p><p>进入api目录，打开窗口后先运行一下：go run main.go或go build -o chatgptplus main.go，将后端打包成chatgptplus文件</p><p>进入web目录，使用npm install npm run dev运行即可</p><p><img src="https://pic4.zhimg.com/80/v2-146a030a2b2156e23f0c80932524b043_720w.webp" alt="img"></p><p><strong>总体感觉：</strong></p><p>（1）功能强大，聊天、绘画功能都有</p><p>（2）配置复杂，每个模型既要考虑代理商的消耗、在本软件中还有有相应设置</p><p>（3）适合用来商用，用来建立自己的代理站点</p><h3 id="5-ChatBox"><a href="#5-ChatBox" class="headerlink" title="5.ChatBox"></a>5.ChatBox</h3><p><strong>项目地址：</strong><a href="https://link.zhihu.com/?target=https://github.com/Bin-Huang/chatbox">https://github.com/Bin-Huang/chatbox</a></p><p><strong>项目说明：</strong>Chatbox is a desktop client for ChatGPT, Claude and other LLMs, available on Windows, Mac, Linux；提供直接下载安装包。</p><p><strong>能够使用的api：</strong>OpenAI (ChatGPT)、Azure OpenAI、Claude、Google Gemini Pro、ChatGLM-6B等各种api</p><p><strong>本地部署（或者通过宝塔）：</strong>相对简单，进入根目录使用npm install npm run start:react运行即可</p><p><img src="https://pic1.zhimg.com/80/v2-ba608d61669368eccef77c412686ad64_720w.webp" alt="img"></p><p><strong>总体感觉：</strong></p><p>（1）开源版本没有绘图功能（好像开源的是0.4.4版本），直接下载安装包有绘图</p><p>（2）简单易用</p><p>综上所述：这几款相对用得还行，当然每个人都有不同的需求，其他如ChatGPT-web、AutoGPT-Next-Web、ChatGPT-Next-Web、FastGPT也简单易用。</p>]]></content>
      
      
      <categories>
          
          <category> 实用教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ai模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GPT-sovits=自定义AI语言模型训练</title>
      <link href="/2024/03/28/%E5%AE%9E%E7%94%A8%E6%95%99%E7%A8%8B/GPT-sovits-%E8%87%AA%E5%AE%9A%E4%B9%89AI%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/"/>
      <url>/2024/03/28/%E5%AE%9E%E7%94%A8%E6%95%99%E7%A8%8B/GPT-sovits-%E8%87%AA%E5%AE%9A%E4%B9%89AI%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/</url>
      
        <content type="html"><![CDATA[<p>GPT-SoVITS是一个开源的TTS项目，只需要1分钟的音频文件就可以克隆声音，支持将汉语、英语、日语三种语言的文本转为克隆声音，作者已测试，部署很方便，训练速度很快，效果很好。项目发布不到1个月就有了9.3k star。</p><p><img src="https://pic1.zhimg.com/80/v2-baee8b7310451f87541d8cfeeec5547c_720w.webp" alt="img"></p><p>直接看作者测试的效果，能够以假乱真了。</p><p><a href="https://pan.baidu.com/link/zhihu/7dhFzeuQhvi1SkF3YmM1RYJ2Z5R1IDWwdqhm=="><img src="https://zhstatic.zhihu.com/assets/zhihu-components/file-icon/zhimg_answer_editor_file_other.svg" alt="img">中文_原声.wav307.9K·百度网盘</a><a href="https://pan.baidu.com/link/zhihu/7hhDzRuUhSi0R2lHtGaKJx0iafLwYTdwZC9W=="><img src="https://zhstatic.zhihu.com/assets/zhihu-components/file-icon/zhimg_answer_editor_file_other.svg" alt="img">中文_克隆声.wav262.4K·百度网盘</a><a href="https://pan.baidu.com/link/zhihu/75h0zVuRhLikYQNXllRids1kM5RRJlUQUtEF=="><img src="https://zhstatic.zhihu.com/assets/zhihu-components/file-icon/zhimg_answer_editor_file_other.svg" alt="img">英语_克隆声.wav423.7K·百度网盘</a><a href="https://pan.baidu.com/link/zhihu/7IhzzXuMhwiUS6Vkl2MFN6NEcidPdlVwdXhU=="><img src="https://zhstatic.zhihu.com/assets/zhihu-components/file-icon/zhimg_answer_editor_file_other.svg" alt="img">日语_克隆声.wav979.2K·百度网盘</a></p><p><strong>GPT-SoVITS项目地址</strong><br><a href="https://link.zhihu.com/?target=https://github.com/RVC-Boss/GPT-SoVITS">https://github.com/RVC-Boss/GPT-SoVITS</a></p><p><strong>以下是详细教程，阅读前请注意：</strong><br>1.本文篇幅较长，截图较多，建议先收藏再阅读。<br>2.本文基于google colab运行，本地部署及autodl云端部署也可参考，基本相同。<br>3.如需本地一键安装包，可关注公众号”AI技术实战”，回复”声音克隆”获取。</p><h2 id="1-部署运行"><a href="#1-部署运行" class="headerlink" title="1.部署运行"></a>1.部署运行</h2><p>GPT-SoVITS提供了colab的notebook，并且提供了web-ui，可以一键部署，非常方便。点击下图中”COLAB”即可打开colab notebook。也可以直接打开链接：<a href="https://link.zhihu.com/?target=https://colab.research.google.com/github/RVC-Boss/GPT-SoVITS/blob/main/colab_webui.ipynb">https://colab.research.google.com/github/RVC-Boss/GPT-SoVITS/blob/main/colab_webui.ipynb</a></p><p><img src="https://pic2.zhimg.com/80/v2-e192c1e5e187755a6d8ddd73c8ddf201_720w.webp" alt="img"></p><p>进入colab笔记本，点击”全部运行”。</p><p><img src="https://pic3.zhimg.com/80/v2-9090b24d23587c81cea38e7d159aa1fa_720w.webp" alt="img"></p><p>等待运行完成，需要等待10几分钟左右。</p><p><img src="https://pic1.zhimg.com/80/v2-98531f14cdf62fa398414647d6dda7dc_720w.webp" alt="img"></p><p>当看到日志出现Running on public URL时，说明启动成功，点击这个url，打开web界面</p><p><img src="https://pic4.zhimg.com/80/v2-dd957cd33a0f8d47c1a4bade31e6cadf_720w.webp" alt="img"></p><p>打开的web界面如下：</p><p><img src="https://pic3.zhimg.com/80/v2-62194ebde7ca70e8092ee3312e97b4fa_720w.webp" alt="img"></p><h2 id="2-声音上传和处理"><a href="#2-声音上传和处理" class="headerlink" title="2.声音上传和处理"></a>2.声音上传和处理</h2><p>准备1分钟以上的干声音频文件用于训练，最好是在安静的环境下录制，如果音频文件中有背景声音或其他声音，则必须进行下面的处理，否则非必须，但是建议处理。</p><h3 id="2-1-人声分离"><a href="#2-1-人声分离" class="headerlink" title="2.1 人声分离"></a>2.1 人声分离</h3><p>这一步操作是使用UVR5(一个处理声音的软件)提取出干净的人声，后面我们会使用提取出来的干声音频训练。进入界面，按下图选中Open UVR5-Webui。</p><p><img src="https://pic1.zhimg.com/80/v2-ca34e3ac1e986f9cb248c671f6d4765c_720w.webp" alt="img"></p><p>在colab控制台会输出一个UVR5的web界面链接</p><p><img src="https://pic3.zhimg.com/80/v2-94d17a8f66f9d209e8e1d74fc133affe_720w.webp" alt="img"></p><p>打开这个链接，进入到UVR5的界面，按如下方式操作：</p><p><img src="https://pic3.zhimg.com/80/v2-560803bee13b5996333e1a866a8a8416_720w.webp" alt="img"></p><p>等待处理完成</p><p><img src="https://pic3.zhimg.com/80/v2-8d286b04a9704911d39b8ab7197d6a02_720w.webp" alt="img"></p><p>处理成功</p><p><img src="https://pic3.zhimg.com/80/v2-1a6e2a668739384d08d3e67dd908b35e_720w.webp" alt="img"></p><p>回到colab界面，按照如下路径打开，可看到人声已经被提取出来了</p><p><img src="https://pic1.zhimg.com/80/v2-78e9466031cace0dff832f25b8da0230_720w.webp" alt="img"></p><p>把instrument开头的文件删除，只保留人声</p><p><img src="https://pic4.zhimg.com/80/v2-33a57e382c1ea050a68555849ef2423b_720w.webp" alt="img"></p><p>使用DeEchoAggressive把干声再处理一次，注意路径要写对。</p><p><img src="https://pic2.zhimg.com/80/v2-32305f00331e5ce83e1b2f0448506b01_720w.webp" alt="img"></p><p>转换成功</p><p><img src="https://pic3.zhimg.com/80/v2-07c3b9a7ba05316af57effaa1a0e9436_720w.webp" alt="img"></p><p>下面箭头指的就是最终提取出的人声，把其他两个都删除，文件夹中只保留这一个文件。</p><p><img src="https://pic4.zhimg.com/80/v2-96f36352de43e3ee78d907adec9544c7_720w.webp" alt="img"></p><p>干声提取成功后，UVR5的界面就可以关掉了，因为后面还会打开几个界面，担心不熟悉的朋友会懵，不关也可以。</p><h3 id="2-2-切割音频"><a href="#2-2-切割音频" class="headerlink" title="2.2 切割音频"></a>2.2 切割音频</h3><p>这一步要将干声音频切割，必须做，否则会爆显存。</p><p><img src="https://pic2.zhimg.com/80/v2-c7e27efd75ac24d59f2b22f559566941_720w.webp" alt="img"></p><p>等待切割完成</p><p><img src="https://pic3.zhimg.com/80/v2-474c1611ae108fe6dca81c27829de8e2_720w.webp" alt="img"></p><p>回到colab页面，可以看到音频被切割为多个小段音频</p><p><img src="https://pic1.zhimg.com/80/v2-673284722c4641b62bb83a97f22dff3c_720w.webp" alt="img"></p><h3 id="2-3-打标"><a href="#2-3-打标" class="headerlink" title="2.3 打标"></a>2.3 打标</h3><p>其实就是自动将输入的干声音频转为文字，用来告诉训练系统，音频中的哪个时间说的是什么字。注意输入的路径是上一步切割后的音频路径。</p><p><img src="https://pic3.zhimg.com/80/v2-e138cdd8b0210bd2a1fe370ee1404e9e_720w.webp" alt="img"></p><p>开始执行了</p><p><img src="https://pic1.zhimg.com/80/v2-fb71c8923355ec5ada2192c6db5f7b10_720w.webp" alt="img"></p><p>可以在colab控制台看到日志</p><p><img src="https://pic1.zhimg.com/80/v2-4f3e5836bdf350b9363e031af12109e8_720w.webp" alt="img"></p><p>稍等一会，在界面可以看到任务完成</p><p><img src="https://pic1.zhimg.com/80/v2-ff9f92c0962e4615dc2edb721809c028_720w.webp" alt="img"></p><p>在colab文件夹可以看到这个目录，.list文件就是自动打标生成的文件。</p><p><img src="https://pic4.zhimg.com/80/v2-621d5a5fd296718824c1f32cad484fab_720w.webp" alt="img"></p><h3 id="2-4-人工校对"><a href="#2-4-人工校对" class="headerlink" title="2.4 人工校对"></a>2.4 人工校对</h3><p>因为上一步是自动做的，可能有些识别的不准，所以人工校对一下，追求完美可以校对，否则可以跳过这一步，系统自动识别的已经很准确了。</p><p><img src="https://pic4.zhimg.com/80/v2-9a09d54f69125378f5f31d45fc8a228f_720w.webp" alt="img"></p><p>可以看到控制台又输出一个url，打开</p><p><img src="https://pic1.zhimg.com/80/v2-31be372c39fb396b418f7691c6676288_720w.webp" alt="img"></p><p>以下是校对的web界面，列出了部分自动识别结果，可根据情况修改，这一步作者没做，有些功能不太清楚，读者有需要的话请自行研究。</p><p><img src="https://pic4.zhimg.com/80/v2-b2ccbf41c9a9a9bd6ca37c7cc6cb0a27_720w.webp" alt="img"></p><h2 id="3-训练"><a href="#3-训练" class="headerlink" title="3.训练"></a>3.训练</h2><p>声音处理完成，终于要开始训练了。</p><h3 id="3-1数据集格式化"><a href="#3-1数据集格式化" class="headerlink" title="3.1数据集格式化"></a>3.1数据集格式化</h3><p>进入界面，按照下图填写，还是要注意各路径要写对。</p><p><img src="https://pic2.zhimg.com/80/v2-b4bc41e24ffaf2e3f8e6c88f574cb205_720w.webp" alt="img"></p><p>填写完成后，分别点击下面三个按钮，每个按钮点完后，等待执行结束再点击下一个。</p><p><img src="https://pic4.zhimg.com/80/v2-997575ff0316da0437cf364d0b3fd67f_720w.webp" alt="img"></p><h3 id="3-2-SoVITS训练"><a href="#3-2-SoVITS训练" class="headerlink" title="3.2 SoVITS训练"></a>3.2 SoVITS训练</h3><p>填写模型名称，设置batch size，建议batch_size设置为显存的一半，高了会爆显存。<br>接着设置轮数(total epoch)，SoVITS模型轮数（下图箭头4)，可以设置的高一点，GPT模型轮数（下图箭头7）不能高于20（一般情况下）建议设置10。<br>点击”Start SoVITS training”（下图箭头5），注意点了箭头5之后，不要再点箭头7。</p><p><img src="https://pic2.zhimg.com/80/v2-0e3508babaa7cd91bfefcb24b8762c51_720w.webp" alt="img"></p><p>点击Start SoVITS traing之后，可以看到开始训练了。</p><p><img src="https://pic4.zhimg.com/80/v2-af0e9d8feb6c45cbfc8332ec1a21efdb_720w.webp" alt="img"></p><p>训练的时候可以查看显卡占用，爆显存了就调低batch size，或者存在过长的音频，需要在切割音频环节将过长音频再次切割。</p><p><img src="https://pic3.zhimg.com/80/v2-727d85223fe43e3f29c392b78d893862_720w.webp" alt="img"></p><p>SoVITS训练完成后会有提示。</p><p><img src="https://pic3.zhimg.com/80/v2-1f7a0e4d00b5978b2cbf0ff6787b5eb6_720w.webp" alt="img"></p><p>可以看到SoVITS_weights目录中多了几个模型。</p><p><img src="https://pic2.zhimg.com/80/v2-2b919abc4866021a329027c0afe48659_720w.webp" alt="img"></p><p>SoVITS训练完成，可以点击”Start GPT traing”开始GPT训练了，同样点完等待。</p><p><img src="https://pic2.zhimg.com/80/v2-0bc1f74bd39f28208bd9e37d34cf0679_720w.webp" alt="img"></p><p>GPT也训练完成了。</p><p><img src="https://pic1.zhimg.com/80/v2-09bcf4faa6978d6a565f4a7696f3361c_720w.webp" alt="img"></p><h2 id="4-推理"><a href="#4-推理" class="headerlink" title="4.推理"></a>4.推理</h2><p>终于可以使用看效果了！推理又是另外一个界面了。</p><p><img src="https://pic1.zhimg.com/80/v2-ade19a682228be3f9b7e25833af51000_720w.webp" alt="img"></p><p>回到colab界面，可以看到又多了一个URL，打开这个URL</p><p><img src="https://pic4.zhimg.com/80/v2-7c79c3b862a11627f63c1ce196e0480b_720w.webp" alt="img"></p><p>这个就是推理界面了。按照下图方式设置，开始文本转语音。</p><p><img src="https://pic3.zhimg.com/80/v2-047e4ab154be22a216b98ee5142732f6_720w.webp" alt="img"></p><p>不出意外的话，稍等一会语音就合成好了，可以试听，也可以点击右边的三个小圆点下载到本地。</p><p><img src="https://pic3.zhimg.com/80/v2-55c20b17cf15d2bd51f178e54d8411ba_720w.webp" alt="img"></p><blockquote><p>上面参考音频，建议是数据集中的音频。最好5秒。参考音频很重要！会学习语速和语气，请认真选择。参考音频的文本是参考音频说什么就填什么，必须要填。语种也要对应。</p><p>切分建议无脑选50字一切，低于50字的不会切。如果50字一切报错的话就是显存太小了可以按句号切。如果不切，显存越大能合成的越多，实测4090大约1000字，但已经胡言乱语了，所以哪怕你是4090也建议切分生成。合成的过长很容易胡言乱语。如果出现吞字，重复，参考音频混入的情况，这是正常现象。不是模型炼差了，不用为模型担心。改善的方法有使用较低轮数的GPT模型、合成文本再短点、换参考音频。官方也在努力修复这个问题。</p></blockquote><h2 id="5-保存模型用于以后推理"><a href="#5-保存模型用于以后推理" class="headerlink" title="5.保存模型用于以后推理"></a>5.保存模型用于以后推理</h2><p>如果对模型比较满意的话，可以将模型保存下来，以后直接使用模型推理，不需要再浪费时间训练了。进入colab文件夹，按照下图方式下载即可。</p><p><img src="https://pic3.zhimg.com/80/v2-6d3028353a22b89aa301e2d9d9fa51de_720w.webp" alt="img"></p><p>当然也可以保存到google云盘，这样下次使用colab的时候，可以直接从google云盘获取模型。下面是保存到google云盘的步骤。</p><p><img src="https://pic3.zhimg.com/80/v2-40c0c15bba643722ec56e7be63918ff2_720w.webp" alt="img"></p><p>上面点完之后，右边会自动多出挂载云盘的代码块，运行即可。</p><p><img src="https://pic2.zhimg.com/80/v2-08a5a5c816bbe31c678a00bf3fbc3a35_720w.webp" alt="img"></p><p>挂载成功之后，刷新左边的文件目录，可以看到drive文件夹，就是google云盘，可以将上面的GPT-SoVITS文件夹中的模型文件拖动到google云盘中。</p><p><img src="https://pic4.zhimg.com/80/v2-87eb0df27cdf5aec7a231ea0ecc049d7_720w.webp" alt="img"></p><p>以后需要推理时，将GPT模型（ckpt后缀）放入GPT_weights文件夹，SoVITS模型（pth后缀）放入SoVITS_weights文件夹，如下图所示，在界面中点击刷新模型，就可以使用这些模型进行推理了。</p><p><img src="https://pic3.zhimg.com/80/v2-0270d11f2fbcd2d239465040f3786cd2_720w.webp" alt="img"></p><h2 id="6-本地部署要求"><a href="#6-本地部署要求" class="headerlink" title="6.本地部署要求"></a>6.本地部署要求</h2><p><strong>训练：</strong><br>1.Windows10&#x2F;11系统，支持 CUDA 的8G以上显存的nVIDIA 显卡;<br>2.macOS 12.3或更高版本，搭载Apple芯片（M系列芯片）或AMD GPU的Mac（如2019款Mac Pro）<br><strong>推理：</strong><br>1.Windows10&#x2F;11系统，支持 CUDA 的4G以上显存的nVIDIA 显卡;<br>2.macOS 12.3或更高版本，搭载Apple芯片（M系列芯片）或AMD GPU的Mac（如2019款Mac Pro）</p>]]></content>
      
      
      <categories>
          
          <category> 实用教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ai模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sd绘画搭建一条路服务</title>
      <link href="/2024/03/28/%E5%AE%9E%E7%94%A8%E6%95%99%E7%A8%8B/sd%E7%BB%98%E7%94%BB%E6%90%AD%E5%BB%BA%E4%B8%80%E6%9D%A1%E8%B7%AF%E6%9C%8D%E5%8A%A1/"/>
      <url>/2024/03/28/%E5%AE%9E%E7%94%A8%E6%95%99%E7%A8%8B/sd%E7%BB%98%E7%94%BB%E6%90%AD%E5%BB%BA%E4%B8%80%E6%9D%A1%E8%B7%AF%E6%9C%8D%E5%8A%A1/</url>
      
        <content type="html"><![CDATA[<p><strong>一、本地部署 Stable Diffusion</strong></p><p><strong>前言</strong></p><p>目前市面上比较权威，并能用于工作中的AI绘画软件其实就两款。一个叫Midjourney（简称MJ），另一个叫Stable-Diffusion（简称SD）。MJ需要付费使用，而SD开源免费，但是上手难度和学习成本略大，并且非常吃电脑配置（显卡、内存）。</p><p>E和Midjourney相比，Stable Diffusion 最大的优势是开源，这意味着Stable Diffusion的潜力巨大、发展飞快。由于开源免费属性，SD 已经收获了大量活跃用户，开发者社群已经为此提供了大量免费高质量的外接预训练模型（fine-tune）和插件，并且在持续维护更新。在第三方插件和模型的加持下，SD拥有比Midjourney更加丰富的个性化功能，在经过使用者调教后可以生成更贴近需求的图片，甚至在 AI 视频特效、AI音乐生成等领域，Stable Diffusion 也占据了一席之地。</p><p>Stable Diffusion是一种潜在扩散模型（Latent Diffusion Model），能够从文本描述中生成详细的图像。它还可以用于图像修复、图像绘制、文本到图像和图像到图像等任务。简单地说，我们只要给出想要的图片的文字描述在提Stable Diffusion就能生成符合你要求的逼真的图像！</p><p><img src="https://pic3.zhimg.com/80/v2-ddc6c5960c6d609ff4f49d31390fb3aa_720w.webp" alt="img"></p><p><strong>电脑配置</strong></p><p>电脑配置最核心的关键点：看显卡、看内存、看硬盘、看CPU。其中最重要的是看显卡。N卡（英伟达Nvida独立显卡）首选，效率远超集显&#x2F;AMD&#x2F;Intel显卡和CPU渲染，最低10系起步，体验感佳用40系，显存最低4G，6G及格，上不封顶；内存最低8G，16G及格，上不封顶；硬盘可用空间最好有个500G朝上，固态最佳。</p><p>系统要求：支持 Win10&#x2F;Win11&#x2F;macOS（仅限Apple Silicon，Intel 版本的 Mac 无法调用 Radeon 显卡）和 Linux 系统，苹果版 SD 兼容的插件数量较少，功能性不及 Windows 与 Linux 电脑。</p><p>如果身边没有合适的电脑可以考虑购买云主机，比如腾讯GPU云服务器。若无法使用独立显卡和云服务，亦可修改启动配置，使用CPU渲染（兼容性强，出图速度慢，需要16G以上内存）。</p><p><img src="https://pic3.zhimg.com/80/v2-f0ba84744604aff06fb0a6462b355f96_720w.webp" alt="img"></p><p>从图中可看出，与AMD或英特尔的任何产品相比，Nvidia的GPU提供了卓越的性能–有时是以巨大的优势。随着Torch的DLL修复到位，RTX 4090的性能比带有xformers的RTX 3090 Ti高出50%，而没有xformers的性能则高出43%。生成每张图片只需要三秒多。</p><p><strong>安装方法</strong></p><p>SD开源地址：<a href="https://link.zhihu.com/?target=https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki">https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki</a></p><p>目前大家普遍采用的Stable Diffusion Web UI是发布于开源程序分享网站 Github 的 Python 项目，和平常软件安装方法有所不同，不是下载安装即可用的软件，需要准备执行环境、编译源码，针对不同操作系统（操作系统依赖）、不同电脑（硬件依赖）还有做些手工调整，这需要使用者拥有一定的程序开发经验（可以现学），已经有很多大佬们写了详细的安装教程。（如<a href="https://link.zhihu.com/?target=https://www.tonyisstark.com/846.html">https://www.tonyisstark.com/846.html</a> @托尼不是塔克）</p><p>如果像我一样是小白不会装，现在可以直接使用大佬们做的一键启动程序包，比如国内@秋葉aaaki大大开发的整合包，极大的降低了安装门槛（强烈推荐！）</p><p><img src="https://pic2.zhimg.com/80/v2-89894ef22d19fa2382153cbaedc2f21d_720w.webp" alt="img"></p><p>（详见<a href="https://link.zhihu.com/?target=https://www.bilibili.com/video/BV1ne4y1V7QU">https://www.bilibili.com/video/BV1ne4y1V7QU</a>）一键启动包只是封装了可视化的一键界面，不影响出图效果，只是降低了本地部署的门槛。</p><p>Nvidia 显卡用户须知：在使用SD前，请登录 Nvidia 官网<a href="https://link.zhihu.com/?target=https://www.nvidia.cn/geforce/drivers/">https://www.nvidia.cn/geforce/drivers/</a>下载安装对应显卡最新版驱动程序，与<a href="https://link.zhihu.com/?target=https://blog.csdn.net/weixin_44177494/article/details/120444922">https://blog.csdn.net/weixin_44177494&#x2F;article&#x2F;details&#x2F;120444922</a>显卡支持的最新版 CUDA 驱动。</p><p><strong>启动SD</strong></p><p>进入SD安装文件夹，双击 webui-user.bat，待其加载完成方可使用浏览器（Chrome&#x2F;Edge）登录默认的加载IP <a href="https://link.zhihu.com/?target=http://127.0.0.1:7860/">http://127.0.0.1:7860/</a></p><p><img src="https://pic2.zhimg.com/80/v2-e9cfefe29dd80365fa9ff7bc1517f47d_720w.webp" alt="img"></p><p><strong>界面汉化</strong></p><p>如果需要中文语言包，可以下载如下中文语言包扩展，下载界面网址为：<a href="https://link.zhihu.com/?target=https://github.com/VinsonLaro/stable-diffusion-webui-chinese">https://github.com/VinsonLaro/stable-diffusion-webui-chinese</a></p><p>方法1：通过WebUI拓展进行安装<br>1.打开stable diffusion webui，进入”Extensions”选项卡<br>2.点击”Install from URL”，注意”URL for extension’s git repository”下方的输入框<br>3.粘贴或输入本Git仓库地址<a href="https://link.zhihu.com/?target=https://github.com/VinsonLaro/stable-diffusion-webui-chinese">https://github.com/VinsonLaro/stable-diffusion-webui-chinese</a><br>4.点击下方的黄色按钮”Install”即可完成安装，然后重启WebUI(点击”Install from URL”左方的”Installed”，然后点击黄色按钮”Apply and restart UI”网页下方的”Reload UI”完成重启)<br>5.点击”Settings”，左侧点击”User interface”界面，在界面里最下方的”Localization (requires restart)”，选择”Chinese-All”或者”Chinese-English”<br>6.点击界面最上方的黄色按钮”Apply settings”，再点击右侧的”Reload UI”即可完成汉化</p><p><strong>二、界面基础</strong></p><p><strong>了解界面</strong></p><p>接下来是具体的使用方法简介。目前SD并不存在通行可靠的使用规范，每个人的电脑配置、需求都不尽相同，cpkd&#x2F;Safetensors大模型、VAE、embeding、lora等AI模型、各类插件、提示词、输出参数的组合牵一发则动全身，需要大家有足够的耐心查阅插件开发者的说明文档和来自<a href="https://link.zhihu.com/?target=https://civitai.com/">https://civitai.com/</a>等分享网站的使用心得，大家可以先到civitai上搜索中意的图例，复用原作者的出图提示词、参数和模型，再以此修改，这样学习的效果最为直观。</p><p><img src="https://pic3.zhimg.com/80/v2-8712f8736f92b80fb19dff0d8f220d0a_720w.webp" alt="img"></p><p><strong>文生图</strong>：根据文本提示生成图像<br><strong>图生图</strong>：根据提供的图像作为范本、结合文本提示生成图像<br><strong>更多</strong>：优化(清晰、扩展)图像<br><strong>图片信息</strong>：显示图像基本信息,包含提示词和模型信息（除非信息被隐藏）<br><strong>模型合并</strong>：把已有的模型按不同比例进行合并生成新模型<br>训练：根据提供的图片训练具有某种图像风格的模型</p><p><img src="https://pic3.zhimg.com/80/v2-f3a273c5f3e0f8c02822091f1cb9858a_720w.webp" alt="img"></p><p>描述语分为正向&#x2F;负向描述，它们也叫tag(标签）或prompt(提示词）<br><strong>正面提示词</strong>：相比Midjourney需要写得更精准和细致，描述少就给AI更多自由发挥空间。<br><strong>负面提示词</strong>：不想让SD生成的内容。<br>正向：masterpiece, best quality, 更多画质词，画面描述<br>反向：nsfw, lowres, bad anatomy, bad hands, text, error, missing fingers,extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry,根据画面产出加不想出现的画面。</p><p><img src="https://pic1.zhimg.com/80/v2-5ada42c007dbdbf9b941cc82113acf8c_720w.webp" alt="img"></p><p><strong>生成下面的5个小图标（从左到右依次分别是）</strong></p><ul><li>复原上次生成图片的提示词（自动记录）</li><li>清空当前所有提示词</li><li>打开模型选择界面</li><li>应用选择的风格模板到当前的提示词</li><li>存档当前的正反向提示词</li></ul><p><img src="https://pic1.zhimg.com/80/v2-91b6f73a0a896678425fb672114f5830_720w.webp" alt="img"></p><p><strong>采样方法</strong><br>1.建议根据自己使用的checkpoint使用脚本跑网格图（用自己关心的参数）然后选择自己想要的结果。<br>2.懒得对比：请使用DPM++ 2M或DPM++ 2M Karras（二次元图）或UniPC，想要点惊喜和变化，Euler a、DPM++ SDE、DPM++ SDE Karras（写实图）、DPM2 a Karras（注意调整对应eta值）<br>3.eta和sigma都是多样性相关的，但是它们的多样性来自步数的变化，追求更大多样性的话应该关注seed的变化，这两项参数应该是在图片框架被选定后，再在此基础上做微调时使用的参数。</p><p><strong>采样步数</strong><br>稳定扩散通过从充满噪音的画布开始创建图像，并逐渐去噪以达到最终输出。此参数控制这些去噪步骤的数量。通常越高越好，但在一定程度上，我们使用的默认值是25个步骤。以下是不同情况下使用哪个步骤编号的一般指南：</p><ul><li>如果您正在测试新的提示，并希望获得快速结果来调整您的输入，请使用10-15个步骤</li><li>当您找到您喜欢的提示时，请将步骤增加到25</li><li>如果是有毛皮的动物或有纹理的主题，生成的图像缺少一些细节，尝试将其提高到40</li></ul><p><img src="https://pic3.zhimg.com/80/v2-0cd6be5cc8b6bc5f9813638ef9cb8a1e_720w.webp" alt="img"></p><p><strong>面部修复</strong>：修复人物的面部，但是非写实风格的人物开启面部修复可能导致面部崩坏。<br><strong>平铺</strong>：生成一张可以平铺的图像<br><strong>高分辨率重绘</strong>：使用两个步骤的过程进行生成，以较小的分辨率创建图像，然后在不改变构图的情况下改进其中的细节，选中该选项会有一系列新的参数，<br>其中重要的是：<br><strong>放大算法</strong>：Latent 在许多情况下效果不错，但重绘幅度小于 0.5 后就不甚理想。ESRGAN_4x、SwinR 4x 对 0.5 以下的重绘幅度有较好支持。<br><strong>放大倍数</strong>: 通常2倍即可<br><strong>重绘幅度</strong>：决定算法对图像内容的保留程度。该值越高，放大后图像就比放大前图像差别越大。低 denoising 意味着修正原图，高 denoising 就和原图就没有大的相关性了。一般来讲阈值是 0.7 左右，超过 0.7 和原图基本上无关，0.3 以下就是稍微改一些，0什么都不会改变，1会得到一个完全不同的图像。具体的执行步骤为 重绘强度 * 重绘步数。</p><p><img src="https://pic4.zhimg.com/80/v2-59dd1d7689b46313cd27b5fd9a85201f_720w.webp" alt="img"></p><p><strong>长宽尺寸</strong>(分辨率）<br>长宽尺寸并非数值越大越好，最佳的范围应在512至768像素之间，比如正方形图多是512<em>512和768</em>768，人像肖像 512x768，风景画 768×512，可按比例加大或减小，这个值必须是 8 的倍数。如果不希望主题对象出现重复，应在此范围内选择适当的尺寸。如果需要更高分辨率的图片，建议先使用SD模型生成图片，然后再使用合适的模型进行upscale。</p><p><strong>生成批次</strong>：每次生成图像的组数。一次运行生成图像的数量为生成批次 * 每批数量。<br><strong>每批数量</strong>：同时生成多少个图像。增加这个值可以提高性能，但你也需要更多的 VRAM。图像总数是这个值乘以批次数。除 4090 等高级显卡以外通常保持为 1。</p><p><strong>提示词相关性CFG</strong>：较高的数值将提高生成结果与提示的匹配度。<br>OpenArt上使用的默认CFG是7，这在创造力和生成你想要的东西之间提供了最佳平衡。通常不建议低于5。<br>CFG量表可以分为不同的范围，每个范围都适合不同的提示类型和目标<br>CFG 2 – 6：有创意，但可能太扭曲，没有遵循提示。对于简短的提示来说，可以很有趣和有用<br>CFG 710：推荐用于大多数提示。创造力和引导一代之间的良好平衡<br>CFG 10-15：当您确定您的提示是详细且非常清晰的，您希望图像是什么样子时<br>CFG 16-20：除非提示非常详细，否则通常不推荐。可能影响一致性和质量<br>CFG &gt;20：几乎无法使用</p><p><strong>随机种子（Seed）</strong>:生成每张图片时的随机种子，这个种子是用来作为确定扩散初始状态的基础。不懂的话，用随机的即可。</p><p><strong>提示词生成</strong></p><p>开始不知道怎么写提示词，可以先参考优秀的风格模板作为起手式，还可以借助描述语工具和网站，多出图多研究，掌握了出图规律，慢慢就可以自己写提示词啦，写提示词要尽可能写的详细。跑AI的过程就像抽卡，抽出一堆卡，选出你审美范畴里觉得好看的。</p><p><img src="https://pic1.zhimg.com/80/v2-189f1c9b48f41d2c2939757ddd9b610c_720w.webp" alt="img"></p><p>找tag关键词网站：<br>可参考Civitai | Stable Diffusion models, embeddings, hypernetworks and more中优秀作品的提示词作为模板。<br>其他网站还有：<br>ChatGPT：<a href="https://link.zhihu.com/?target=https://chat.openai.com/">https://chat.openai.com/</a><br>AI Creator：<a href="https://link.zhihu.com/?target=https://ai-creator.net/arts">https://ai-creator.net/arts</a><br>NovelAI：<a href="https://link.zhihu.com/?target=https://spell.novelai.dev">https://spell.novelai.dev</a><br>魔咒百科词典：<a href="https://link.zhihu.com/?target=https://aitag.top">https://aitag.top</a><br>AI咒术生成器：<a href="https://link.zhihu.com/?target=https://tag.redsex.cc/">https://tag.redsex.cc/</a><br>AI词汇加速器 AcceleratorI Prompt：<br>词图 PromptTool：<a href="https://link.zhihu.com/?target=https://www.prompttool.com/NovelAI">https://www.prompttool.com/NovelAI</a><br>鳖哲法典：<a href="https://link.zhihu.com/?target=http://tomxlysplay.com.cn/%23/">http://tomxlysplay.com.cn/#/</a><br>Danbooru tag：Tag Groups Wiki | Danbooru (<a href="https://link.zhihu.com/?target=http://donmai.us">http://donmai.us</a>)</p><p><strong>Prompt格式优化</strong></p><p>第一段：画质tag，画风tag<br>第二段：画面主体，主体强调，主体细节概括（主体可以是人、事、物、景）画面核心内容<br>第三段：画面场景细节，或人物细节，embedding tag。画面细节内容<br>第二段一般提供人数，人物主要特征，主要动作（一般置于人物之前），物体主要特征，主景或景色框架等</p><p><strong>举个例子</strong><br>(具体场景还是要灵活应用，多尝试，找到合适自己的节奏和风格）<br>第一段：masterpiece, best quality, 4k, ( Pixar - style :1.4)<br>第二段：1boy,(Cute,handsome,wearing outdoor sportswear :0.7), 3D,(Face close-up :1.2), (at night, surrounded by glowing plants, flowers, flying fireflies, bonfires), (Ultra detailed, aesthetic, beautiful composition, rich bright colors, volumetric soft light).<br>第三段：Inspired by Alice in Wonderland, magic, fairy tales. unreal Engine, octane render, cuteness render, awe inspiring, beautiful, <a href="lora:blindbox_V1Mix:1">lora:blindbox_V1Mix:1</a></p><p><img src="https://pic3.zhimg.com/80/v2-abe713ec346a9e8d82398373aaf5444e_720w.webp" alt="img"></p><p><strong>Prompt规则细节</strong></p><p>\1. 越靠前的Tag权重越大。<br>\2. 生成图片的大小会影响Prompt的效果，图片越大需要的Prompt越多，不然Prompt会相互污染。<br>3.Stable-diffusion中，可以使用括号人工修改提示词的权重，方法如下：<br>(word) - 将权重提高 1.1 倍<br>((word)) - 将权重提高 1.21 倍（&#x3D; 1.1 * 1.1）<br>[word] - 将权重降低至原先的 90.91%<br>(word:1.5) - 将权重提高 1.5 倍<br>(word:0.25) - 将权重减少为原先的 25%<br>请注意，权重值最好不要超过 1.5</p><p><img src="https://pic3.zhimg.com/80/v2-8af7500f4597553e3348f0bebb4f0fa2_720w.webp" alt="img"></p><p>\4. Prompt支持使用emoji，可通过添加emoji达到表现效果。如 形容表情， 可修手。<br>5.“+” ， “ AND” ， “|” 用法：“+”和“ AND ”都是用于连接短Tag，但AND两端要加空格。”+”约等于” and “；“|” 为循环绘制符号（融合符号）(Prompt A: w1)|(Prompt B: w2)<br>以上表达适用于WebUI，w1、w2为权重。AI会对A、 B两Prompt进行循环绘制。可往后无限加入Prompt。<br>6.tag不一定是多么充满细节，只要模型稳定。小图+高分辨率重绘。800<em>400的图变成1600</em>800，初识小图减少崩坏概率。<br>7.关键词最好具有特异性，譬如 Anime(动漫)一词就相对泛化，而Jojo一词就能清晰地指向 Jojo动漫的画风。措辞越不抽象越好，尽可能避免留下解释空间的措辞。</p><p><strong>三、了解模型</strong></p><p><strong>下载模型</strong></p><p>主流模型下载网站：<br>Hugging face是一个专注于构建、训练和部署先进开源机器学习模型的网站：<a href="https://link.zhihu.com/?target=https://huggingface.co/">https://huggingface.co/</a><br>Civitai是一个专为Stable Diffusion AI艺术模型设计的网站，是非常好的AI模型库：<a href="https://link.zhihu.com/?target=https://civitai.com/">https://civitai.com/</a><br>主流模型被删除可以去备用模型站下载：<a href="https://link.zhihu.com/?target=https://www.4b3.com">https://www.4b3.com</a></p><p><img src="https://pic2.zhimg.com/80/v2-45091ab3c9ea33d2c4cc12eb8bdcc031_720w.webp" alt="img"></p><p><strong>模型选择</strong></p><p>如何选择合适模型是最重要的。<br>从你想画的风格（写实、二次元、卡通盲盒等）来选择大模型，再搭配合适的Lora。</p><p><strong>1.Checkpoint</strong><br>体积较大，也被称为大模型，不同的大模型使用不同的图片训练而成，对应不同的风格，相当于最底层的引擎。有时候需要大模型+VAE+emb+Lora联合搭配使用以达到需要的效果。<br>下载的大模型可放置于SD文件夹&#x2F;models&#x2F;Stable-diffusion内。</p><p><img src="https://pic2.zhimg.com/80/v2-dcefdb6c297905e79e1880f5aed650ed_720w.webp" alt="img"></p><p><strong>2.Lora</strong></p><p>Lora是特征模型，体积较小，是基于某个确定的角色、确定的风格或者固定的动作训练而成的模型，可使用权重控制，确定性要远强于embedding。embedding和Lora有功能交集的部分，也有互相不可取代的地方。</p><p>在ckpt大模型上附加使用，对人物、姿势、物体表现较好。在webui界面的Additional Networks下勾线Enable启用，然后在Model下选择模型，并可用Weight调整权重。权重越大，该 Lora 的影响也越大。不建议权重过大（超过1.2），否则很容易出现扭曲的结果。</p><p>多个Lora模型混合使用可以起到叠加效果，譬如一个控制面部的Lora 配合一个控制画风的 Lora就可以生成具有特定画风的特定人物。因此可以使用多个专注于不同方面优化的Lora，分别调整权重，结合出自己想要实现的效果。</p><p>LoHA 模型是一种 LORA 模型的改进。<br>LoCon 模型也一种 LORA 模型的改进，泛化能力更强。<br>下载的Lora可放置于SD文件夹&#x2F;models&#x2F;Lora内。</p><p><img src="https://pic1.zhimg.com/80/v2-fda222f7c354e0b7777c3f5177e71d10_720w.webp" alt="img"></p><p><strong>3.VAE</strong><br>VAE模型类似滤镜，对画面进行调色与微调，一般需要搭配相应的模型一起使用。（如果图片比较灰，颜色不太靓丽，就可能是没加载vae)<br>下载的VAE可放置于SD文件夹&#x2F;models&#x2F;VAE内。</p><p><strong>4.Textual inversion（embedding）</strong><br>关键词预设模型，即关键词打包，即等于预设好一篮子关键词a,b,c打包，进而来指代特定的对象&#x2F;风格。也可以通过下载Textual inversion进行使用。<br>下载的embedding可放置于SD文件夹&#x2F;embeddings内。</p><p><strong>四、ControlNet</strong></p><p>ControlNet使得SD从玩具变成做商业项目的神器，接下来会重中之重来详细讲解一下。<br>ControlNet是斯坦福大学研究人员开发的Stable Diffusion的扩展，使创作者能够轻松地控制AI图像和视频中的对象。它将根据边缘检测、草图处理或人体姿势等各种条件来控制图像生成。ControlNet可以概括为一种简单的稳定扩散微调方法。ControlNet的工作原理是将可训练的网络模块附加到稳定扩散模型的U-Net （噪声预测器）的各个部分。Stable Diffusion 模型的权重是锁定的，在训练过程中它们是不变的。在训练期间仅修改附加模块。</p><p><strong>安装</strong><br>从github上找到并把网址填到扩展里安装，安装完后记得点击 Apply and restart UI（<a href="https://link.zhihu.com/?target=https://github.com/Mikubill/sd-webui-controlnet">https://github.com/Mikubill/sd-webui-controlnet</a>）</p><p><img src="https://pic4.zhimg.com/80/v2-c08d40337960bebab28092dce6b331b7_720w.webp" alt="img"></p><p>1.将ControlNet模型（.pt、.pth、.ckpt或.safetensors）放入models&#x2F;ControlNet文件夹。<br>2.打开“txt2img”或“img2img”选项卡，写下您的提示。<br>3.按“刷新模型”，选择要使用的模型。（若没有出现，请尝试重新加载&#x2F;重新启动webui）<br>4.上传您的图像并选择预处理器，完成。<br>目前，它支持完整型号和修剪型号。使用extract_controlnet.py从原始.pth文件中提取controlnet。<br>预训练模型：<a href="https://link.zhihu.com/?target=https://huggingface.co/lllyasviel/ControlNet/tree/main/models">https://huggingface.co/lllyasviel/ControlNet/tree/main/models</a></p><p><strong>界面介绍</strong></p><p><img src="https://pic1.zhimg.com/80/v2-934c4f18855d9fbd10d25549d44158b4_720w.webp" alt="img"></p><p><strong>开启</strong> :选中此框以启用ControlNet。<br><strong>颜色反转</strong>：交换黑色和白色。例如，它可以在您上传涂鸦时使用。ControlNet 需要黑色背景和白色涂鸦。如果您使用白色背景的外部软件创建涂鸦，则必须使用此选项。如果您使用 ControlNet 的界面创建涂鸦，则不需要使用此选项。<br><strong>RGB转为BGR</strong> :用于检测用户导入图像中的颜色信息。有时图像中的颜色信息可能与扩展所期望的不同。如果您上传图像并使用预处理，则无需选中此框。<br><strong>低显存</strong>:这将减缓ETA进程，但有助于使用更少的计算空间(显存小于8 GB VRAM建议使用)，检查您是否用完了 GPU 内存，或者想要增加处理的图像数量。<br><strong>推测模式</strong>:ControlNet自动识别图像(不需要提示和负面提示)与选定的预处理器。它强制 ControlNet 编码器遵循输入控制图（如深度、边缘等)，即使没有提示也是如此。使用此模式时使用更高的步进，例如50，但是这个效果不一定好。</p><p><img src="https://pic1.zhimg.com/80/v2-7a2d84822fb58596f7e2f1758a2226ac_720w.webp" alt="img"></p><p>**权重（Weight)**：代表使用 ControlNet 生成图片时被应用的权重占比。<br>**引导介入时机（Guidance Start)**：在理解此功能之前，我们应该先知道生成图片的 Sampling steps 采样步数功能，步数代表生成一张图片要刷新计算多少次，Guidance Start(T) 设置为 0 即代表开始时就介入，默认为 0，设置为 0.5 时即代表 ControlNet 从 50% 步数时开始介入计算。<br>**引导退出时机（Guidance End)**：和引导介入时机相对应，如设置为1，则表示在100%计算完时才会退出介入也就是不退出，默认为 1，可调节范围 0-1，如设置为 0.8 时即代表从80% 步数时退出介入。</p><p><img src="https://pic4.zhimg.com/80/v2-557a54044f165ca8d0ec1a7e0b043173_720w.webp" alt="img"></p><p>调整大小模式提供了调整ControlNet大小和上传图像的纵横比。<br><strong>Just Resize</strong>:不保留纵横比的情况下，改变ControlNet图像的大小以匹配Txt2Img设置的宽度和高度。这包括拉伸或压缩图像以适应指定的尺寸。<br><strong>Scale to Fit (Inner Fit)</strong>:调整ControlNet图像的大小以适应Txt2Image的尺寸。它将调整图像的大小，直到它能够适应Txt2Image设置的宽度和高度。<br><strong>Envelope (Outer Fit)</strong>:调整Txt2Image的大小以适应ControlNet图像的尺寸。它将调整图像的大小，直到Txt2Image设置可以适合ControlNet图像。<br><strong>画布宽度 和 画布高度</strong> 提供手动创建绘图或草图以，不上传任何图像（最好使用 Scribble 预处理器以获得良好的输出）。它会调整空白画布的大小来进行绘制，不会影响上传的原始图像。</p><p><img src="https://pic2.zhimg.com/80/v2-7468dde77697fbb5543c4e56149acb41_720w.webp" alt="img"></p><p><strong>预览图片处理结果</strong>：能够快速查看选择的预处理器是如何将上传的图像或绘图转换为 ControlNet的检测图。对在渲染输出图像之前尝试各种预处理器有用，可节省我们的时间。<br><strong>隐藏处理结果</strong>：删除预览图像。</p><p><img src="https://pic4.zhimg.com/80/v2-937665d029d4e7197cbc5c4e18945937_720w.webp" alt="img"></p><p>预处理器和模型是ControlNet的主要选项。<br><strong>预处理器</strong>：用于对输入图像进行预处理，例如检测边缘、深度和法线贴图。None使用输入图像作为控制图。 根据所需的输出，用户可以选择相应的控制方法。<br><strong>模型</strong>：如果您选择了预处理器，您通常会选择相应的模型。但是它并不限制你混合和匹配所有的预处理器和模型，但是混合多了就会产生负面效果，所以最好使用更加匹配的模型并且越少越好。ControlNet模型与在AUTOMATIC1111 GUI顶部选择的稳定扩散模型一起使用。</p><p><strong>预处理器</strong></p><p>下面我们介绍几个常用的 ControlNet，并在下面举例说明如何使用它。</p><p><strong>1、Canny边缘检测</strong><br>Canny通过使用边缘检测器创建高对比度区域的轮廓来检测输入图像。线条可以捕捉到非常详细的信息，但如果你的图像背景中有一些物体，它很可能会检测到不需要的物体。所以背景中物体越少效果越好。用于此预处理器的最佳模型是control_sd15_canny。</p><p><img src="https://pic3.zhimg.com/80/v2-b1c53af984360b0217ccd3e2c4887ad6_720w.webp" alt="img"></p><p><strong>2、Depth &amp; Depth Leres</strong><br>这个预处理器有助于生成输入图像的深度估计。深度通常用于控制图像内物体的空间定位。浅色区域意味着它离用户更近，而深色区域则离用户更远。<br>在大图像时它可能会丢失图像内部的细节(面部表情等)。一般会与control_sd15_depth模型组合使用。Midas Resolution函数用于增加或减少detectmap中的大小和细节级别。它的级别越高，将使用更多的VRAM，但可以生成更高质量的图像，反之亦然。<br>Depth Leres有与Depth 相同的基本概念，但在地图中包含更广泛的范围。但有时它会从图片中捕获了太多信息，可能会生成与原始图像略有不同的图像。所以最好先试用两种预处理器，然后决定哪一种。</p><p><img src="https://pic4.zhimg.com/80/v2-9a694ed81408f57cb3dac6ea6d32a11f_720w.webp" alt="img"></p><p><strong>3、HED (Holistically-Nested Edge Detection)</strong><br>Hed可以在物体周围创建清晰和精细的边界，输出类似于Canny，但减少了噪声和更柔软的边缘。它的有效性在于能够捕捉复杂的细节和轮廓，同时保留细节特征(面部表情、头发、手指等)。Hed预处理器可用于修改图像的风格和颜色。用于此预处理器的最佳模型是control_sd15_hed。</p><p><img src="https://pic3.zhimg.com/80/v2-1c2337626e557a396beff1e7a1c74b5a_720w.webp" alt="img"></p><p><strong>4、MLSD ( Mobile Line Segment Detection)</strong><br>MLSD Preprocessor 最适合生成强有力的线条，这些线条能够检测出需要独特和刚性轮廓的建筑和其他人造作品。但是它不适用于处理非刚性或弯曲的物体。MLSD适用于生成室内布局或建筑结构，因为它可以突出直线和边缘。用于此预处理器的最佳模型是control_sd15_mlsd。</p><p><img src="https://pic2.zhimg.com/80/v2-523d96396c2af3d266688bc27f941c1d_720w.webp" alt="img"></p><p><strong>5、Normal map</strong><br>法线图使用了三种主要颜色(红、绿、蓝)，通过不同的角度来精确定位物体的粗糙度和光滑程度。它生成法线图的基本估计，可以保留相当多的细节，但可能会产生意想不到的结果，因为法线图完全来自图像，而不是在3D建模软件中构建的。<br>法线图有利于突出复杂的细节和轮廓，并且在定位对象方面也很有效，特别是在接近度和距离方面。“Normal Background Threshold”用于调整背景成分。设置一个更高的阈值可以移除背景的远处部分(将其混合成紫色)。降低阈值将命令AI保留甚至显示额外的背景元素。用于此预处理器的最佳模型是control_sd15_normal。</p><p><img src="https://pic4.zhimg.com/80/v2-f1e80b5281beced419c71ee884c8d1ff_720w.webp" alt="img"></p><p><strong>6、OpenPose</strong><br>这个预处理器生成了一个基本的骨骼火柴人形象。这种技术被广泛采用，因为多个 OpenPose骨架可以组合成一个图像，这有助于引导稳定扩散生成多个一致的主题。骨架图有很多关节点，每个点代表如下图所示。</p><p><img src="https://pic1.zhimg.com/80/v2-5979d8f2284d698c10c55646546816ac_720w.webp" alt="img"></p><p><strong>7、Scribble</strong><br>涂鸦的目的是从简单的黑白线条画和草图生成图像。用户也可以使用“Canvas”选项创建特定大小的空白画布，用于手动素描（也可以直接上传图像）。如果草图和绘图由白色背景上的黑线组成，则需要选中“Invert Input Color”复选框。用于这个预处理器的最佳模型是control_sd15_openpose。</p><p><img src="https://pic3.zhimg.com/80/v2-dfca58146adafab31b4a77037647fa56_720w.webp" alt="img"></p><p><strong>8、Segmentation</strong><br>分割预处理器检测并将上传的图像分割为同一图像内的段或区域。该模型在生成一组新的图像时，将detectmap图像应用于文本提示。用于此预处理器的最佳模型是control_sd15_seg。</p><p><img src="https://pic4.zhimg.com/80/v2-5347107433e8bd44fba0ab6101a36b83_720w.webp" alt="img"></p><p><strong>附录：预处理器与对应模型清单</strong></p><p><img src="https://pic1.zhimg.com/80/v2-2f68a20f9ff5f97601b90e4df3f6e9f8_720w.webp" alt="img"></p><p><strong>总结</strong></p><p>使用AI绘图工具Stable Diffusion确实能提高美术工作者的生产效率，但是请记住：人工智能，没有人工就没有智能。Stable Diffusion并不是简单易上手的APP，我们需要花费一定的时间和精力去学习和不断调试，才能使其真正为我们所用，高效产出效果符合需求的图片。</p><p>最后，我为大家简单罗列一下使用SD的几项核心能力：<br>1.Github使用能力，使用者在熟练掌握Github开源项目的安装、调参、排错、编程环境设置等技能后，就不会在SD报错时六神无主了。<br>2.基础出图调试能力，这项能力能够让使用者无需协助就能自行摸索稳定输出可用的图片。<br>3.Controlnet构图能力，基于Controlnet的构图控制是美术从业者驾驭SD的缰绳，不会用Controlnet，你只会被随机噪声牵着走。<br>4.学习插件并组合使用的能力。<br>5.Lora等小模型的训练能力（进阶）。</p><p>如本文对您有帮助，欢迎将其分享给需要的朋友～关注我，接下来会分享更多关于Stable Diffusion的进阶内容和商业落地项目。</p><p><strong>站在巨人的肩膀上</strong></p><p><a href="https://link.zhihu.com/?target=https://avoid.overfit.cn/post/acbb609d015a40fc8d0cd26f8e215dd9">https://avoid.overfit.cn/post/acbb609d015a40fc8d0cd26f8e215dd9</a><br><a href="https://link.zhihu.com/?target=https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features%23attentionemphasis">https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#attentionemphasis</a><br><a href="https://link.zhihu.com/?target=https://muhou.net/document/236688.html">https://muhou.net/document/236688.html</a><br><a href="https://link.zhihu.com/?target=https://guide.novelai.dev/guide/prompt-engineering/practice">https://guide.novelai.dev/guide/prompt-engineering/practice</a><br><a href="https://zhuanlan.zhihu.com/p/619721909">https://zhuanlan.zhihu.com/p/619721909</a><br><a href="https://zhuanlan.zhihu.com/p/612572004">https://zhuanlan.zhihu.com/p/612572004</a><br><a href="https://link.zhihu.com/?target=https://www.163.com/dy/article/I22IV66G0518R7MO.html">https://www.163.com/dy/article/I22IV66G0518R7MO.html</a><br><a href="https://link.zhihu.com/?target=https://stable-diffusion-art.com/controlnet/">https://stable-diffusion-art.com</a></p>]]></content>
      
      
      <categories>
          
          <category> 实用教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ai绘画 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AIGC介绍及站内简介</title>
      <link href="/2024/03/27/AIGC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E7%AB%99%E5%86%85%E7%AE%80%E4%BB%8B/"/>
      <url>/2024/03/27/AIGC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E7%AB%99%E5%86%85%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<p>共同打造中国开源AIGC大环境🐲本群为AIGC资源项目分享群，不定时分享AI使用思路、干货分享、介绍AIGC，并给大家提供思路和机遇，提升认知与技术层次。</p><p>⭐️本群分享GPT4&#x2F;5、SD、MJ、Sora等AIGC的使用和一手讯息。想抓住AI风口、学习AI、想用AI提升自己的必来之地，提升自我，抓住机遇，遇见贵人。</p><p>🛫AIGC使用干货整理：</p><p>群Bot开源地址：<a href="https://github.com/zhayujie/chatgpt-on-wechat">https://github.com/zhayujie/chatgpt-on-wechat</a></p><p>🤖Bot使用指令：</p><p>① Della3 画图指令—-&gt;@创元猫 画+关键词。（ex:@创元猫 画一个苹果）</p><p>​     ⭐️如果画接口报错，就用帮我画。    （ex：@创元猫 帮我画一个苹果）</p><p>② GPT4对话指令—-&gt;@创元猫</p><p>③ 机器人指令查询—-&gt;@创元猫 #help</p><p>④ @创元猫 发送“摸鱼”获取摸鱼人日历。</p><p>⑤ @创元猫 发送“xx热榜”获取查看支持的热榜。（目前支持：微博&#x2F;虎扑&#x2F;知乎&#x2F;哔哩哔哩&#x2F;36氪&#x2F;抖音&#x2F;少数派&#x2F;IT最新&#x2F;IT科技）</p><p>⑥ @创元猫 发送星座名称查看今日运势，如“白羊座”</p><p>💫【注意事项】</p><p>🚫禁止发送一切链接</p><p>🚫禁止讨论：敏感话题、任何网络&#x2F;账号问题、政治</p><p>🚫禁止操作：批量爆粉（在群员列表里胡乱加人，引导加好友）</p>]]></content>
      
      
      <categories>
          
          <category> 站内资源 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>下一个AI风口,sora文本视频生成式AI</title>
      <link href="/2024/02/17/%E6%9C%80%E6%96%B0%E8%B5%84%E8%AE%AF/%E4%B8%8B%E4%B8%80%E4%B8%AAAI%E9%A3%8E%E5%8F%A3-sora%E6%96%87%E6%9C%AC%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E5%BC%8FAI/"/>
      <url>/2024/02/17/%E6%9C%80%E6%96%B0%E8%B5%84%E8%AE%AF/%E4%B8%8B%E4%B8%80%E4%B8%AAAI%E9%A3%8E%E5%8F%A3-sora%E6%96%87%E6%9C%AC%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E5%BC%8FAI/</url>
      
        <content type="html"><![CDATA[<p>2024.2.16 openai 官网发布了关于 sora 文本视频生成式ai的介绍（原生文）：</p><h1 id="Video-generation-models-as-world-simulators"><a href="#Video-generation-models-as-world-simulators" class="headerlink" title="Video generation models as world simulators"></a>Video generation models as world simulators</h1><p>We explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.</p><h3 id="More-resources"><a href="#More-resources" class="headerlink" title="More resources"></a>More resources</h3><ul><li><a href="https://openai.com/sora">View Sora overview</a></li></ul><p><a href="https://openai.com/research?topics=video-generation">Video generation</a>, <a href="https://openai.com/research?models=sora">Sora</a>, <a href="https://openai.com/research?contentTypes=milestone">Milestone</a>, <a href="https://openai.com/research?contentTypes=release">Release</a></p><p>This technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.</p><p>Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-1">1</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-2">2</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-3">3</a> generative adversarial networks,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-4">4</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-5">5</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-6">6</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-7">7</a> autoregressive transformers,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-8">8</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-9">9</a> and diffusion models.<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-10">10</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-11">11</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-12">12</a> These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.</p><h2 id="Turning-visual-data-into-patches"><a href="#Turning-visual-data-into-patches" class="headerlink" title="Turning visual data into patches"></a>Turning visual data into patches</h2><p>We take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-13">13</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-14">14</a> The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text—code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual <em>patches</em>. Patches have previously been shown to be an effective representation for models of visual data.<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-15">15</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-16">16</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-17">17</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-18">18</a> We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.</p><p><img src="https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&width=10&height=10&quality=50" alt="Figure Patches"></p><p>At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-19">19</a> and subsequently decomposing the representation into spacetime patches.</p><h2 id="Video-compression-network"><a href="#Video-compression-network" class="headerlink" title="Video compression network"></a>Video compression network</h2><p>We train a network that reduces the dimensionality of visual data.<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-20">20</a> This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.</p><h2 id="Spacetime-latent-patches"><a href="#Spacetime-latent-patches" class="headerlink" title="Spacetime latent patches"></a>Spacetime latent patches</h2><p>Given a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.</p><h2 id="Scaling-transformers-for-video-generation"><a href="#Scaling-transformers-for-video-generation" class="headerlink" title="Scaling transformers for video generation"></a>Scaling transformers for video generation</h2><p>Sora is a diffusion model<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-21">21</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-22">22</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-23">23</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-24">24</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-25">25</a>; given input noisy patches (and conditioning information like text prompts), it’s trained to predict the original “clean” patches. Importantly, Sora is a diffusion <em>transformer</em>.<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-26">26</a> Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-13">13</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-14">14</a> computer vision,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-15">15</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-16">16</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-17">17</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-18">18</a> and image generation.<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-27">27</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-28">28</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-29">29</a></p><p><img src="https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&width=10&height=10&quality=50" alt="Figure Diffusion"></p><p>In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.</p><p>Base compute</p><p>4x compute</p><p>32x compute</p><h2 id="Variable-durations-resolutions-aspect-ratios"><a href="#Variable-durations-resolutions-aspect-ratios" class="headerlink" title="Variable durations, resolutions, aspect ratios"></a>Variable durations, resolutions, aspect ratios</h2><p>Past approaches to image and video generation typically resize, crop or trim videos to a standard size—e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.</p><h3 id="Sampling-flexibility"><a href="#Sampling-flexibility" class="headerlink" title="Sampling flexibility"></a>Sampling flexibility</h3><p>Sora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution—all with the same model.</p><h3 id="Improved-framing-and-composition"><a href="#Improved-framing-and-composition" class="headerlink" title="Improved framing and composition"></a>Improved framing and composition</h3><p>We empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.</p><h2 id="Language-understanding"><a href="#Language-understanding" class="headerlink" title="Language understanding"></a>Language understanding</h2><p>Training text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL·E 3<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-30">30</a> to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.</p><p>Similar to DALL·E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.</p><p>a woman</p><p>wearing</p><p>a green dress and a sun hat</p><p>taking a pleasant stroll in</p><p>Johannesburg, South Africa</p><p>during</p><p>a winter storm</p><h2 id="Prompting-with-images-and-videos"><a href="#Prompting-with-images-and-videos" class="headerlink" title="Prompting with images and videos"></a>Prompting with images and videos</h2><p>All of the results above and in our <a href="https://openai.com/sora">landing page</a> show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks—creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.</p><h3 id="Animating-DALL·E-images"><a href="#Animating-DALL·E-images" class="headerlink" title="Animating DALL·E images"></a>Animating DALL·E images</h3><p>Sora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL·E 2<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-31">31</a> and DALL·E 3<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-30">30</a> images.</p><p><img src="https://cdn.openai.com/tmp/s/prompting_0.png" alt="img"></p><p>A Shiba Inu dog wearing a beret and black turtleneck.</p><p><img src="https://cdn.openai.com/tmp/s/prompting_2.png" alt="img"></p><p>Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.</p><p><img src="https://cdn.openai.com/tmp/s/prompting_4.png" alt="img"></p><p>An image of a realistic cloud that spells “SORA”.</p><p><img src="https://cdn.openai.com/tmp/s/prompting_6.png" alt="img"></p><p>In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.</p><h3 id="Extending-generated-videos"><a href="#Extending-generated-videos" class="headerlink" title="Extending generated videos"></a>Extending generated videos</h3><p>Sora is also capable of extending videos, either forward or backward in time. Below are four videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the four videos starts different from the others, yet all four videos lead to the same ending.</p><p><video class="w-full h-full object-cover" style="box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / .5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: 134.325px; width: 238.8px; object-fit: cover;"></video></p><p><video class="w-full h-full object-cover" style="box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / .5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: 134.325px; width: 238.8px; object-fit: cover;"></video></p><p><video class="w-full h-full object-cover" style="box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / .5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: 134.325px; width: 238.8px; object-fit: cover;"></video></p><p>00:0000:20</p><p>We can use this method to extend a video both forward and backward to produce a seamless infinite loop.</p><h3 id="Video-to-video-editing"><a href="#Video-to-video-editing" class="headerlink" title="Video-to-video editing"></a>Video-to-video editing</h3><p>Diffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-32">32</a> to Sora. This technique enables Sora to transform the styles and environments of input videos zero-shot.</p><p>Input video</p><p><video class="w-full h-full object-cover" style="box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / .5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: 196.988px; width: 350.2px; object-fit: cover;"></video></p><p>change the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art style</p><p><video class="w-full h-full object-cover" style="box-sizing: border-box; border-width: 0px; border-style: solid; border-color: currentcolor; --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / .5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; display: block; vertical-align: middle; max-width: 100%; height: 196.988px; width: 350.2px; object-fit: cover;"></video></p><h3 id="Connecting-videos"><a href="#Connecting-videos" class="headerlink" title="Connecting videos"></a>Connecting videos</h3><p>We can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.</p><h2 id="Image-generation-capabilities"><a href="#Image-generation-capabilities" class="headerlink" title="Image generation capabilities"></a>Image generation capabilities</h2><p>Sora is also capable of generating images. We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes—up to 2048x2048 resolution.</p><p><img src="https://cdn.openai.com/tmp/s/image_0.png" alt="img">Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of field</p><p><img src="https://cdn.openai.com/tmp/s/image_1.png" alt="img">Vibrant coral reef teeming with colorful fish and sea creatures</p><p><img src="https://cdn.openai.com/tmp/s/image_2.png" alt="img">Digital art of a young tiger under an apple tree in a matte painting style with gorgeous details</p><p><img src="https://cdn.openai.com/tmp/s/image_3.png" alt="img">A snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f&#x2F;1.2</p><h2 id="Emerging-simulation-capabilities"><a href="#Emerging-simulation-capabilities" class="headerlink" title="Emerging simulation capabilities"></a>Emerging simulation capabilities</h2><p>We find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.—they are purely phenomena of scale.</p><p><strong>3D consistency.</strong> Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.</p><p><strong>Long-range coherence and object permanence.</strong> A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.</p><p><strong>Interacting with the world.</strong> Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.</p><p><strong>Simulating digital worlds.</strong> Sora is also able to simulate artificial processes–one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning “Minecraft.”</p><p>These capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.</p><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>Sora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering. Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model—such as incoherencies that develop in long duration samples or spontaneous appearances of objects—in our <a href="https://openai.com/sora">landing page</a>.</p><p>We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.</p><h1 id="我的打算"><a href="#我的打算" class="headerlink" title="我的打算"></a>我的打算</h1><p>肯定要抓住啊，之前chatgpt出来的时候一堆套壳网站，圈了好多钱。</p><p>打算用vue+springboot+python来做个项目，就是调用官方接口，前几次免费后面收费，就这样。</p>]]></content>
      
      
      <categories>
          
          <category> 最新资讯 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
